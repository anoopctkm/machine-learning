{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Deep Learning\n",
    "## Project: Build a Digit Recognition Program\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 1: Design and Test a Model Architecture\n",
    "Design and implement a deep learning model that learns to recognize sequences of digits. Train the model using synthetic data generated by concatenating character images from [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) or [MNIST](http://yann.lecun.com/exdb/mnist/). To produce a synthetic sequence of digits for testing, you can for example limit yourself to sequences up to five digits, and use five classifiers on top of your deep network. You would have to incorporate an additional ‘blank’ character to account for shorter number sequences.\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "- Your model can be derived from a deep neural net or a convolutional network.\n",
    "- You could experiment sharing or not the weights between the softmax classifiers.\n",
    "- You can also use a recurrent network in your deep neural net to replace the classification layers and directly emit the sequence of digits one-at-a-time.\n",
    "\n",
    "You can use ** Keras ** to implement your model. Read more at [keras.io](https://keras.io/).\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf). ([video](https://www.youtube.com/watch?v=vGPI_JvLoN0)). You are not expected to model your architecture precisely using this model nor get the same performance levels, but this is more to show an exampe of an approach used to solve this particular problem. We encourage you to try out different architectures for yourself and see what works best for you. Here is a useful [forum post](https://discussions.udacity.com/t/goodfellow-et-al-2013-architecture/202363) discussing the architecture as described in the paper and here is [another one](https://discussions.udacity.com/t/what-loss-function-to-use-for-multi-digit-svhn-training/176897) discussing the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Modules \"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import struct\n",
    "import sys\n",
    "import tarfile\n",
    "import gzip\n",
    "import idx2numpy\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "#from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from array import array as pyarray\n",
    "from PIL import Image\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST data set to local machine from links provided [here](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "Contains a training set of 60,000 examples, and a test set of 10,000 handwritten digits that have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Import the MNIST Data \"\"\"\n",
    "url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "last_percent_reported = None\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    \"\"\"\n",
    "    A hook to report the progress of a download. \n",
    "    Reports every 1% change in download progress.\n",
    "    \"\"\"\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "    else:\n",
    "        sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if force or not os.path.exists(filename):\n",
    "        print('Attempting to download:', filename) \n",
    "        filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload Complete!')\n",
    "    statinfo = os.stat(filename)\n",
    "    if expected_bytes == None:\n",
    "        print('Found', filename, 'but no size to verify against')\n",
    "    elif statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified train-images-idx3-ubyte.gz\n",
      "Found and verified train-labels-idx1-ubyte.gz\n",
      "Found and verified t10k-images-idx3-ubyte.gz\n",
      "Found and verified t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "train_images_filename = maybe_download('train-images-idx3-ubyte.gz', 9912422)\n",
    "train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz', 28881)\n",
    "test_images_filename  = maybe_download('t10k-images-idx3-ubyte.gz', 1648877)\n",
    "test_labels_filename  = maybe_download('t10k-labels-idx1-ubyte.gz', 4542)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the dataset from the compressed .gz files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-images-idx3-ubyte already present - Skipping extraction of train-images-idx3-ubyte.gz.\n",
      "train-labels-idx1-ubyte already present - Skipping extraction of train-labels-idx1-ubyte.gz.\n",
      "t10k-images-idx3-ubyte already present - Skipping extraction of t10k-images-idx3-ubyte.gz.\n",
      "t10k-labels-idx1-ubyte already present - Skipping extraction of t10k-labels-idx1-ubyte.gz.\n",
      "\n",
      "Dimension check...\n",
      "\t training labels: (60000,)\n",
      "\t training images: (60000, 28, 28)\n",
      "\t test set labels: (10000,)\n",
      "\t test set labels: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "def maybe_extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .gz\n",
    "    # extract data\n",
    "    if os.path.isfile(root) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        inF = gzip.GzipFile(filename, 'rb')\n",
    "        s = inF.read()\n",
    "        inF.close()\n",
    "\n",
    "        outF = open(root, 'wb')\n",
    "        outF.write(s)\n",
    "        outF.close()\n",
    "    # Return as numpy array\n",
    "    f = open(root, 'rb')\n",
    "    if 'images' in root:\n",
    "        x = idx2numpy.convert_from_file(f)\n",
    "        f.close()\n",
    "\n",
    "    elif 'labels' in root:\n",
    "        magic_nr, size = struct.unpack(\">II\", f.read(8))\n",
    "        x = pyarray(\"b\", f.read())\n",
    "        f.close()\n",
    "\n",
    "    return np.array(x)\n",
    "\n",
    "train_images = maybe_extract(train_images_filename)\n",
    "train_labels = maybe_extract(train_labels_filename)\n",
    "test_images  = maybe_extract(test_images_filename)\n",
    "test_labels  = maybe_extract(test_labels_filename)\n",
    "\n",
    "print (\"\\nDimension check...\")\n",
    "print (\"\\t training labels:\", train_labels.shape)\n",
    "print (\"\\t training images:\", train_images.shape)\n",
    "print (\"\\t test set labels:\", test_labels.shape)\n",
    "print (\"\\t test set labels:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine a few images to make sure that it's sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show(image):\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 2D array of pixel data.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(5):\n",
    "    print(\"Image below should be a\", train_labels[i])\n",
    "    show(train_images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to make sure that the image values are normalized (to suit to DNN).\n",
    "\n",
    "Currently, the range of values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.min(),train_images.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll normalize them to a range of 0 to 1, which is suitable for hidden relu layers.\n",
    "\n",
    "Aside, this is in contrast to activation functions like `tanh` are used, which would better suit a range of -.5 to .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New min and max: 0.0 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFdCAYAAADSR9wBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116ae9390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pixel_max = float(train_images.max())\n",
    "\n",
    "train_images = train_images / pixel_max\n",
    "test_images  = test_images / pixel_max\n",
    "\n",
    "# Check images are still legit:\n",
    "print(\"New min and max:\", train_images.min(),train_images.max())\n",
    "\n",
    "show(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create synthetic combinations of these images. Eg., up to five images combined at a time.\n",
    "\n",
    "This section will create training and test sets of these synthetic sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAACECAYAAAB1VJkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x175455810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using numpy.hstack will stack together the pixel arrays to form synthetic sequences.\n",
    "# Example:\n",
    "show(np.hstack(train_images[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will need to create a random (ideally reproducible) set of these sequences up to length 5.\n",
    "\n",
    "We also need to handle sequences of varying lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image below should show [ 4  4  5 -1 -1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAACECAYAAAB1VJkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116703610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image below should show [4 2 7 7 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAACECAYAAAB1VJkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhkgCp8ERGRDPh/jZ2HOD2Rv+cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116baa910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image below should show [ 8 -1 -1 -1 -1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAACECAYAAAB1VJkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCBR4yDpXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117bd77d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define maximum sequence length\n",
    "seq_max_n = 5\n",
    "\n",
    "# Define the number of training, validation, and test sequences to create\n",
    "n_train_seq = 100000\n",
    "n_valid_seq =  30000  # Note - these will come from test data\n",
    "n_test_seq  =  30000\n",
    "\n",
    "# Create max-length sequences as an array of references to single digits\n",
    "np.random.seed(seed=345)\n",
    "train_seq = np.random.randint(len(train_images), size=[n_train_seq, seq_max_n])\n",
    "valid_seq = np.random.randint(len(test_images), size=[n_valid_seq, seq_max_n])\n",
    "test_seq  = np.random.randint(len(test_images),  size=[n_test_seq, seq_max_n])\n",
    "\n",
    "# Generate sequence lengths (1 - seq_max_n)\n",
    "np.random.seed(seed=54)\n",
    "train_seq_n = np.random.randint(seq_max_n, size=n_train_seq) + 1\n",
    "valid_seq_n = np.random.randint(seq_max_n, size=n_valid_seq) + 1\n",
    "test_seq_n  = np.random.randint(seq_max_n, size=n_test_seq) + 1\n",
    "\n",
    "# Define a no-digit matrix of zeros and append as last item in image sets\n",
    "# Label for this is -1\n",
    "no_digit_array = [np.zeros(train_images[0].shape)]\n",
    "train_images=np.append(train_images, no_digit_array, axis=0)\n",
    "test_images=np.append(test_images, no_digit_array, axis=0)\n",
    "train_labels=np.append(train_labels, -1)\n",
    "test_labels=np.append(test_labels, -1)\n",
    "\n",
    "# Reference 'no_digit' beyond allocated sequence length\n",
    "\n",
    "''' Substitute in no-digit matrices beyond allocated sequence length '''\n",
    "def no_digit_sub(seq, seq_n, seq_max_n = seq_max_n):\n",
    "    if seq_n < seq_max_n:\n",
    "        to_zero = range(seq_n, seq_max_n)\n",
    "        for z in to_zero:\n",
    "            # Nb, this -1 references last item in array\n",
    "            # Not to be confused with -1 label above\n",
    "            seq[z] = -1\n",
    "    \n",
    "for i, n in enumerate(train_seq_n):\n",
    "    no_digit_sub(train_seq[i], n)\n",
    "    \n",
    "for i, n in enumerate(valid_seq_n):\n",
    "    no_digit_sub(valid_seq[i], n)\n",
    "    \n",
    "for i, n in enumerate(test_seq_n):\n",
    "    no_digit_sub(test_seq[i], n)\n",
    "\n",
    "print (\"The image below should show\", train_labels[train_seq[0]])\n",
    "show(np.hstack(train_images[train_seq[0]]))\n",
    "\n",
    "print (\"The image below should show\", test_labels[valid_seq[0]])\n",
    "show(np.hstack(test_images[valid_seq[0]]))\n",
    "\n",
    "print (\"The image below should show\", test_labels[test_seq[0]])\n",
    "show(np.hstack(test_images[test_seq[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we created an array of length-5 arrays. Each length-5 array contains index references to single digits.\n",
    "\n",
    "The images above are created by combining these references. However, we need to store these as single images to operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final image array shapes...\n",
      "train\t (100000, 28, 140)\n",
      "valid\t (30000, 28, 140)\n",
      "test\t (30000, 28, 140)\n",
      "\n",
      "The image below should show [ 4  4  5 -1 -1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAACECAYAAAB1VJkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116adc790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image below should show [4 2 7 7 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAACECAYAAAB1VJkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhkgCp8ERGRDPh/jZ2HOD2Rv+cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11be66490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image below should show [ 8 -1 -1 -1 -1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAACECAYAAAB1VJkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCBR4yDpXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1719e89d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize final train, valid, and test sets of images:\n",
    "train = np.empty(np.append(n_train_seq, np.hstack(train_images[train_seq[0]]).shape), dtype=float)\n",
    "valid = np.empty(np.append(n_valid_seq, np.hstack(test_images[valid_seq[0]]).shape), dtype=float)\n",
    "test  = np.empty(np.append(n_test_seq, np.hstack(test_images[test_seq[0]]).shape), dtype=float)\n",
    "\n",
    "train_l = np.empty(np.append(n_train_seq, seq_max_n), dtype=int)\n",
    "valid_l = np.empty(np.append(n_valid_seq, seq_max_n), dtype=int)\n",
    "test_l  = np.empty(np.append(n_test_seq, seq_max_n), dtype=int)\n",
    "\n",
    "print(\"Final image array shapes...\")\n",
    "print('train\\t', train.shape)\n",
    "print('valid\\t', valid.shape)\n",
    "print('test\\t', test.shape)\n",
    "\n",
    "# Fill in these arrays with stacked images\n",
    "for i, s in enumerate(train_seq):\n",
    "    train[i] = np.hstack(train_images[s])\n",
    "    train_l[i] = train_labels[s]\n",
    "\n",
    "for i, s in enumerate(valid_seq):\n",
    "    valid[i] = np.hstack(test_images[s])\n",
    "    valid_l[i] = test_labels[s]\n",
    "\n",
    "for i, s in enumerate(test_seq):\n",
    "    test[i] = np.hstack(test_images[s])\n",
    "    test_l[i] = test_labels[s]\n",
    "\n",
    "print()\n",
    "# Tests...\n",
    "print (\"The image below should show\", train_l[0])\n",
    "show(train[0])\n",
    "\n",
    "print (\"The image below should show\", valid_l[0])\n",
    "show(valid[0])\n",
    "\n",
    "print (\"The image below should show\", test_l[0])\n",
    "show(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to get data into shape suitable for convolution network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 140, 28, 1) (100000, 5, 11)\n",
      "Validation set (30000, 140, 28, 1) (30000, 5, 11)\n",
      "Test set (30000, 140, 28, 1) (30000, 5, 11)\n"
     ]
    }
   ],
   "source": [
    "img_height = train[0].shape[0]\n",
    "img_width  = train[0].shape[1]\n",
    "num_labels = 11  # -1 to 9\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, img_width, img_height, num_channels)).astype(np.float32)\n",
    "    labels = ((np.arange(num_labels) - 1) == labels[:,:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train, train_l = reformat(train, train_l)\n",
    "valid, valid_l = reformat(valid, valid_l)\n",
    "test, test_l   = reformat(test, test_l)\n",
    "print('Training set', train.shape, train_l.shape)\n",
    "print('Validation set', valid.shape, valid_l.shape)\n",
    "print('Test set', test.shape, test_l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the above that the labels have been one-hot encoded as 2D vectors, which provide location on axis 0 and value information on axis 1.\n",
    "\n",
    "For example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_l[0])  # Should correspond to 4,4,5 shown above (trailed by -1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our images and labels in reasonable shape.\n",
    "\n",
    "Time to implement our DNN. We'll start with convoluation layers feeding into 5 logit classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to determine prediction accuracy\n",
    "def accuracy(predictions, labels):\n",
    "    return(100.0 * np.sum(np.argmax(predictions, 2) == np.argmax(labels, 2))\n",
    "          / predictions.shape[0] / predictions.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"pack:0\", shape=(128, 5, 11), dtype=float32)\n",
      "Tensor(\"pack_1:0\", shape=(30000, 5, 11), dtype=float32)\n",
      "Tensor(\"pack_2:0\", shape=(30000, 5, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "patch_size = 5\n",
    "\n",
    "# Conv depths...\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "\n",
    "# Hidden depths...\n",
    "n_hidden1 = 128\n",
    "n_hidden2 = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data -----------------------\n",
    "    tf_train = tf.placeholder(\n",
    "        tf.float32, shape = (batch_size, img_width, img_height, num_channels)\n",
    "    )\n",
    "    tf_train_l = tf.placeholder(\n",
    "        tf.float32, shape = (batch_size, svhn_max_digits, num_labels)\n",
    "    )\n",
    "    tf_valid = tf.constant(valid)\n",
    "    tf_test  = tf.constant(test)\n",
    "    \n",
    "    # Net parameters -----------------------\n",
    "    \n",
    "    ## TODO: create wrapper function to initialize weights and biases\n",
    "    \n",
    "    # Convolution layers\n",
    "    c1_w = tf.Variable(\n",
    "        # truncated_normal good to avoid vanishing gradient; set stdev to something small (.05)\n",
    "        # don't use 'random'\n",
    "        # Deep Learning researcher - Yoshua Bengio\n",
    "        # Tensorflow has Xavier initialization scheme to help optimize weight initialization\n",
    "        # Supports quicker learning and avoid vanishing gradient\n",
    "        # Contrib layers in tensorflow\n",
    "        tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev = 0.05)\n",
    "    )\n",
    "    c1_b = tf.Variable(\n",
    "        tf.zeros([depth1])\n",
    "    )\n",
    "    c2_w = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, depth1, depth2], stddev = 0.05)\n",
    "    )\n",
    "    c2_b = tf.Variable(\n",
    "        tf.constant(1.0, shape = [depth2])\n",
    "    )\n",
    "    c3_w = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, depth2, depth3], stddev = 0.05)\n",
    "    )\n",
    "    c3_b = tf.Variable(\n",
    "        tf.constant(1.0, shape = [depth3])\n",
    "    )\n",
    "    # Fully connected layers\n",
    "    f1_w = tf.Variable(\n",
    "        tf.truncated_normal([depth3*3, n_hidden1], stddev = 0.05)\n",
    "    )\n",
    "    f1_b = tf.constant(1.0, shape = [n_hidden1])\n",
    "    f2_w = tf.Variable(\n",
    "        tf.truncated_normal([n_hidden1, n_hidden2], stddev = 0.05)\n",
    "    )\n",
    "    f2_b = tf.constant(1.0, shape = [n_hidden2])\n",
    "    \n",
    "    \n",
    "    # Logistic classifier layers (one for each digit)\n",
    "    s1_w = tf.Variable(tf.truncated_normal([n_hidden2, num_labels], stddev = 0.05))\n",
    "    s1_b = tf.constant(1.0, shape = [num_labels])\n",
    "    s2_w = tf.Variable(tf.truncated_normal([n_hidden2, num_labels], stddev = 0.05))\n",
    "    s2_b = tf.constant(1.0, shape = [num_labels])\n",
    "    s3_w = tf.Variable(tf.truncated_normal([n_hidden2, num_labels], stddev = 0.05))\n",
    "    s3_b = tf.constant(1.0, shape = [num_labels])\n",
    "    s4_w = tf.Variable(tf.truncated_normal([n_hidden2, num_labels], stddev = 0.05))\n",
    "    s4_b = tf.constant(1.0, shape = [num_labels])\n",
    "    s5_w = tf.Variable(tf.truncated_normal([n_hidden2, num_labels], stddev = 0.05))\n",
    "    s5_b = tf.constant(1.0, shape = [num_labels])\n",
    "    \n",
    "    # Model ----------------------\n",
    "    \n",
    "    # Convolution wrapper\n",
    "    # - Can move this (and others) to external python file with required libraries and import\n",
    "    def conv2d(X, W, b):\n",
    "        conv = tf.nn.conv2d(X, W, strides = [1, 2, 2, 1], padding = 'SAME') + b\n",
    "        # Order (1) pool then (2) hidden reduces computation requirements and speeds learning\n",
    "        pool = tf.nn.max_pool(conv, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "        hidden = tf.nn.relu(pool)\n",
    "        return hidden\n",
    "    \n",
    "    # Model\n",
    "    def model(data, keep_prob = 1):\n",
    "        # Convolution layers\n",
    "        x = conv2d(data, c1_w, c1_b)\n",
    "        x = conv2d(x,    c2_w, c2_b)\n",
    "        x = conv2d(x,    c3_w, c3_b)\n",
    "        \n",
    "        # Fully connected\n",
    "        shape = x.get_shape().as_list()  ## TODO: lookup tf.flatten to handle this MUCH faster (in C++)\n",
    "        reshape = tf.reshape(x, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        x = tf.nn.relu(tf.matmul(reshape, f1_w) + f1_b)\n",
    "        x = tf.nn.relu(tf.matmul(x, f2_w) + f2_b)\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        \n",
    "        # Logistic classifiers for each digit\n",
    "        logit1 = tf.matmul(x, s1_w) + s1_b\n",
    "        logit2 = tf.matmul(x, s2_w) + s2_b\n",
    "        logit3 = tf.matmul(x, s3_w) + s3_b\n",
    "        logit4 = tf.matmul(x, s4_w) + s4_b\n",
    "        logit5 = tf.matmul(x, s5_w) + s5_b\n",
    "        \n",
    "        # Combine predictions to match 2D encoding\n",
    "        # Note. if using sparse_... need pack([]), but not `axis=1`\n",
    "        pred = tf.pack([logit1, logit2, logit3, logit4, logit5], axis=1)\n",
    "        print(pred)\n",
    "        return(pred)\n",
    "\n",
    "    # Run --------------------------\n",
    "    # If model seems to be overfitting, look at:\n",
    "    # l1 and l2 regularisers; batch normalization\n",
    "    \n",
    "    logits = model(tf_train, 0.50)  # Drop out - .75 too high. Try not to exceed .50\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits[:, 0, :], tf_train_l[:, 0])) +\\\n",
    "        tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits[:, 1, :], tf_train_l[:, 1])) +\\\n",
    "        tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits[:, 2, :], tf_train_l[:, 2])) +\\\n",
    "        tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits[:, 3, :], tf_train_l[:, 3])) +\\\n",
    "        tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits[:, 4, :], tf_train_l[:, 4]))\n",
    "    \n",
    "    ## TODO: examine Adam optimizer (has a self adjusting learning rate)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    train_pred = tf.nn.softmax(logits)\n",
    "    valid_pred = tf.nn.softmax(model(tf_valid))\n",
    "    test_pred  = tf.nn.softmax(model(tf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-79-c21b1c08ec02>:6 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Intialized\n",
      "\n",
      "--- Step 0 ---\n",
      "Time elapsed: 4.4 seconds\n",
      "Minibatch loss: 13.666569\n",
      "Minibatch accuracy: 6.7%\n",
      "Validation accuracy: 6.1%\n",
      "\n",
      "--- Step 1000 ---\n",
      "Time elapsed: 120.6 seconds\n",
      "Minibatch loss: 6.869938\n",
      "Minibatch accuracy: 51.2%\n",
      "Validation accuracy: 50.3%\n",
      "\n",
      "--- Step 2000 ---\n",
      "Time elapsed: 231.2 seconds\n",
      "Minibatch loss: 6.398312\n",
      "Minibatch accuracy: 55.6%\n",
      "Validation accuracy: 54.5%\n",
      "\n",
      "--- Step 3000 ---\n",
      "Time elapsed: 340.6 seconds\n",
      "Minibatch loss: 6.349179\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 57.5%\n",
      "\n",
      "--- Step 4000 ---\n",
      "Time elapsed: 450.8 seconds\n",
      "Minibatch loss: 6.066477\n",
      "Minibatch accuracy: 57.2%\n",
      "Validation accuracy: 59.1%\n",
      "\n",
      "--- Step 5000 ---\n",
      "Time elapsed: 561.2 seconds\n",
      "Minibatch loss: 5.937634\n",
      "Minibatch accuracy: 55.5%\n",
      "Validation accuracy: 60.3%\n",
      "\n",
      "--- Step 6000 ---\n",
      "Time elapsed: 670.1 seconds\n",
      "Minibatch loss: 5.629911\n",
      "Minibatch accuracy: 59.8%\n",
      "Validation accuracy: 61.3%\n",
      "\n",
      "--- Step 7000 ---\n",
      "Time elapsed: 779.3 seconds\n",
      "Minibatch loss: 5.715333\n",
      "Minibatch accuracy: 58.9%\n",
      "Validation accuracy: 62.2%\n",
      "\n",
      "--- Step 8000 ---\n",
      "Time elapsed: 892.3 seconds\n",
      "Minibatch loss: 5.592608\n",
      "Minibatch accuracy: 57.7%\n",
      "Validation accuracy: 62.6%\n",
      "\n",
      "--- Step 9000 ---\n",
      "Time elapsed: 1002.4 seconds\n",
      "Minibatch loss: 4.895943\n",
      "Minibatch accuracy: 65.5%\n",
      "Validation accuracy: 63.1%\n",
      "\n",
      "--- Step 10000 ---\n",
      "Time elapsed: 1111.5 seconds\n",
      "Minibatch loss: 5.442333\n",
      "Minibatch accuracy: 59.7%\n",
      "Validation accuracy: 63.6%\n",
      "\n",
      "--- Step 11000 ---\n",
      "Time elapsed: 1221.0 seconds\n",
      "Minibatch loss: 4.878127\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 64.3%\n",
      "\n",
      "--- Step 12000 ---\n",
      "Time elapsed: 1330.3 seconds\n",
      "Minibatch loss: 5.686264\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 65.3%\n",
      "\n",
      "--- Step 13000 ---\n",
      "Time elapsed: 1439.7 seconds\n",
      "Minibatch loss: 5.597323\n",
      "Minibatch accuracy: 59.8%\n",
      "Validation accuracy: 66.1%\n",
      "\n",
      "--- Step 14000 ---\n",
      "Time elapsed: 1549.3 seconds\n",
      "Minibatch loss: 5.415405\n",
      "Minibatch accuracy: 60.3%\n",
      "Validation accuracy: 67.0%\n",
      "\n",
      "--- Step 15000 ---\n",
      "Time elapsed: 1658.8 seconds\n",
      "Minibatch loss: 4.764009\n",
      "Minibatch accuracy: 65.9%\n",
      "Validation accuracy: 68.0%\n",
      "\n",
      "--- Step 16000 ---\n",
      "Time elapsed: 1767.9 seconds\n",
      "Minibatch loss: 5.302951\n",
      "Minibatch accuracy: 61.6%\n",
      "Validation accuracy: 68.6%\n",
      "\n",
      "--- Step 17000 ---\n",
      "Time elapsed: 1876.8 seconds\n",
      "Minibatch loss: 4.561212\n",
      "Minibatch accuracy: 67.0%\n",
      "Validation accuracy: 69.2%\n",
      "\n",
      "--- Step 18000 ---\n",
      "Time elapsed: 1985.3 seconds\n",
      "Minibatch loss: 4.958930\n",
      "Minibatch accuracy: 63.0%\n",
      "Validation accuracy: 69.7%\n",
      "\n",
      "--- Step 19000 ---\n",
      "Time elapsed: 2094.1 seconds\n",
      "Minibatch loss: 5.221433\n",
      "Minibatch accuracy: 63.1%\n",
      "Validation accuracy: 70.0%\n",
      "\n",
      "--- Step 20000 ---\n",
      "Time elapsed: 2203.0 seconds\n",
      "Minibatch loss: 4.756404\n",
      "Minibatch accuracy: 64.7%\n",
      "Validation accuracy: 70.7%\n",
      "\n",
      "--- Step 21000 ---\n",
      "Time elapsed: 2312.2 seconds\n",
      "Minibatch loss: 5.008261\n",
      "Minibatch accuracy: 64.5%\n",
      "Validation accuracy: 70.9%\n",
      "\n",
      "--- Step 22000 ---\n",
      "Time elapsed: 2420.9 seconds\n",
      "Minibatch loss: 5.010370\n",
      "Minibatch accuracy: 64.2%\n",
      "Validation accuracy: 71.3%\n",
      "\n",
      "--- Step 23000 ---\n",
      "Time elapsed: 2529.9 seconds\n",
      "Minibatch loss: 4.829802\n",
      "Minibatch accuracy: 65.0%\n",
      "Validation accuracy: 71.7%\n",
      "\n",
      "--- Step 24000 ---\n",
      "Time elapsed: 2638.7 seconds\n",
      "Minibatch loss: 5.046252\n",
      "Minibatch accuracy: 63.4%\n",
      "Validation accuracy: 71.9%\n",
      "\n",
      "--- Step 25000 ---\n",
      "Time elapsed: 2748.2 seconds\n",
      "Minibatch loss: 5.160916\n",
      "Minibatch accuracy: 63.0%\n",
      "Validation accuracy: 72.2%\n",
      "\n",
      "--- Step 26000 ---\n",
      "Time elapsed: 2857.8 seconds\n",
      "Minibatch loss: 4.564767\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 72.4%\n",
      "\n",
      "--- Step 27000 ---\n",
      "Time elapsed: 2966.9 seconds\n",
      "Minibatch loss: 4.292427\n",
      "Minibatch accuracy: 67.5%\n",
      "Validation accuracy: 72.8%\n",
      "\n",
      "--- Step 28000 ---\n",
      "Time elapsed: 3075.9 seconds\n",
      "Minibatch loss: 5.106241\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 73.1%\n",
      "\n",
      "--- Step 29000 ---\n",
      "Time elapsed: 3184.8 seconds\n",
      "Minibatch loss: 4.549870\n",
      "Minibatch accuracy: 66.7%\n",
      "Validation accuracy: 73.5%\n",
      "\n",
      "--- Step 30000 ---\n",
      "Time elapsed: 3294.4 seconds\n",
      "Minibatch loss: 4.271664\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 73.7%\n",
      "\n",
      "--- Step 31000 ---\n",
      "Time elapsed: 3404.0 seconds\n",
      "Minibatch loss: 4.617121\n",
      "Minibatch accuracy: 68.4%\n",
      "Validation accuracy: 73.8%\n",
      "\n",
      "--- Step 32000 ---\n",
      "Time elapsed: 3513.3 seconds\n",
      "Minibatch loss: 4.615280\n",
      "Minibatch accuracy: 65.2%\n",
      "Validation accuracy: 74.2%\n",
      "\n",
      "--- Step 33000 ---\n",
      "Time elapsed: 3622.5 seconds\n",
      "Minibatch loss: 4.970768\n",
      "Minibatch accuracy: 63.6%\n",
      "Validation accuracy: 74.5%\n",
      "\n",
      "--- Step 34000 ---\n",
      "Time elapsed: 3731.4 seconds\n",
      "Minibatch loss: 3.832706\n",
      "Minibatch accuracy: 71.4%\n",
      "Validation accuracy: 74.8%\n",
      "\n",
      "--- Step 35000 ---\n",
      "Time elapsed: 3840.8 seconds\n",
      "Minibatch loss: 4.424398\n",
      "Minibatch accuracy: 69.8%\n",
      "Validation accuracy: 75.1%\n",
      "\n",
      "--- Step 36000 ---\n",
      "Time elapsed: 3950.1 seconds\n",
      "Minibatch loss: 4.133277\n",
      "Minibatch accuracy: 72.0%\n",
      "Validation accuracy: 75.2%\n",
      "\n",
      "--- Step 37000 ---\n",
      "Time elapsed: 4059.5 seconds\n",
      "Minibatch loss: 4.470990\n",
      "Minibatch accuracy: 69.1%\n",
      "Validation accuracy: 75.6%\n",
      "\n",
      "--- Step 38000 ---\n",
      "Time elapsed: 4168.7 seconds\n",
      "Minibatch loss: 4.685812\n",
      "Minibatch accuracy: 66.1%\n",
      "Validation accuracy: 75.7%\n",
      "\n",
      "--- Step 39000 ---\n",
      "Time elapsed: 4278.3 seconds\n",
      "Minibatch loss: 4.158319\n",
      "Minibatch accuracy: 68.4%\n",
      "Validation accuracy: 76.0%\n",
      "\n",
      "--- Step 40000 ---\n",
      "Time elapsed: 4387.7 seconds\n",
      "Minibatch loss: 3.741835\n",
      "Minibatch accuracy: 74.1%\n",
      "Validation accuracy: 76.4%\n",
      "\n",
      "--- Step 41000 ---\n",
      "Time elapsed: 4496.8 seconds\n",
      "Minibatch loss: 4.041820\n",
      "Minibatch accuracy: 71.2%\n",
      "Validation accuracy: 76.7%\n",
      "\n",
      "--- Step 42000 ---\n",
      "Time elapsed: 4605.9 seconds\n",
      "Minibatch loss: 4.719523\n",
      "Minibatch accuracy: 66.9%\n",
      "Validation accuracy: 76.8%\n",
      "\n",
      "--- Step 43000 ---\n",
      "Time elapsed: 4714.9 seconds\n",
      "Minibatch loss: 4.098563\n",
      "Minibatch accuracy: 70.6%\n",
      "Validation accuracy: 77.2%\n",
      "\n",
      "--- Step 44000 ---\n",
      "Time elapsed: 4824.6 seconds\n",
      "Minibatch loss: 4.120086\n",
      "Minibatch accuracy: 68.6%\n",
      "Validation accuracy: 77.4%\n",
      "\n",
      "--- Step 45000 ---\n",
      "Time elapsed: 4934.4 seconds\n",
      "Minibatch loss: 4.203897\n",
      "Minibatch accuracy: 70.9%\n",
      "Validation accuracy: 77.8%\n",
      "\n",
      "--- Step 46000 ---\n",
      "Time elapsed: 5043.7 seconds\n",
      "Minibatch loss: 4.277735\n",
      "Minibatch accuracy: 69.8%\n",
      "Validation accuracy: 77.8%\n",
      "\n",
      "--- Step 47000 ---\n",
      "Time elapsed: 5152.8 seconds\n",
      "Minibatch loss: 4.270726\n",
      "Minibatch accuracy: 68.1%\n",
      "Validation accuracy: 78.3%\n",
      "\n",
      "--- Step 48000 ---\n",
      "Time elapsed: 5261.7 seconds\n",
      "Minibatch loss: 3.972500\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 78.3%\n",
      "\n",
      "--- Step 49000 ---\n",
      "Time elapsed: 5371.1 seconds\n",
      "Minibatch loss: 3.571532\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.8%\n",
      "\n",
      "--- Step 50000 ---\n",
      "Time elapsed: 5479.6 seconds\n",
      "Minibatch loss: 3.848175\n",
      "Minibatch accuracy: 72.5%\n",
      "Validation accuracy: 78.9%\n",
      "\n",
      "--- Step 51000 ---\n",
      "Time elapsed: 5588.7 seconds\n",
      "Minibatch loss: 3.385436\n",
      "Minibatch accuracy: 76.1%\n",
      "Validation accuracy: 79.1%\n",
      "\n",
      "--- Step 52000 ---\n",
      "Time elapsed: 5697.8 seconds\n",
      "Minibatch loss: 3.458796\n",
      "Minibatch accuracy: 75.9%\n",
      "Validation accuracy: 79.1%\n",
      "\n",
      "--- Step 53000 ---\n",
      "Time elapsed: 5807.4 seconds\n",
      "Minibatch loss: 3.807719\n",
      "Minibatch accuracy: 72.3%\n",
      "Validation accuracy: 79.5%\n",
      "\n",
      "--- Step 54000 ---\n",
      "Time elapsed: 5916.8 seconds\n",
      "Minibatch loss: 4.208333\n",
      "Minibatch accuracy: 69.7%\n",
      "Validation accuracy: 79.6%\n",
      "\n",
      "--- Step 55000 ---\n",
      "Time elapsed: 6025.8 seconds\n",
      "Minibatch loss: 4.600707\n",
      "Minibatch accuracy: 65.8%\n",
      "Validation accuracy: 79.9%\n",
      "\n",
      "--- Step 56000 ---\n",
      "Time elapsed: 6135.3 seconds\n",
      "Minibatch loss: 3.735917\n",
      "Minibatch accuracy: 75.3%\n",
      "Validation accuracy: 80.1%\n",
      "\n",
      "--- Step 57000 ---\n",
      "Time elapsed: 6244.7 seconds\n",
      "Minibatch loss: 4.265768\n",
      "Minibatch accuracy: 68.3%\n",
      "Validation accuracy: 80.3%\n",
      "\n",
      "--- Step 58000 ---\n",
      "Time elapsed: 6353.9 seconds\n",
      "Minibatch loss: 3.328046\n",
      "Minibatch accuracy: 74.5%\n",
      "Validation accuracy: 80.3%\n",
      "\n",
      "--- Step 59000 ---\n",
      "Time elapsed: 6463.1 seconds\n",
      "Minibatch loss: 3.700539\n",
      "Minibatch accuracy: 72.5%\n",
      "Validation accuracy: 80.6%\n",
      "\n",
      "--- Step 60000 ---\n",
      "Time elapsed: 6572.7 seconds\n",
      "Minibatch loss: 3.406351\n",
      "Minibatch accuracy: 75.9%\n",
      "Validation accuracy: 80.6%\n",
      "\n",
      "--- Step 61000 ---\n",
      "Time elapsed: 6682.0 seconds\n",
      "Minibatch loss: 3.953346\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 80.9%\n",
      "\n",
      "--- Step 62000 ---\n",
      "Time elapsed: 6791.4 seconds\n",
      "Minibatch loss: 3.368050\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.0%\n",
      "\n",
      "--- Step 63000 ---\n",
      "Time elapsed: 6900.6 seconds\n",
      "Minibatch loss: 3.362617\n",
      "Minibatch accuracy: 76.1%\n",
      "Validation accuracy: 81.3%\n",
      "\n",
      "--- Step 64000 ---\n",
      "Time elapsed: 7009.5 seconds\n",
      "Minibatch loss: 3.497580\n",
      "Minibatch accuracy: 73.9%\n",
      "Validation accuracy: 81.4%\n",
      "\n",
      "--- Step 65000 ---\n",
      "Time elapsed: 7118.8 seconds\n",
      "Minibatch loss: 3.874711\n",
      "Minibatch accuracy: 72.5%\n",
      "Validation accuracy: 81.5%\n",
      "\n",
      "--- Step 66000 ---\n",
      "Time elapsed: 7228.1 seconds\n",
      "Minibatch loss: 3.615062\n",
      "Minibatch accuracy: 73.8%\n",
      "Validation accuracy: 81.7%\n",
      "\n",
      "--- Step 67000 ---\n",
      "Time elapsed: 7337.3 seconds\n",
      "Minibatch loss: 3.392045\n",
      "Minibatch accuracy: 74.7%\n",
      "Validation accuracy: 81.7%\n",
      "\n",
      "--- Step 68000 ---\n",
      "Time elapsed: 7446.5 seconds\n",
      "Minibatch loss: 3.722491\n",
      "Minibatch accuracy: 74.1%\n",
      "Validation accuracy: 82.0%\n",
      "\n",
      "--- Step 69000 ---\n",
      "Time elapsed: 7555.9 seconds\n",
      "Minibatch loss: 3.327047\n",
      "Minibatch accuracy: 76.4%\n",
      "Validation accuracy: 82.1%\n",
      "\n",
      "--- Step 70000 ---\n",
      "Time elapsed: 7665.0 seconds\n",
      "Minibatch loss: 3.555609\n",
      "Minibatch accuracy: 74.8%\n",
      "Validation accuracy: 82.3%\n",
      "\n",
      "--- Step 71000 ---\n",
      "Time elapsed: 7773.9 seconds\n",
      "Minibatch loss: 3.883096\n",
      "Minibatch accuracy: 73.1%\n",
      "Validation accuracy: 82.3%\n",
      "\n",
      "--- Step 72000 ---\n",
      "Time elapsed: 7883.2 seconds\n",
      "Minibatch loss: 3.433878\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.4%\n",
      "\n",
      "--- Step 73000 ---\n",
      "Time elapsed: 7993.0 seconds\n",
      "Minibatch loss: 3.734098\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 82.7%\n",
      "\n",
      "--- Step 74000 ---\n",
      "Time elapsed: 8102.4 seconds\n",
      "Minibatch loss: 3.802382\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.7%\n",
      "\n",
      "--- Step 75000 ---\n",
      "Time elapsed: 8211.6 seconds\n",
      "Minibatch loss: 3.462471\n",
      "Minibatch accuracy: 73.9%\n",
      "Validation accuracy: 82.8%\n",
      "\n",
      "--- Step 76000 ---\n",
      "Time elapsed: 8320.9 seconds\n",
      "Minibatch loss: 3.102582\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 82.9%\n",
      "\n",
      "--- Step 77000 ---\n",
      "Time elapsed: 8429.6 seconds\n",
      "Minibatch loss: 3.579885\n",
      "Minibatch accuracy: 74.8%\n",
      "Validation accuracy: 82.9%\n",
      "\n",
      "--- Step 78000 ---\n",
      "Time elapsed: 8538.9 seconds\n",
      "Minibatch loss: 3.403753\n",
      "Minibatch accuracy: 76.4%\n",
      "Validation accuracy: 83.1%\n",
      "\n",
      "--- Step 79000 ---\n",
      "Time elapsed: 8648.7 seconds\n",
      "Minibatch loss: 3.576964\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 83.2%\n",
      "\n",
      "--- Step 80000 ---\n",
      "Time elapsed: 8758.2 seconds\n",
      "Minibatch loss: 3.118675\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 83.3%\n",
      "\n",
      "--- Step 81000 ---\n",
      "Time elapsed: 8867.2 seconds\n",
      "Minibatch loss: 3.824402\n",
      "Minibatch accuracy: 70.8%\n",
      "Validation accuracy: 83.5%\n",
      "\n",
      "--- Step 82000 ---\n",
      "Time elapsed: 8976.7 seconds\n",
      "Minibatch loss: 3.539561\n",
      "Minibatch accuracy: 76.4%\n",
      "Validation accuracy: 83.6%\n",
      "\n",
      "--- Step 83000 ---\n",
      "Time elapsed: 9086.1 seconds\n",
      "Minibatch loss: 3.723532\n",
      "Minibatch accuracy: 73.6%\n",
      "Validation accuracy: 83.7%\n",
      "\n",
      "--- Step 84000 ---\n",
      "Time elapsed: 9195.3 seconds\n",
      "Minibatch loss: 3.670103\n",
      "Minibatch accuracy: 72.5%\n",
      "Validation accuracy: 83.7%\n",
      "\n",
      "--- Step 85000 ---\n",
      "Time elapsed: 9304.4 seconds\n",
      "Minibatch loss: 3.094536\n",
      "Minibatch accuracy: 78.6%\n",
      "Validation accuracy: 83.8%\n",
      "\n",
      "--- Step 86000 ---\n",
      "Time elapsed: 9413.5 seconds\n",
      "Minibatch loss: 3.504808\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 83.9%\n",
      "\n",
      "--- Step 87000 ---\n",
      "Time elapsed: 9522.9 seconds\n",
      "Minibatch loss: 2.647522\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 83.9%\n",
      "\n",
      "--- Step 88000 ---\n",
      "Time elapsed: 9632.2 seconds\n",
      "Minibatch loss: 3.650341\n",
      "Minibatch accuracy: 73.8%\n",
      "Validation accuracy: 84.0%\n",
      "\n",
      "--- Step 89000 ---\n",
      "Time elapsed: 9741.1 seconds\n",
      "Minibatch loss: 3.294771\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 84.0%\n",
      "\n",
      "--- Step 90000 ---\n",
      "Time elapsed: 9850.4 seconds\n",
      "Minibatch loss: 3.383500\n",
      "Minibatch accuracy: 75.2%\n",
      "Validation accuracy: 84.1%\n",
      "\n",
      "--- Step 91000 ---\n",
      "Time elapsed: 9960.0 seconds\n",
      "Minibatch loss: 3.095859\n",
      "Minibatch accuracy: 78.4%\n",
      "Validation accuracy: 84.3%\n",
      "\n",
      "--- Step 92000 ---\n",
      "Time elapsed: 10069.0 seconds\n",
      "Minibatch loss: 2.923968\n",
      "Minibatch accuracy: 76.9%\n",
      "Validation accuracy: 84.4%\n",
      "\n",
      "--- Step 93000 ---\n",
      "Time elapsed: 10177.9 seconds\n",
      "Minibatch loss: 3.159782\n",
      "Minibatch accuracy: 77.7%\n",
      "Validation accuracy: 84.4%\n",
      "\n",
      "--- Step 94000 ---\n",
      "Time elapsed: 10287.0 seconds\n",
      "Minibatch loss: 3.743082\n",
      "Minibatch accuracy: 73.6%\n",
      "Validation accuracy: 84.4%\n",
      "\n",
      "--- Step 95000 ---\n",
      "Time elapsed: 10396.2 seconds\n",
      "Minibatch loss: 3.531571\n",
      "Minibatch accuracy: 75.3%\n",
      "Validation accuracy: 84.5%\n",
      "\n",
      "--- Step 96000 ---\n",
      "Time elapsed: 10505.0 seconds\n",
      "Minibatch loss: 3.176287\n",
      "Minibatch accuracy: 74.7%\n",
      "Validation accuracy: 84.5%\n",
      "\n",
      "--- Step 97000 ---\n",
      "Time elapsed: 10614.1 seconds\n",
      "Minibatch loss: 3.179522\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 84.6%\n",
      "\n",
      "--- Step 98000 ---\n",
      "Time elapsed: 10723.5 seconds\n",
      "Minibatch loss: 3.032857\n",
      "Minibatch accuracy: 75.6%\n",
      "Validation accuracy: 84.8%\n",
      "\n",
      "--- Step 99000 ---\n",
      "Time elapsed: 10833.1 seconds\n",
      "Minibatch loss: 3.285819\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 84.7%\n",
      "\n",
      "--- Step 100000 ---\n",
      "Time elapsed: 10942.5 seconds\n",
      "Minibatch loss: 3.303511\n",
      "Minibatch accuracy: 73.8%\n",
      "Validation accuracy: 84.8%\n",
      "\n",
      "--- Step 101000 ---\n",
      "Time elapsed: 11052.1 seconds\n",
      "Minibatch loss: 3.351594\n",
      "Minibatch accuracy: 75.2%\n",
      "Validation accuracy: 85.0%\n",
      "\n",
      "--- Step 102000 ---\n",
      "Time elapsed: 11161.5 seconds\n",
      "Minibatch loss: 3.341811\n",
      "Minibatch accuracy: 77.0%\n",
      "Validation accuracy: 85.0%\n",
      "\n",
      "--- Step 103000 ---\n",
      "Time elapsed: 11270.7 seconds\n",
      "Minibatch loss: 3.100675\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 85.1%\n",
      "\n",
      "--- Step 104000 ---\n",
      "Time elapsed: 11380.0 seconds\n",
      "Minibatch loss: 3.456053\n",
      "Minibatch accuracy: 75.2%\n",
      "Validation accuracy: 85.1%\n",
      "\n",
      "--- Step 105000 ---\n",
      "Time elapsed: 11489.3 seconds\n",
      "Minibatch loss: 3.414390\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.3%\n",
      "\n",
      "--- Step 106000 ---\n",
      "Time elapsed: 11598.6 seconds\n",
      "Minibatch loss: 2.884160\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 85.3%\n",
      "\n",
      "--- Step 107000 ---\n",
      "Time elapsed: 11707.7 seconds\n",
      "Minibatch loss: 3.678513\n",
      "Minibatch accuracy: 75.9%\n",
      "Validation accuracy: 85.4%\n",
      "\n",
      "--- Step 108000 ---\n",
      "Time elapsed: 11816.9 seconds\n",
      "Minibatch loss: 3.252474\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 85.5%\n",
      "\n",
      "--- Step 109000 ---\n",
      "Time elapsed: 11926.2 seconds\n",
      "Minibatch loss: 3.223047\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 85.4%\n",
      "\n",
      "--- Step 110000 ---\n",
      "Time elapsed: 12035.0 seconds\n",
      "Minibatch loss: 3.155443\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 85.5%\n",
      "\n",
      "--- Step 111000 ---\n",
      "Time elapsed: 12144.4 seconds\n",
      "Minibatch loss: 3.559799\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 85.6%\n",
      "\n",
      "--- Step 112000 ---\n",
      "Time elapsed: 12253.8 seconds\n",
      "Minibatch loss: 3.236048\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 85.6%\n",
      "\n",
      "--- Step 113000 ---\n",
      "Time elapsed: 12362.6 seconds\n",
      "Minibatch loss: 3.090747\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 85.7%\n",
      "\n",
      "--- Step 114000 ---\n",
      "Time elapsed: 12471.7 seconds\n",
      "Minibatch loss: 3.349650\n",
      "Minibatch accuracy: 75.5%\n",
      "Validation accuracy: 85.8%\n",
      "\n",
      "--- Step 115000 ---\n",
      "Time elapsed: 12580.9 seconds\n",
      "Minibatch loss: 3.331007\n",
      "Minibatch accuracy: 74.5%\n",
      "Validation accuracy: 85.6%\n",
      "\n",
      "--- Step 116000 ---\n",
      "Time elapsed: 12689.5 seconds\n",
      "Minibatch loss: 3.020400\n",
      "Minibatch accuracy: 78.8%\n",
      "Validation accuracy: 85.9%\n",
      "\n",
      "--- Step 117000 ---\n",
      "Time elapsed: 12798.0 seconds\n",
      "Minibatch loss: 3.035706\n",
      "Minibatch accuracy: 77.0%\n",
      "Validation accuracy: 85.9%\n",
      "\n",
      "--- Step 118000 ---\n",
      "Time elapsed: 12907.8 seconds\n",
      "Minibatch loss: 2.982586\n",
      "Minibatch accuracy: 79.2%\n",
      "Validation accuracy: 86.0%\n",
      "\n",
      "--- Step 119000 ---\n",
      "Time elapsed: 13017.1 seconds\n",
      "Minibatch loss: 3.445278\n",
      "Minibatch accuracy: 74.4%\n",
      "Validation accuracy: 85.8%\n",
      "\n",
      "--- Step 120000 ---\n",
      "Time elapsed: 13125.4 seconds\n",
      "Minibatch loss: 2.721058\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 86.2%\n",
      "\n",
      "--- Step 121000 ---\n",
      "Time elapsed: 13234.8 seconds\n",
      "Minibatch loss: 2.919858\n",
      "Minibatch accuracy: 79.8%\n",
      "Validation accuracy: 86.2%\n",
      "\n",
      "--- Step 122000 ---\n",
      "Time elapsed: 13343.7 seconds\n",
      "Minibatch loss: 3.144112\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 86.2%\n",
      "\n",
      "--- Step 123000 ---\n",
      "Time elapsed: 13452.9 seconds\n",
      "Minibatch loss: 3.006724\n",
      "Minibatch accuracy: 79.2%\n",
      "Validation accuracy: 86.3%\n",
      "\n",
      "--- Step 124000 ---\n",
      "Time elapsed: 13562.5 seconds\n",
      "Minibatch loss: 2.989736\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 86.4%\n",
      "\n",
      "--- Step 125000 ---\n",
      "Time elapsed: 13671.4 seconds\n",
      "Minibatch loss: 2.901999\n",
      "Minibatch accuracy: 77.7%\n",
      "Validation accuracy: 86.5%\n",
      "\n",
      "--- Step 126000 ---\n",
      "Time elapsed: 13780.1 seconds\n",
      "Minibatch loss: 3.288156\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 86.4%\n",
      "\n",
      "--- Step 127000 ---\n",
      "Time elapsed: 13889.6 seconds\n",
      "Minibatch loss: 3.017833\n",
      "Minibatch accuracy: 79.1%\n",
      "Validation accuracy: 86.5%\n",
      "\n",
      "--- Step 128000 ---\n",
      "Time elapsed: 13998.6 seconds\n",
      "Minibatch loss: 2.726364\n",
      "Minibatch accuracy: 79.4%\n",
      "Validation accuracy: 86.5%\n",
      "\n",
      "--- Step 129000 ---\n",
      "Time elapsed: 14107.7 seconds\n",
      "Minibatch loss: 2.525000\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.6%\n",
      "\n",
      "--- Step 130000 ---\n",
      "Time elapsed: 14216.7 seconds\n",
      "Minibatch loss: 3.364347\n",
      "Minibatch accuracy: 74.7%\n",
      "Validation accuracy: 86.7%\n",
      "\n",
      "--- Step 131000 ---\n",
      "Time elapsed: 14325.5 seconds\n",
      "Minibatch loss: 3.105849\n",
      "Minibatch accuracy: 77.7%\n",
      "Validation accuracy: 86.7%\n",
      "\n",
      "--- Step 132000 ---\n",
      "Time elapsed: 14434.7 seconds\n",
      "Minibatch loss: 2.923675\n",
      "Minibatch accuracy: 79.4%\n",
      "Validation accuracy: 86.6%\n",
      "\n",
      "--- Step 133000 ---\n",
      "Time elapsed: 14544.0 seconds\n",
      "Minibatch loss: 2.752743\n",
      "Minibatch accuracy: 78.8%\n",
      "Validation accuracy: 86.9%\n",
      "\n",
      "--- Step 134000 ---\n",
      "Time elapsed: 14652.9 seconds\n",
      "Minibatch loss: 2.482276\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 86.9%\n",
      "\n",
      "--- Step 135000 ---\n",
      "Time elapsed: 14762.2 seconds\n",
      "Minibatch loss: 3.306067\n",
      "Minibatch accuracy: 75.5%\n",
      "Validation accuracy: 86.9%\n",
      "\n",
      "--- Step 136000 ---\n",
      "Time elapsed: 14871.3 seconds\n",
      "Minibatch loss: 2.926707\n",
      "Minibatch accuracy: 78.0%\n",
      "Validation accuracy: 86.9%\n",
      "\n",
      "--- Step 137000 ---\n",
      "Time elapsed: 14980.8 seconds\n",
      "Minibatch loss: 3.117081\n",
      "Minibatch accuracy: 78.4%\n",
      "Validation accuracy: 87.0%\n",
      "\n",
      "--- Step 138000 ---\n",
      "Time elapsed: 15090.0 seconds\n",
      "Minibatch loss: 2.993269\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 87.0%\n",
      "\n",
      "--- Step 139000 ---\n",
      "Time elapsed: 15198.7 seconds\n",
      "Minibatch loss: 2.947113\n",
      "Minibatch accuracy: 79.4%\n",
      "Validation accuracy: 87.2%\n",
      "\n",
      "--- Step 140000 ---\n",
      "Time elapsed: 15307.6 seconds\n",
      "Minibatch loss: 3.264317\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 87.2%\n",
      "\n",
      "--- Step 141000 ---\n",
      "Time elapsed: 15417.0 seconds\n",
      "Minibatch loss: 2.749835\n",
      "Minibatch accuracy: 79.8%\n",
      "Validation accuracy: 87.3%\n",
      "\n",
      "--- Step 142000 ---\n",
      "Time elapsed: 15526.0 seconds\n",
      "Minibatch loss: 2.585537\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.3%\n",
      "\n",
      "--- Step 143000 ---\n",
      "Time elapsed: 15635.1 seconds\n",
      "Minibatch loss: 2.700537\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 87.1%\n",
      "\n",
      "--- Step 144000 ---\n",
      "Time elapsed: 15744.7 seconds\n",
      "Minibatch loss: 2.856934\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 87.5%\n",
      "\n",
      "--- Step 145000 ---\n",
      "Time elapsed: 15853.9 seconds\n",
      "Minibatch loss: 2.998665\n",
      "Minibatch accuracy: 77.7%\n",
      "Validation accuracy: 87.3%\n",
      "\n",
      "--- Step 146000 ---\n",
      "Time elapsed: 15963.0 seconds\n",
      "Minibatch loss: 2.547539\n",
      "Minibatch accuracy: 81.4%\n",
      "Validation accuracy: 87.3%\n",
      "\n",
      "--- Step 147000 ---\n",
      "Time elapsed: 16071.9 seconds\n",
      "Minibatch loss: 3.141942\n",
      "Minibatch accuracy: 76.7%\n",
      "Validation accuracy: 87.5%\n",
      "\n",
      "--- Step 148000 ---\n",
      "Time elapsed: 16181.5 seconds\n",
      "Minibatch loss: 3.217317\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 87.7%\n",
      "\n",
      "--- Step 149000 ---\n",
      "Time elapsed: 16290.3 seconds\n",
      "Minibatch loss: 2.996237\n",
      "Minibatch accuracy: 78.4%\n",
      "Validation accuracy: 87.6%\n",
      "\n",
      "--- Step 150000 ---\n",
      "Time elapsed: 16399.5 seconds\n",
      "Minibatch loss: 2.558785\n",
      "Minibatch accuracy: 80.3%\n",
      "Validation accuracy: 87.5%\n",
      "\n",
      "--- Step 151000 ---\n",
      "Time elapsed: 16508.8 seconds\n",
      "Minibatch loss: 3.028026\n",
      "Minibatch accuracy: 78.0%\n",
      "Validation accuracy: 87.7%\n",
      "\n",
      "--- Step 152000 ---\n",
      "Time elapsed: 16618.1 seconds\n",
      "Minibatch loss: 3.049199\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 87.6%\n",
      "\n",
      "--- Step 153000 ---\n",
      "Time elapsed: 16727.1 seconds\n",
      "Minibatch loss: 2.747063\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.7%\n",
      "\n",
      "--- Step 154000 ---\n",
      "Time elapsed: 16836.3 seconds\n",
      "Minibatch loss: 3.549647\n",
      "Minibatch accuracy: 75.2%\n",
      "Validation accuracy: 87.8%\n",
      "\n",
      "--- Step 155000 ---\n",
      "Time elapsed: 16945.5 seconds\n",
      "Minibatch loss: 3.262188\n",
      "Minibatch accuracy: 76.9%\n",
      "Validation accuracy: 87.9%\n",
      "\n",
      "--- Step 156000 ---\n",
      "Time elapsed: 17054.9 seconds\n",
      "Minibatch loss: 3.123691\n",
      "Minibatch accuracy: 77.0%\n",
      "Validation accuracy: 87.9%\n",
      "\n",
      "--- Step 157000 ---\n",
      "Time elapsed: 17164.5 seconds\n",
      "Minibatch loss: 2.892419\n",
      "Minibatch accuracy: 77.0%\n",
      "Validation accuracy: 87.9%\n",
      "\n",
      "--- Step 158000 ---\n",
      "Time elapsed: 17273.6 seconds\n",
      "Minibatch loss: 2.343392\n",
      "Minibatch accuracy: 83.1%\n",
      "Validation accuracy: 88.0%\n",
      "\n",
      "--- Step 159000 ---\n",
      "Time elapsed: 17382.9 seconds\n",
      "Minibatch loss: 3.299603\n",
      "Minibatch accuracy: 76.1%\n",
      "Validation accuracy: 87.8%\n",
      "\n",
      "--- Step 160000 ---\n",
      "Time elapsed: 17493.1 seconds\n",
      "Minibatch loss: 3.078335\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 87.9%\n",
      "\n",
      "--- Step 161000 ---\n",
      "Time elapsed: 17602.5 seconds\n",
      "Minibatch loss: 3.001184\n",
      "Minibatch accuracy: 78.3%\n",
      "Validation accuracy: 88.0%\n",
      "\n",
      "--- Step 162000 ---\n",
      "Time elapsed: 17711.7 seconds\n",
      "Minibatch loss: 1.991637\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 88.1%\n",
      "\n",
      "--- Step 163000 ---\n",
      "Time elapsed: 17820.6 seconds\n",
      "Minibatch loss: 3.378706\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 88.1%\n",
      "\n",
      "--- Step 164000 ---\n",
      "Time elapsed: 17930.0 seconds\n",
      "Minibatch loss: 2.889625\n",
      "Minibatch accuracy: 79.5%\n",
      "Validation accuracy: 88.2%\n",
      "\n",
      "--- Step 165000 ---\n",
      "Time elapsed: 18039.3 seconds\n",
      "Minibatch loss: 3.601682\n",
      "Minibatch accuracy: 75.6%\n",
      "Validation accuracy: 88.2%\n",
      "\n",
      "--- Step 166000 ---\n",
      "Time elapsed: 18149.0 seconds\n",
      "Minibatch loss: 2.722797\n",
      "Minibatch accuracy: 80.6%\n",
      "Validation accuracy: 88.2%\n",
      "\n",
      "--- Step 167000 ---\n",
      "Time elapsed: 18259.1 seconds\n",
      "Minibatch loss: 2.468139\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 88.3%\n",
      "\n",
      "--- Step 168000 ---\n",
      "Time elapsed: 18368.5 seconds\n",
      "Minibatch loss: 2.933084\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 88.5%\n",
      "\n",
      "--- Step 169000 ---\n",
      "Time elapsed: 18477.6 seconds\n",
      "Minibatch loss: 2.649758\n",
      "Minibatch accuracy: 81.1%\n",
      "Validation accuracy: 88.4%\n",
      "\n",
      "--- Step 170000 ---\n",
      "Time elapsed: 18586.8 seconds\n",
      "Minibatch loss: 3.010160\n",
      "Minibatch accuracy: 78.3%\n",
      "Validation accuracy: 88.3%\n",
      "\n",
      "--- Step 171000 ---\n",
      "Time elapsed: 18696.3 seconds\n",
      "Minibatch loss: 2.912429\n",
      "Minibatch accuracy: 79.1%\n",
      "Validation accuracy: 88.4%\n",
      "\n",
      "--- Step 172000 ---\n",
      "Time elapsed: 18805.1 seconds\n",
      "Minibatch loss: 2.312288\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 88.4%\n",
      "\n",
      "--- Step 173000 ---\n",
      "Time elapsed: 18914.5 seconds\n",
      "Minibatch loss: 2.527901\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 88.4%\n",
      "\n",
      "--- Step 174000 ---\n",
      "Time elapsed: 19023.9 seconds\n",
      "Minibatch loss: 3.123688\n",
      "Minibatch accuracy: 79.1%\n",
      "Validation accuracy: 88.5%\n",
      "\n",
      "--- Step 175000 ---\n",
      "Time elapsed: 19132.7 seconds\n",
      "Minibatch loss: 3.158221\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "\n",
      "--- Step 176000 ---\n",
      "Time elapsed: 19242.7 seconds\n",
      "Minibatch loss: 2.597591\n",
      "Minibatch accuracy: 81.4%\n",
      "Validation accuracy: 88.6%\n",
      "\n",
      "--- Step 177000 ---\n",
      "Time elapsed: 19351.9 seconds\n",
      "Minibatch loss: 3.312057\n",
      "Minibatch accuracy: 77.7%\n",
      "Validation accuracy: 88.5%\n",
      "\n",
      "--- Step 178000 ---\n",
      "Time elapsed: 19461.0 seconds\n",
      "Minibatch loss: 2.979232\n",
      "Minibatch accuracy: 78.8%\n",
      "Validation accuracy: 88.6%\n",
      "\n",
      "--- Step 179000 ---\n",
      "Time elapsed: 19570.5 seconds\n",
      "Minibatch loss: 2.600412\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 88.7%\n",
      "\n",
      "--- Step 180000 ---\n",
      "Time elapsed: 19679.5 seconds\n",
      "Minibatch loss: 2.019797\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 88.6%\n",
      "\n",
      "--- Step 181000 ---\n",
      "Time elapsed: 19789.1 seconds\n",
      "Minibatch loss: 2.459019\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 88.7%\n",
      "\n",
      "--- Step 182000 ---\n",
      "Time elapsed: 19898.0 seconds\n",
      "Minibatch loss: 2.579876\n",
      "Minibatch accuracy: 83.3%\n",
      "Validation accuracy: 88.8%\n",
      "\n",
      "--- Step 183000 ---\n",
      "Time elapsed: 20007.0 seconds\n",
      "Minibatch loss: 2.563405\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 88.9%\n",
      "\n",
      "--- Step 184000 ---\n",
      "Time elapsed: 20116.0 seconds\n",
      "Minibatch loss: 2.374102\n",
      "Minibatch accuracy: 82.7%\n",
      "Validation accuracy: 88.8%\n",
      "\n",
      "--- Step 185000 ---\n",
      "Time elapsed: 20225.9 seconds\n",
      "Minibatch loss: 3.007026\n",
      "Minibatch accuracy: 77.5%\n",
      "Validation accuracy: 88.7%\n",
      "\n",
      "--- Step 186000 ---\n",
      "Time elapsed: 20335.0 seconds\n",
      "Minibatch loss: 2.353314\n",
      "Minibatch accuracy: 84.7%\n",
      "Validation accuracy: 88.9%\n",
      "\n",
      "--- Step 187000 ---\n",
      "Time elapsed: 20444.2 seconds\n",
      "Minibatch loss: 2.333335\n",
      "Minibatch accuracy: 81.9%\n",
      "Validation accuracy: 88.6%\n",
      "\n",
      "--- Step 188000 ---\n",
      "Time elapsed: 20553.2 seconds\n",
      "Minibatch loss: 2.337862\n",
      "Minibatch accuracy: 84.5%\n",
      "Validation accuracy: 89.1%\n",
      "\n",
      "--- Step 189000 ---\n",
      "Time elapsed: 20662.2 seconds\n",
      "Minibatch loss: 2.454356\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 88.9%\n",
      "\n",
      "--- Step 190000 ---\n",
      "Time elapsed: 20771.6 seconds\n",
      "Minibatch loss: 2.525281\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.0%\n",
      "\n",
      "--- Step 191000 ---\n",
      "Time elapsed: 20880.7 seconds\n",
      "Minibatch loss: 2.711903\n",
      "Minibatch accuracy: 81.4%\n",
      "Validation accuracy: 89.1%\n",
      "\n",
      "--- Step 192000 ---\n",
      "Time elapsed: 20989.5 seconds\n",
      "Minibatch loss: 2.461602\n",
      "Minibatch accuracy: 80.2%\n",
      "Validation accuracy: 89.1%\n",
      "\n",
      "--- Step 193000 ---\n",
      "Time elapsed: 21098.4 seconds\n",
      "Minibatch loss: 2.792299\n",
      "Minibatch accuracy: 79.5%\n",
      "Validation accuracy: 89.2%\n",
      "\n",
      "--- Step 194000 ---\n",
      "Time elapsed: 21207.2 seconds\n",
      "Minibatch loss: 3.093204\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 89.1%\n",
      "\n",
      "--- Step 195000 ---\n",
      "Time elapsed: 21316.6 seconds\n",
      "Minibatch loss: 2.697156\n",
      "Minibatch accuracy: 80.8%\n",
      "Validation accuracy: 89.2%\n",
      "\n",
      "--- Step 196000 ---\n",
      "Time elapsed: 21426.0 seconds\n",
      "Minibatch loss: 2.671411\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 89.2%\n",
      "\n",
      "--- Step 197000 ---\n",
      "Time elapsed: 21535.3 seconds\n",
      "Minibatch loss: 2.949455\n",
      "Minibatch accuracy: 78.6%\n",
      "Validation accuracy: 89.2%\n",
      "\n",
      "--- Step 198000 ---\n",
      "Time elapsed: 21644.2 seconds\n",
      "Minibatch loss: 2.643589\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 89.3%\n",
      "\n",
      "--- Step 199000 ---\n",
      "Time elapsed: 21753.2 seconds\n",
      "Minibatch loss: 2.507764\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.3%\n",
      "\n",
      "--- Step 200000 ---\n",
      "Time elapsed: 21862.3 seconds\n",
      "Minibatch loss: 2.829165\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 89.3%\n",
      "\n",
      "--- Step 201000 ---\n",
      "Time elapsed: 21971.7 seconds\n",
      "Minibatch loss: 2.520789\n",
      "Minibatch accuracy: 81.4%\n",
      "Validation accuracy: 89.4%\n",
      "\n",
      "--- Step 202000 ---\n",
      "Time elapsed: 22081.1 seconds\n",
      "Minibatch loss: 2.699746\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 89.3%\n",
      "\n",
      "--- Step 203000 ---\n",
      "Time elapsed: 22190.4 seconds\n",
      "Minibatch loss: 2.368881\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 89.5%\n",
      "\n",
      "--- Step 204000 ---\n",
      "Time elapsed: 22300.4 seconds\n",
      "Minibatch loss: 2.799972\n",
      "Minibatch accuracy: 79.5%\n",
      "Validation accuracy: 89.4%\n",
      "\n",
      "--- Step 205000 ---\n",
      "Time elapsed: 22409.5 seconds\n",
      "Minibatch loss: 2.516086\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 89.4%\n",
      "\n",
      "--- Step 206000 ---\n",
      "Time elapsed: 22518.8 seconds\n",
      "Minibatch loss: 2.448014\n",
      "Minibatch accuracy: 81.4%\n",
      "Validation accuracy: 89.6%\n",
      "\n",
      "--- Step 207000 ---\n",
      "Time elapsed: 22628.5 seconds\n",
      "Minibatch loss: 2.696688\n",
      "Minibatch accuracy: 81.1%\n",
      "Validation accuracy: 89.6%\n",
      "\n",
      "--- Step 208000 ---\n",
      "Time elapsed: 22738.6 seconds\n",
      "Minibatch loss: 2.583476\n",
      "Minibatch accuracy: 80.6%\n",
      "Validation accuracy: 89.5%\n",
      "\n",
      "--- Step 209000 ---\n",
      "Time elapsed: 22848.9 seconds\n",
      "Minibatch loss: 2.687474\n",
      "Minibatch accuracy: 80.6%\n",
      "Validation accuracy: 89.5%\n",
      "\n",
      "--- Step 210000 ---\n",
      "Time elapsed: 22957.8 seconds\n",
      "Minibatch loss: 2.332852\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.5%\n",
      "\n",
      "--- Step 211000 ---\n",
      "Time elapsed: 23066.8 seconds\n",
      "Minibatch loss: 2.351474\n",
      "Minibatch accuracy: 83.3%\n",
      "Validation accuracy: 89.5%\n",
      "\n",
      "--- Step 212000 ---\n",
      "Time elapsed: 23176.0 seconds\n",
      "Minibatch loss: 2.920525\n",
      "Minibatch accuracy: 78.4%\n",
      "Validation accuracy: 89.6%\n",
      "\n",
      "--- Step 213000 ---\n",
      "Time elapsed: 23285.3 seconds\n",
      "Minibatch loss: 3.017912\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 89.6%\n",
      "\n",
      "--- Step 214000 ---\n",
      "Time elapsed: 23394.2 seconds\n",
      "Minibatch loss: 2.525997\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 89.6%\n",
      "\n",
      "--- Step 215000 ---\n",
      "Time elapsed: 23503.2 seconds\n",
      "Minibatch loss: 2.139472\n",
      "Minibatch accuracy: 84.5%\n",
      "Validation accuracy: 89.7%\n",
      "\n",
      "--- Step 216000 ---\n",
      "Time elapsed: 23612.1 seconds\n",
      "Minibatch loss: 2.129240\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.8%\n",
      "\n",
      "--- Step 217000 ---\n",
      "Time elapsed: 23721.5 seconds\n",
      "Minibatch loss: 2.561933\n",
      "Minibatch accuracy: 81.9%\n",
      "Validation accuracy: 89.6%\n",
      "\n",
      "--- Step 218000 ---\n",
      "Time elapsed: 23831.6 seconds\n",
      "Minibatch loss: 2.595423\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 89.8%\n",
      "\n",
      "--- Step 219000 ---\n",
      "Time elapsed: 23941.1 seconds\n",
      "Minibatch loss: 2.740384\n",
      "Minibatch accuracy: 79.5%\n",
      "Validation accuracy: 89.8%\n",
      "\n",
      "--- Step 220000 ---\n",
      "Time elapsed: 24049.8 seconds\n",
      "Minibatch loss: 2.277179\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 89.9%\n",
      "\n",
      "--- Step 221000 ---\n",
      "Time elapsed: 24158.7 seconds\n",
      "Minibatch loss: 2.385311\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 89.8%\n",
      "\n",
      "--- Step 222000 ---\n",
      "Time elapsed: 24267.6 seconds\n",
      "Minibatch loss: 2.310980\n",
      "Minibatch accuracy: 84.2%\n",
      "Validation accuracy: 89.9%\n",
      "\n",
      "--- Step 223000 ---\n",
      "Time elapsed: 24376.9 seconds\n",
      "Minibatch loss: 3.091905\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 89.9%\n",
      "\n",
      "--- Step 224000 ---\n",
      "Time elapsed: 24486.1 seconds\n",
      "Minibatch loss: 2.243631\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 90.0%\n",
      "\n",
      "--- Step 225000 ---\n",
      "Time elapsed: 24595.3 seconds\n",
      "Minibatch loss: 2.343750\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 90.0%\n",
      "\n",
      "--- Step 226000 ---\n",
      "Time elapsed: 24704.5 seconds\n",
      "Minibatch loss: 3.298024\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 90.1%\n",
      "\n",
      "--- Step 227000 ---\n",
      "Time elapsed: 24813.5 seconds\n",
      "Minibatch loss: 1.803168\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 90.0%\n",
      "\n",
      "--- Step 228000 ---\n",
      "Time elapsed: 24923.0 seconds\n",
      "Minibatch loss: 2.058744\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 90.0%\n",
      "\n",
      "--- Step 229000 ---\n",
      "Time elapsed: 25032.0 seconds\n",
      "Minibatch loss: 1.913347\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 90.1%\n",
      "\n",
      "--- Step 230000 ---\n",
      "Time elapsed: 25141.0 seconds\n",
      "Minibatch loss: 2.511898\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 90.0%\n",
      "\n",
      "--- Step 231000 ---\n",
      "Time elapsed: 25249.9 seconds\n",
      "Minibatch loss: 2.547350\n",
      "Minibatch accuracy: 81.1%\n",
      "Validation accuracy: 90.1%\n",
      "\n",
      "--- Step 232000 ---\n",
      "Time elapsed: 25358.8 seconds\n",
      "Minibatch loss: 2.746615\n",
      "Minibatch accuracy: 79.8%\n",
      "Validation accuracy: 90.2%\n",
      "\n",
      "--- Step 233000 ---\n",
      "Time elapsed: 25468.1 seconds\n",
      "Minibatch loss: 2.645279\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 90.2%\n",
      "\n",
      "--- Step 234000 ---\n",
      "Time elapsed: 25577.2 seconds\n",
      "Minibatch loss: 1.931152\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 90.3%\n",
      "\n",
      "--- Step 235000 ---\n",
      "Time elapsed: 25686.4 seconds\n",
      "Minibatch loss: 2.748178\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 90.2%\n",
      "\n",
      "--- Step 236000 ---\n",
      "Time elapsed: 25796.0 seconds\n",
      "Minibatch loss: 2.241683\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 90.3%\n",
      "\n",
      "--- Step 237000 ---\n",
      "Time elapsed: 25905.0 seconds\n",
      "Minibatch loss: 2.198852\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 90.1%\n",
      "\n",
      "--- Step 238000 ---\n",
      "Time elapsed: 26013.8 seconds\n",
      "Minibatch loss: 2.103364\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 90.3%\n",
      "\n",
      "--- Step 239000 ---\n",
      "Time elapsed: 26123.0 seconds\n",
      "Minibatch loss: 2.048407\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 90.1%\n",
      "\n",
      "--- Step 240000 ---\n",
      "Time elapsed: 26232.0 seconds\n",
      "Minibatch loss: 2.683085\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 90.2%\n",
      "\n",
      "--- Step 241000 ---\n",
      "Time elapsed: 26340.9 seconds\n",
      "Minibatch loss: 2.385352\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 90.3%\n",
      "\n",
      "--- Step 242000 ---\n",
      "Time elapsed: 26450.2 seconds\n",
      "Minibatch loss: 2.843951\n",
      "Minibatch accuracy: 81.1%\n",
      "Validation accuracy: 90.3%\n",
      "\n",
      "--- Step 243000 ---\n",
      "Time elapsed: 26559.3 seconds\n",
      "Minibatch loss: 2.750874\n",
      "Minibatch accuracy: 78.3%\n",
      "Validation accuracy: 90.4%\n",
      "\n",
      "--- Step 244000 ---\n",
      "Time elapsed: 26668.4 seconds\n",
      "Minibatch loss: 2.336481\n",
      "Minibatch accuracy: 83.1%\n",
      "Validation accuracy: 90.4%\n",
      "\n",
      "--- Step 245000 ---\n",
      "Time elapsed: 26777.6 seconds\n",
      "Minibatch loss: 2.277934\n",
      "Minibatch accuracy: 83.3%\n",
      "Validation accuracy: 90.5%\n",
      "\n",
      "--- Step 246000 ---\n",
      "Time elapsed: 26886.7 seconds\n",
      "Minibatch loss: 2.192204\n",
      "Minibatch accuracy: 84.5%\n",
      "Validation accuracy: 90.5%\n",
      "\n",
      "--- Step 247000 ---\n",
      "Time elapsed: 26995.8 seconds\n",
      "Minibatch loss: 2.283624\n",
      "Minibatch accuracy: 83.1%\n",
      "Validation accuracy: 90.5%\n",
      "\n",
      "--- Step 248000 ---\n",
      "Time elapsed: 27104.7 seconds\n",
      "Minibatch loss: 2.570128\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 90.5%\n",
      "\n",
      "--- Step 249000 ---\n",
      "Time elapsed: 27213.7 seconds\n",
      "Minibatch loss: 2.520693\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 90.6%\n",
      "\n",
      "--- Step 250000 ---\n",
      "Time elapsed: 27322.9 seconds\n",
      "Minibatch loss: 2.313386\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 90.4%\n",
      "\n",
      "--- Step 251000 ---\n",
      "Time elapsed: 27432.2 seconds\n",
      "Minibatch loss: 2.777021\n",
      "Minibatch accuracy: 80.2%\n",
      "Validation accuracy: 90.5%\n",
      "\n",
      "--- Step 252000 ---\n",
      "Time elapsed: 27541.5 seconds\n",
      "Minibatch loss: 2.371321\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 90.6%\n",
      "\n",
      "--- Step 253000 ---\n",
      "Time elapsed: 27650.8 seconds\n",
      "Minibatch loss: 2.555686\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 90.6%\n",
      "\n",
      "--- Step 254000 ---\n",
      "Time elapsed: 27760.1 seconds\n",
      "Minibatch loss: 2.127942\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 90.5%\n",
      "\n",
      "--- Step 255000 ---\n",
      "Time elapsed: 27869.2 seconds\n",
      "Minibatch loss: 2.126033\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 90.7%\n",
      "\n",
      "--- Step 256000 ---\n",
      "Time elapsed: 27978.3 seconds\n",
      "Minibatch loss: 2.136305\n",
      "Minibatch accuracy: 84.5%\n",
      "Validation accuracy: 90.7%\n",
      "\n",
      "--- Step 257000 ---\n",
      "Time elapsed: 28087.3 seconds\n",
      "Minibatch loss: 2.681242\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 90.7%\n",
      "\n",
      "--- Step 258000 ---\n",
      "Time elapsed: 28196.3 seconds\n",
      "Minibatch loss: 2.647779\n",
      "Minibatch accuracy: 79.5%\n",
      "Validation accuracy: 90.7%\n",
      "\n",
      "--- Step 259000 ---\n",
      "Time elapsed: 28305.2 seconds\n",
      "Minibatch loss: 1.676727\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 90.6%\n",
      "\n",
      "--- Step 260000 ---\n",
      "Time elapsed: 28414.3 seconds\n",
      "Minibatch loss: 2.544936\n",
      "Minibatch accuracy: 81.1%\n",
      "Validation accuracy: 90.7%\n",
      "\n",
      "--- Step 261000 ---\n",
      "Time elapsed: 28523.3 seconds\n",
      "Minibatch loss: 2.426288\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 90.6%\n",
      "\n",
      "--- Step 262000 ---\n",
      "Time elapsed: 28632.0 seconds\n",
      "Minibatch loss: 2.351807\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 90.7%\n",
      "\n",
      "--- Step 263000 ---\n",
      "Time elapsed: 28741.6 seconds\n",
      "Minibatch loss: 2.221329\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 90.8%\n",
      "\n",
      "--- Step 264000 ---\n",
      "Time elapsed: 28851.6 seconds\n",
      "Minibatch loss: 2.761632\n",
      "Minibatch accuracy: 79.8%\n",
      "Validation accuracy: 90.9%\n",
      "\n",
      "--- Step 265000 ---\n",
      "Time elapsed: 28960.6 seconds\n",
      "Minibatch loss: 2.472519\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 90.8%\n",
      "\n",
      "--- Step 266000 ---\n",
      "Time elapsed: 29069.9 seconds\n",
      "Minibatch loss: 2.587602\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 90.8%\n",
      "\n",
      "--- Step 267000 ---\n",
      "Time elapsed: 29179.5 seconds\n",
      "Minibatch loss: 2.189906\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 90.8%\n",
      "\n",
      "--- Step 268000 ---\n",
      "Time elapsed: 29289.4 seconds\n",
      "Minibatch loss: 2.179197\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 90.8%\n",
      "\n",
      "--- Step 269000 ---\n",
      "Time elapsed: 29398.5 seconds\n",
      "Minibatch loss: 2.202808\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 90.8%\n",
      "\n",
      "--- Step 270000 ---\n",
      "Time elapsed: 29508.6 seconds\n",
      "Minibatch loss: 1.954758\n",
      "Minibatch accuracy: 84.7%\n",
      "Validation accuracy: 90.9%\n",
      "\n",
      "--- Step 271000 ---\n",
      "Time elapsed: 29618.2 seconds\n",
      "Minibatch loss: 1.953787\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 90.9%\n",
      "\n",
      "--- Step 272000 ---\n",
      "Time elapsed: 29727.7 seconds\n",
      "Minibatch loss: 2.138161\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 90.8%\n",
      "\n",
      "--- Step 273000 ---\n",
      "Time elapsed: 29836.8 seconds\n",
      "Minibatch loss: 2.435108\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 91.0%\n",
      "\n",
      "--- Step 274000 ---\n",
      "Time elapsed: 29946.0 seconds\n",
      "Minibatch loss: 2.054184\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 90.9%\n",
      "\n",
      "--- Step 275000 ---\n",
      "Time elapsed: 30055.2 seconds\n",
      "Minibatch loss: 2.324111\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 90.8%\n",
      "\n",
      "--- Step 276000 ---\n",
      "Time elapsed: 30164.5 seconds\n",
      "Minibatch loss: 2.167037\n",
      "Minibatch accuracy: 83.9%\n",
      "Validation accuracy: 91.0%\n",
      "\n",
      "--- Step 277000 ---\n",
      "Time elapsed: 30273.7 seconds\n",
      "Minibatch loss: 2.398530\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 91.0%\n",
      "\n",
      "--- Step 278000 ---\n",
      "Time elapsed: 30382.9 seconds\n",
      "Minibatch loss: 2.517923\n",
      "Minibatch accuracy: 82.7%\n",
      "Validation accuracy: 90.9%\n",
      "\n",
      "--- Step 279000 ---\n",
      "Time elapsed: 30491.9 seconds\n",
      "Minibatch loss: 2.134111\n",
      "Minibatch accuracy: 83.3%\n",
      "Validation accuracy: 91.0%\n",
      "\n",
      "--- Step 280000 ---\n",
      "Time elapsed: 30600.9 seconds\n",
      "Minibatch loss: 2.065513\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 91.0%\n",
      "\n",
      "--- Step 281000 ---\n",
      "Time elapsed: 30709.9 seconds\n",
      "Minibatch loss: 2.462565\n",
      "Minibatch accuracy: 81.1%\n",
      "Validation accuracy: 91.0%\n",
      "\n",
      "--- Step 282000 ---\n",
      "Time elapsed: 30819.3 seconds\n",
      "Minibatch loss: 2.010878\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 90.9%\n",
      "\n",
      "--- Step 283000 ---\n",
      "Time elapsed: 30928.8 seconds\n",
      "Minibatch loss: 2.062280\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 91.1%\n",
      "\n",
      "--- Step 284000 ---\n",
      "Time elapsed: 31037.6 seconds\n",
      "Minibatch loss: 2.374774\n",
      "Minibatch accuracy: 81.7%\n",
      "Validation accuracy: 90.9%\n",
      "\n",
      "--- Step 285000 ---\n",
      "Time elapsed: 31146.9 seconds\n",
      "Minibatch loss: 2.239943\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 91.1%\n",
      "\n",
      "--- Step 286000 ---\n",
      "Time elapsed: 31255.9 seconds\n",
      "Minibatch loss: 2.080768\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 91.1%\n",
      "\n",
      "--- Step 287000 ---\n",
      "Time elapsed: 31364.7 seconds\n",
      "Minibatch loss: 1.978912\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 91.1%\n",
      "\n",
      "--- Step 288000 ---\n",
      "Time elapsed: 31473.6 seconds\n",
      "Minibatch loss: 2.171391\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 91.1%\n",
      "\n",
      "--- Step 289000 ---\n",
      "Time elapsed: 31582.8 seconds\n",
      "Minibatch loss: 1.824959\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 91.1%\n",
      "\n",
      "--- Step 290000 ---\n",
      "Time elapsed: 31691.6 seconds\n",
      "Minibatch loss: 1.879123\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 91.3%\n",
      "\n",
      "--- Step 291000 ---\n",
      "Time elapsed: 31800.8 seconds\n",
      "Minibatch loss: 2.201372\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 91.2%\n",
      "\n",
      "--- Step 292000 ---\n",
      "Time elapsed: 31909.9 seconds\n",
      "Minibatch loss: 2.239258\n",
      "Minibatch accuracy: 83.3%\n",
      "Validation accuracy: 91.2%\n",
      "\n",
      "--- Step 293000 ---\n",
      "Time elapsed: 32018.8 seconds\n",
      "Minibatch loss: 2.280519\n",
      "Minibatch accuracy: 84.2%\n",
      "Validation accuracy: 91.1%\n",
      "\n",
      "--- Step 294000 ---\n",
      "Time elapsed: 32128.2 seconds\n",
      "Minibatch loss: 2.261098\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.2%\n",
      "\n",
      "--- Step 295000 ---\n",
      "Time elapsed: 32237.9 seconds\n",
      "Minibatch loss: 2.028038\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 91.2%\n",
      "\n",
      "--- Step 296000 ---\n",
      "Time elapsed: 32347.2 seconds\n",
      "Minibatch loss: 2.151507\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 91.2%\n",
      "\n",
      "--- Step 297000 ---\n",
      "Time elapsed: 32456.2 seconds\n",
      "Minibatch loss: 2.117702\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 91.3%\n",
      "\n",
      "--- Step 298000 ---\n",
      "Time elapsed: 32565.1 seconds\n",
      "Minibatch loss: 2.167169\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 91.2%\n",
      "\n",
      "--- Step 299000 ---\n",
      "Time elapsed: 32674.1 seconds\n",
      "Minibatch loss: 1.892318\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.3%\n",
      "\n",
      "--- Step 300000 ---\n",
      "Time elapsed: 32783.6 seconds\n",
      "Minibatch loss: 2.341475\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 91.3%\n",
      "\n",
      "--- Step 301000 ---\n",
      "Time elapsed: 32892.7 seconds\n",
      "Minibatch loss: 2.495523\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 91.2%\n",
      "\n",
      "--- Step 302000 ---\n",
      "Time elapsed: 33001.3 seconds\n",
      "Minibatch loss: 2.064412\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 91.3%\n",
      "\n",
      "--- Step 303000 ---\n",
      "Time elapsed: 33110.4 seconds\n",
      "Minibatch loss: 2.625055\n",
      "Minibatch accuracy: 81.7%\n",
      "Validation accuracy: 91.4%\n",
      "\n",
      "--- Step 304000 ---\n",
      "Time elapsed: 33219.1 seconds\n",
      "Minibatch loss: 1.832854\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.4%\n",
      "\n",
      "--- Step 305000 ---\n",
      "Time elapsed: 33328.1 seconds\n",
      "Minibatch loss: 2.371896\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 91.4%\n",
      "\n",
      "--- Step 306000 ---\n",
      "Time elapsed: 33437.1 seconds\n",
      "Minibatch loss: 2.871758\n",
      "Minibatch accuracy: 79.2%\n",
      "Validation accuracy: 91.2%\n",
      "\n",
      "--- Step 307000 ---\n",
      "Time elapsed: 33546.0 seconds\n",
      "Minibatch loss: 2.301624\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 91.4%\n",
      "\n",
      "--- Step 308000 ---\n",
      "Time elapsed: 33655.3 seconds\n",
      "Minibatch loss: 1.981879\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 91.3%\n",
      "\n",
      "--- Step 309000 ---\n",
      "Time elapsed: 33764.2 seconds\n",
      "Minibatch loss: 2.367444\n",
      "Minibatch accuracy: 81.7%\n",
      "Validation accuracy: 91.3%\n",
      "\n",
      "--- Step 310000 ---\n",
      "Time elapsed: 33873.3 seconds\n",
      "Minibatch loss: 2.146992\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 91.4%\n",
      "\n",
      "--- Step 311000 ---\n",
      "Time elapsed: 33982.6 seconds\n",
      "Minibatch loss: 1.994765\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.3%\n",
      "\n",
      "--- Step 312000 ---\n",
      "Time elapsed: 34091.4 seconds\n",
      "Minibatch loss: 2.370954\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 91.5%\n",
      "\n",
      "--- Step 313000 ---\n",
      "Time elapsed: 34200.4 seconds\n",
      "Minibatch loss: 2.099850\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 91.5%\n",
      "\n",
      "--- Step 314000 ---\n",
      "Time elapsed: 34310.1 seconds\n",
      "Minibatch loss: 2.415674\n",
      "Minibatch accuracy: 83.3%\n",
      "Validation accuracy: 91.4%\n",
      "\n",
      "--- Step 315000 ---\n",
      "Time elapsed: 34419.1 seconds\n",
      "Minibatch loss: 1.783575\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 91.4%\n",
      "\n",
      "--- Step 316000 ---\n",
      "Time elapsed: 34528.3 seconds\n",
      "Minibatch loss: 2.142608\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 91.6%\n",
      "\n",
      "--- Step 317000 ---\n",
      "Time elapsed: 34638.1 seconds\n",
      "Minibatch loss: 1.953881\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.4%\n",
      "\n",
      "--- Step 318000 ---\n",
      "Time elapsed: 34747.2 seconds\n",
      "Minibatch loss: 2.129536\n",
      "Minibatch accuracy: 84.5%\n",
      "Validation accuracy: 91.6%\n",
      "\n",
      "--- Step 319000 ---\n",
      "Time elapsed: 34856.6 seconds\n",
      "Minibatch loss: 2.123986\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 91.5%\n",
      "\n",
      "--- Step 320000 ---\n",
      "Time elapsed: 34965.5 seconds\n",
      "Minibatch loss: 2.205064\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 91.5%\n",
      "\n",
      "--- Step 321000 ---\n",
      "Time elapsed: 35074.7 seconds\n",
      "Minibatch loss: 2.207926\n",
      "Minibatch accuracy: 84.2%\n",
      "Validation accuracy: 91.5%\n",
      "\n",
      "--- Step 322000 ---\n",
      "Time elapsed: 35184.1 seconds\n",
      "Minibatch loss: 2.422898\n",
      "Minibatch accuracy: 82.7%\n",
      "Validation accuracy: 91.5%\n",
      "\n",
      "--- Step 323000 ---\n",
      "Time elapsed: 35293.4 seconds\n",
      "Minibatch loss: 2.340686\n",
      "Minibatch accuracy: 83.3%\n",
      "Validation accuracy: 91.5%\n",
      "\n",
      "--- Step 324000 ---\n",
      "Time elapsed: 35402.8 seconds\n",
      "Minibatch loss: 2.470901\n",
      "Minibatch accuracy: 81.7%\n",
      "Validation accuracy: 91.6%\n",
      "\n",
      "--- Step 325000 ---\n",
      "Time elapsed: 35512.1 seconds\n",
      "Minibatch loss: 1.901918\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 326000 ---\n",
      "Time elapsed: 35621.8 seconds\n",
      "Minibatch loss: 2.180910\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 91.5%\n",
      "\n",
      "--- Step 327000 ---\n",
      "Time elapsed: 35730.9 seconds\n",
      "Minibatch loss: 1.845690\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 328000 ---\n",
      "Time elapsed: 35840.2 seconds\n",
      "Minibatch loss: 1.929881\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 91.6%\n",
      "\n",
      "--- Step 329000 ---\n",
      "Time elapsed: 35949.3 seconds\n",
      "Minibatch loss: 2.003505\n",
      "Minibatch accuracy: 84.5%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 330000 ---\n",
      "Time elapsed: 36058.6 seconds\n",
      "Minibatch loss: 2.451341\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 91.6%\n",
      "\n",
      "--- Step 331000 ---\n",
      "Time elapsed: 36167.9 seconds\n",
      "Minibatch loss: 2.009707\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 91.6%\n",
      "\n",
      "--- Step 332000 ---\n",
      "Time elapsed: 36277.3 seconds\n",
      "Minibatch loss: 2.486895\n",
      "Minibatch accuracy: 81.7%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 333000 ---\n",
      "Time elapsed: 36386.8 seconds\n",
      "Minibatch loss: 2.065299\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 334000 ---\n",
      "Time elapsed: 36495.7 seconds\n",
      "Minibatch loss: 1.874545\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 335000 ---\n",
      "Time elapsed: 36605.0 seconds\n",
      "Minibatch loss: 2.581623\n",
      "Minibatch accuracy: 81.7%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 336000 ---\n",
      "Time elapsed: 36714.1 seconds\n",
      "Minibatch loss: 2.408838\n",
      "Minibatch accuracy: 83.1%\n",
      "Validation accuracy: 91.8%\n",
      "\n",
      "--- Step 337000 ---\n",
      "Time elapsed: 36823.2 seconds\n",
      "Minibatch loss: 1.586619\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 338000 ---\n",
      "Time elapsed: 36932.4 seconds\n",
      "Minibatch loss: 2.144291\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.8%\n",
      "\n",
      "--- Step 339000 ---\n",
      "Time elapsed: 37041.4 seconds\n",
      "Minibatch loss: 2.073310\n",
      "Minibatch accuracy: 83.9%\n",
      "Validation accuracy: 91.6%\n",
      "\n",
      "--- Step 340000 ---\n",
      "Time elapsed: 37150.5 seconds\n",
      "Minibatch loss: 2.574358\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 341000 ---\n",
      "Time elapsed: 37259.5 seconds\n",
      "Minibatch loss: 1.893023\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 342000 ---\n",
      "Time elapsed: 37373.1 seconds\n",
      "Minibatch loss: 1.862344\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 91.6%\n",
      "\n",
      "--- Step 343000 ---\n",
      "Time elapsed: 37489.0 seconds\n",
      "Minibatch loss: 2.683373\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 91.8%\n",
      "\n",
      "--- Step 344000 ---\n",
      "Time elapsed: 37599.2 seconds\n",
      "Minibatch loss: 1.966249\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 345000 ---\n",
      "Time elapsed: 37708.9 seconds\n",
      "Minibatch loss: 2.053005\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 91.8%\n",
      "\n",
      "--- Step 346000 ---\n",
      "Time elapsed: 37818.6 seconds\n",
      "Minibatch loss: 1.874772\n",
      "Minibatch accuracy: 85.6%\n",
      "Validation accuracy: 91.9%\n",
      "\n",
      "--- Step 347000 ---\n",
      "Time elapsed: 37928.0 seconds\n",
      "Minibatch loss: 2.863319\n",
      "Minibatch accuracy: 79.5%\n",
      "Validation accuracy: 91.7%\n",
      "\n",
      "--- Step 348000 ---\n",
      "Time elapsed: 38037.8 seconds\n",
      "Minibatch loss: 2.321271\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 91.8%\n",
      "\n",
      "--- Step 349000 ---\n",
      "Time elapsed: 38148.1 seconds\n",
      "Minibatch loss: 2.303090\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 91.8%\n",
      "\n",
      "--- Step 350000 ---\n",
      "Time elapsed: 38260.9 seconds\n",
      "Minibatch loss: 1.816881\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 91.8%\n",
      "\n",
      "--- Step 351000 ---\n",
      "Time elapsed: 38370.7 seconds\n",
      "Minibatch loss: 2.034592\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 91.9%\n",
      "\n",
      "--- Step 352000 ---\n",
      "Time elapsed: 38480.0 seconds\n",
      "Minibatch loss: 1.996457\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 91.8%\n",
      "\n",
      "--- Step 353000 ---\n",
      "Time elapsed: 38588.8 seconds\n",
      "Minibatch loss: 2.310488\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 91.9%\n",
      "\n",
      "--- Step 354000 ---\n",
      "Time elapsed: 38697.6 seconds\n",
      "Minibatch loss: 1.522068\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 91.9%\n",
      "\n",
      "--- Step 355000 ---\n",
      "Time elapsed: 38806.8 seconds\n",
      "Minibatch loss: 2.424371\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 91.8%\n",
      "\n",
      "--- Step 356000 ---\n",
      "Time elapsed: 38915.9 seconds\n",
      "Minibatch loss: 2.459110\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 91.8%\n",
      "\n",
      "--- Step 357000 ---\n",
      "Time elapsed: 39024.4 seconds\n",
      "Minibatch loss: 1.952949\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 91.9%\n",
      "\n",
      "--- Step 358000 ---\n",
      "Time elapsed: 39133.3 seconds\n",
      "Minibatch loss: 1.898314\n",
      "Minibatch accuracy: 85.6%\n",
      "Validation accuracy: 91.9%\n",
      "\n",
      "--- Step 359000 ---\n",
      "Time elapsed: 39241.7 seconds\n",
      "Minibatch loss: 1.911331\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 91.9%\n",
      "\n",
      "--- Step 360000 ---\n",
      "Time elapsed: 39350.7 seconds\n",
      "Minibatch loss: 2.217101\n",
      "Minibatch accuracy: 83.3%\n",
      "Validation accuracy: 91.9%\n",
      "\n",
      "--- Step 361000 ---\n",
      "Time elapsed: 39459.0 seconds\n",
      "Minibatch loss: 2.237061\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 362000 ---\n",
      "Time elapsed: 39567.9 seconds\n",
      "Minibatch loss: 2.619836\n",
      "Minibatch accuracy: 80.6%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 363000 ---\n",
      "Time elapsed: 39676.7 seconds\n",
      "Minibatch loss: 1.846227\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 91.9%\n",
      "\n",
      "--- Step 364000 ---\n",
      "Time elapsed: 39785.5 seconds\n",
      "Minibatch loss: 1.750521\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 365000 ---\n",
      "Time elapsed: 39894.3 seconds\n",
      "Minibatch loss: 2.667104\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 366000 ---\n",
      "Time elapsed: 40003.2 seconds\n",
      "Minibatch loss: 2.416453\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 367000 ---\n",
      "Time elapsed: 40111.6 seconds\n",
      "Minibatch loss: 2.362591\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 368000 ---\n",
      "Time elapsed: 40220.5 seconds\n",
      "Minibatch loss: 1.823663\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 369000 ---\n",
      "Time elapsed: 40329.1 seconds\n",
      "Minibatch loss: 1.879384\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 370000 ---\n",
      "Time elapsed: 40438.5 seconds\n",
      "Minibatch loss: 2.164402\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 371000 ---\n",
      "Time elapsed: 40546.9 seconds\n",
      "Minibatch loss: 2.005229\n",
      "Minibatch accuracy: 84.7%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 372000 ---\n",
      "Time elapsed: 40655.5 seconds\n",
      "Minibatch loss: 2.228631\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 373000 ---\n",
      "Time elapsed: 40764.0 seconds\n",
      "Minibatch loss: 2.083629\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 374000 ---\n",
      "Time elapsed: 40872.7 seconds\n",
      "Minibatch loss: 1.816888\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 375000 ---\n",
      "Time elapsed: 40981.8 seconds\n",
      "Minibatch loss: 1.860652\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 376000 ---\n",
      "Time elapsed: 41091.2 seconds\n",
      "Minibatch loss: 2.284468\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 377000 ---\n",
      "Time elapsed: 41200.4 seconds\n",
      "Minibatch loss: 2.343422\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 378000 ---\n",
      "Time elapsed: 41309.4 seconds\n",
      "Minibatch loss: 2.444391\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 379000 ---\n",
      "Time elapsed: 41418.6 seconds\n",
      "Minibatch loss: 1.965867\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 380000 ---\n",
      "Time elapsed: 41527.3 seconds\n",
      "Minibatch loss: 2.109500\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 381000 ---\n",
      "Time elapsed: 41635.9 seconds\n",
      "Minibatch loss: 2.356423\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 382000 ---\n",
      "Time elapsed: 41744.3 seconds\n",
      "Minibatch loss: 2.108382\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 91.9%\n",
      "\n",
      "--- Step 383000 ---\n",
      "Time elapsed: 41853.3 seconds\n",
      "Minibatch loss: 1.985074\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 384000 ---\n",
      "Time elapsed: 41961.6 seconds\n",
      "Minibatch loss: 2.426299\n",
      "Minibatch accuracy: 80.9%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 385000 ---\n",
      "Time elapsed: 42070.0 seconds\n",
      "Minibatch loss: 1.623078\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.9%\n",
      "\n",
      "--- Step 386000 ---\n",
      "Time elapsed: 42178.8 seconds\n",
      "Minibatch loss: 1.850331\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 387000 ---\n",
      "Time elapsed: 42287.8 seconds\n",
      "Minibatch loss: 1.893883\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 388000 ---\n",
      "Time elapsed: 42396.6 seconds\n",
      "Minibatch loss: 1.838717\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 389000 ---\n",
      "Time elapsed: 42505.4 seconds\n",
      "Minibatch loss: 2.223000\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 390000 ---\n",
      "Time elapsed: 42614.1 seconds\n",
      "Minibatch loss: 2.007653\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 391000 ---\n",
      "Time elapsed: 42722.8 seconds\n",
      "Minibatch loss: 2.583097\n",
      "Minibatch accuracy: 80.2%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 392000 ---\n",
      "Time elapsed: 42831.9 seconds\n",
      "Minibatch loss: 1.727315\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 393000 ---\n",
      "Time elapsed: 42941.1 seconds\n",
      "Minibatch loss: 2.047571\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 394000 ---\n",
      "Time elapsed: 43050.0 seconds\n",
      "Minibatch loss: 2.277487\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 395000 ---\n",
      "Time elapsed: 43159.2 seconds\n",
      "Minibatch loss: 2.174131\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 396000 ---\n",
      "Time elapsed: 43267.8 seconds\n",
      "Minibatch loss: 2.158624\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 397000 ---\n",
      "Time elapsed: 43376.5 seconds\n",
      "Minibatch loss: 1.814025\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 398000 ---\n",
      "Time elapsed: 43485.1 seconds\n",
      "Minibatch loss: 1.861497\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 399000 ---\n",
      "Time elapsed: 43594.0 seconds\n",
      "Minibatch loss: 2.121980\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 400000 ---\n",
      "Time elapsed: 43702.5 seconds\n",
      "Minibatch loss: 1.850311\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 401000 ---\n",
      "Time elapsed: 43811.0 seconds\n",
      "Minibatch loss: 2.056395\n",
      "Minibatch accuracy: 84.5%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 402000 ---\n",
      "Time elapsed: 43920.1 seconds\n",
      "Minibatch loss: 2.004577\n",
      "Minibatch accuracy: 84.7%\n",
      "Validation accuracy: 92.0%\n",
      "\n",
      "--- Step 403000 ---\n",
      "Time elapsed: 44028.9 seconds\n",
      "Minibatch loss: 1.808599\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 404000 ---\n",
      "Time elapsed: 44137.4 seconds\n",
      "Minibatch loss: 1.967323\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 405000 ---\n",
      "Time elapsed: 44245.9 seconds\n",
      "Minibatch loss: 1.722834\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 406000 ---\n",
      "Time elapsed: 44354.5 seconds\n",
      "Minibatch loss: 1.753157\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 407000 ---\n",
      "Time elapsed: 44463.4 seconds\n",
      "Minibatch loss: 1.808861\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 92.1%\n",
      "\n",
      "--- Step 408000 ---\n",
      "Time elapsed: 44574.8 seconds\n",
      "Minibatch loss: 1.947096\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 409000 ---\n",
      "Time elapsed: 44683.9 seconds\n",
      "Minibatch loss: 2.070745\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 410000 ---\n",
      "Time elapsed: 44792.8 seconds\n",
      "Minibatch loss: 1.996490\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 411000 ---\n",
      "Time elapsed: 44901.3 seconds\n",
      "Minibatch loss: 2.209412\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 412000 ---\n",
      "Time elapsed: 45010.2 seconds\n",
      "Minibatch loss: 2.374836\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 413000 ---\n",
      "Time elapsed: 45118.8 seconds\n",
      "Minibatch loss: 2.344744\n",
      "Minibatch accuracy: 82.7%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 414000 ---\n",
      "Time elapsed: 45228.0 seconds\n",
      "Minibatch loss: 2.050309\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 415000 ---\n",
      "Time elapsed: 45348.5 seconds\n",
      "Minibatch loss: 1.899009\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 416000 ---\n",
      "Time elapsed: 45466.3 seconds\n",
      "Minibatch loss: 2.138322\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 417000 ---\n",
      "Time elapsed: 45574.7 seconds\n",
      "Minibatch loss: 1.733774\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 418000 ---\n",
      "Time elapsed: 45683.0 seconds\n",
      "Minibatch loss: 1.981485\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 419000 ---\n",
      "Time elapsed: 45791.9 seconds\n",
      "Minibatch loss: 1.766195\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 420000 ---\n",
      "Time elapsed: 45901.5 seconds\n",
      "Minibatch loss: 2.373702\n",
      "Minibatch accuracy: 83.9%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 421000 ---\n",
      "Time elapsed: 46010.5 seconds\n",
      "Minibatch loss: 2.145423\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 422000 ---\n",
      "Time elapsed: 46119.1 seconds\n",
      "Minibatch loss: 1.883630\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 423000 ---\n",
      "Time elapsed: 46228.1 seconds\n",
      "Minibatch loss: 2.046296\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 424000 ---\n",
      "Time elapsed: 46337.0 seconds\n",
      "Minibatch loss: 1.963868\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 425000 ---\n",
      "Time elapsed: 46446.0 seconds\n",
      "Minibatch loss: 2.253572\n",
      "Minibatch accuracy: 83.1%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 426000 ---\n",
      "Time elapsed: 46555.2 seconds\n",
      "Minibatch loss: 1.853639\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 92.2%\n",
      "\n",
      "--- Step 427000 ---\n",
      "Time elapsed: 46663.8 seconds\n",
      "Minibatch loss: 1.971561\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 428000 ---\n",
      "Time elapsed: 46772.6 seconds\n",
      "Minibatch loss: 1.887094\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 429000 ---\n",
      "Time elapsed: 46881.3 seconds\n",
      "Minibatch loss: 1.946681\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 430000 ---\n",
      "Time elapsed: 46989.8 seconds\n",
      "Minibatch loss: 1.911511\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 431000 ---\n",
      "Time elapsed: 47098.4 seconds\n",
      "Minibatch loss: 1.830965\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 432000 ---\n",
      "Time elapsed: 47207.3 seconds\n",
      "Minibatch loss: 1.863511\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 433000 ---\n",
      "Time elapsed: 47316.1 seconds\n",
      "Minibatch loss: 1.855456\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 434000 ---\n",
      "Time elapsed: 47424.6 seconds\n",
      "Minibatch loss: 1.945402\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 435000 ---\n",
      "Time elapsed: 47533.3 seconds\n",
      "Minibatch loss: 2.451798\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 436000 ---\n",
      "Time elapsed: 47642.5 seconds\n",
      "Minibatch loss: 2.189078\n",
      "Minibatch accuracy: 83.3%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 437000 ---\n",
      "Time elapsed: 47752.4 seconds\n",
      "Minibatch loss: 1.887475\n",
      "Minibatch accuracy: 85.6%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 438000 ---\n",
      "Time elapsed: 47861.4 seconds\n",
      "Minibatch loss: 2.025223\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 439000 ---\n",
      "Time elapsed: 47969.7 seconds\n",
      "Minibatch loss: 1.853400\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 440000 ---\n",
      "Time elapsed: 48077.9 seconds\n",
      "Minibatch loss: 2.283289\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 441000 ---\n",
      "Time elapsed: 48186.5 seconds\n",
      "Minibatch loss: 2.246974\n",
      "Minibatch accuracy: 82.7%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 442000 ---\n",
      "Time elapsed: 48295.4 seconds\n",
      "Minibatch loss: 1.886685\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 443000 ---\n",
      "Time elapsed: 48404.1 seconds\n",
      "Minibatch loss: 1.494296\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 444000 ---\n",
      "Time elapsed: 48513.2 seconds\n",
      "Minibatch loss: 1.544131\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 445000 ---\n",
      "Time elapsed: 48622.4 seconds\n",
      "Minibatch loss: 1.420384\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 446000 ---\n",
      "Time elapsed: 48730.9 seconds\n",
      "Minibatch loss: 1.823662\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 447000 ---\n",
      "Time elapsed: 48839.5 seconds\n",
      "Minibatch loss: 2.243968\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 448000 ---\n",
      "Time elapsed: 48948.7 seconds\n",
      "Minibatch loss: 1.981585\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 449000 ---\n",
      "Time elapsed: 49057.5 seconds\n",
      "Minibatch loss: 1.624851\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 450000 ---\n",
      "Time elapsed: 49166.6 seconds\n",
      "Minibatch loss: 2.110618\n",
      "Minibatch accuracy: 84.7%\n",
      "Validation accuracy: 92.3%\n",
      "\n",
      "--- Step 451000 ---\n",
      "Time elapsed: 49275.9 seconds\n",
      "Minibatch loss: 1.744528\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 452000 ---\n",
      "Time elapsed: 49384.7 seconds\n",
      "Minibatch loss: 2.084377\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 453000 ---\n",
      "Time elapsed: 49493.2 seconds\n",
      "Minibatch loss: 2.319659\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 454000 ---\n",
      "Time elapsed: 49602.1 seconds\n",
      "Minibatch loss: 1.847760\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 455000 ---\n",
      "Time elapsed: 49710.9 seconds\n",
      "Minibatch loss: 1.838561\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 456000 ---\n",
      "Time elapsed: 49819.4 seconds\n",
      "Minibatch loss: 1.650738\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 457000 ---\n",
      "Time elapsed: 49928.3 seconds\n",
      "Minibatch loss: 2.132039\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 458000 ---\n",
      "Time elapsed: 50036.6 seconds\n",
      "Minibatch loss: 2.034415\n",
      "Minibatch accuracy: 84.7%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 459000 ---\n",
      "Time elapsed: 50145.2 seconds\n",
      "Minibatch loss: 1.504616\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 460000 ---\n",
      "Time elapsed: 50253.9 seconds\n",
      "Minibatch loss: 2.104476\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 461000 ---\n",
      "Time elapsed: 50362.5 seconds\n",
      "Minibatch loss: 1.766047\n",
      "Minibatch accuracy: 85.6%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 462000 ---\n",
      "Time elapsed: 50471.4 seconds\n",
      "Minibatch loss: 2.311086\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 463000 ---\n",
      "Time elapsed: 50580.1 seconds\n",
      "Minibatch loss: 2.172584\n",
      "Minibatch accuracy: 84.2%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 464000 ---\n",
      "Time elapsed: 50691.5 seconds\n",
      "Minibatch loss: 1.956041\n",
      "Minibatch accuracy: 83.9%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 465000 ---\n",
      "Time elapsed: 50800.4 seconds\n",
      "Minibatch loss: 2.015836\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 466000 ---\n",
      "Time elapsed: 50909.1 seconds\n",
      "Minibatch loss: 1.748301\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 467000 ---\n",
      "Time elapsed: 51017.7 seconds\n",
      "Minibatch loss: 1.764229\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 468000 ---\n",
      "Time elapsed: 51127.3 seconds\n",
      "Minibatch loss: 1.961397\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 469000 ---\n",
      "Time elapsed: 51237.3 seconds\n",
      "Minibatch loss: 1.870450\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 470000 ---\n",
      "Time elapsed: 51346.6 seconds\n",
      "Minibatch loss: 2.139875\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 471000 ---\n",
      "Time elapsed: 51455.2 seconds\n",
      "Minibatch loss: 1.702644\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 472000 ---\n",
      "Time elapsed: 51564.0 seconds\n",
      "Minibatch loss: 1.614923\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 473000 ---\n",
      "Time elapsed: 51672.6 seconds\n",
      "Minibatch loss: 1.771873\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 474000 ---\n",
      "Time elapsed: 51781.2 seconds\n",
      "Minibatch loss: 1.470137\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 475000 ---\n",
      "Time elapsed: 51889.8 seconds\n",
      "Minibatch loss: 1.375277\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 476000 ---\n",
      "Time elapsed: 51998.2 seconds\n",
      "Minibatch loss: 1.970217\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 477000 ---\n",
      "Time elapsed: 52107.1 seconds\n",
      "Minibatch loss: 1.761831\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 478000 ---\n",
      "Time elapsed: 52216.1 seconds\n",
      "Minibatch loss: 1.822224\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 479000 ---\n",
      "Time elapsed: 52324.9 seconds\n",
      "Minibatch loss: 2.211620\n",
      "Minibatch accuracy: 83.9%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 480000 ---\n",
      "Time elapsed: 52434.0 seconds\n",
      "Minibatch loss: 1.440836\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 481000 ---\n",
      "Time elapsed: 52542.6 seconds\n",
      "Minibatch loss: 1.693954\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 482000 ---\n",
      "Time elapsed: 52651.4 seconds\n",
      "Minibatch loss: 1.922923\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 92.4%\n",
      "\n",
      "--- Step 483000 ---\n",
      "Time elapsed: 52764.7 seconds\n",
      "Minibatch loss: 1.750611\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 484000 ---\n",
      "Time elapsed: 52876.7 seconds\n",
      "Minibatch loss: 2.323454\n",
      "Minibatch accuracy: 83.9%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 485000 ---\n",
      "Time elapsed: 52991.5 seconds\n",
      "Minibatch loss: 1.865037\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 486000 ---\n",
      "Time elapsed: 53104.7 seconds\n",
      "Minibatch loss: 2.083633\n",
      "Minibatch accuracy: 83.9%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 487000 ---\n",
      "Time elapsed: 53213.7 seconds\n",
      "Minibatch loss: 2.295161\n",
      "Minibatch accuracy: 80.6%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 488000 ---\n",
      "Time elapsed: 53322.4 seconds\n",
      "Minibatch loss: 2.190975\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 489000 ---\n",
      "Time elapsed: 53431.3 seconds\n",
      "Minibatch loss: 1.828522\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 490000 ---\n",
      "Time elapsed: 53540.1 seconds\n",
      "Minibatch loss: 1.396757\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 491000 ---\n",
      "Time elapsed: 53648.7 seconds\n",
      "Minibatch loss: 1.908530\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 492000 ---\n",
      "Time elapsed: 53757.2 seconds\n",
      "Minibatch loss: 1.766950\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 493000 ---\n",
      "Time elapsed: 53865.9 seconds\n",
      "Minibatch loss: 1.467287\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 494000 ---\n",
      "Time elapsed: 53974.7 seconds\n",
      "Minibatch loss: 1.454266\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 495000 ---\n",
      "Time elapsed: 54083.4 seconds\n",
      "Minibatch loss: 2.048473\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 496000 ---\n",
      "Time elapsed: 54195.8 seconds\n",
      "Minibatch loss: 1.848185\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 497000 ---\n",
      "Time elapsed: 54304.8 seconds\n",
      "Minibatch loss: 1.951688\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 498000 ---\n",
      "Time elapsed: 54414.9 seconds\n",
      "Minibatch loss: 1.766024\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 499000 ---\n",
      "Time elapsed: 54523.7 seconds\n",
      "Minibatch loss: 2.211786\n",
      "Minibatch accuracy: 83.1%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 500000 ---\n",
      "Time elapsed: 54632.2 seconds\n",
      "Minibatch loss: 1.668035\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 501000 ---\n",
      "Time elapsed: 54740.7 seconds\n",
      "Minibatch loss: 1.997567\n",
      "Minibatch accuracy: 85.6%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 502000 ---\n",
      "Time elapsed: 54849.5 seconds\n",
      "Minibatch loss: 1.908632\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 503000 ---\n",
      "Time elapsed: 54958.7 seconds\n",
      "Minibatch loss: 2.347743\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 504000 ---\n",
      "Time elapsed: 55068.4 seconds\n",
      "Minibatch loss: 1.993924\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 505000 ---\n",
      "Time elapsed: 55177.0 seconds\n",
      "Minibatch loss: 1.798779\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 506000 ---\n",
      "Time elapsed: 55285.6 seconds\n",
      "Minibatch loss: 1.957883\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 507000 ---\n",
      "Time elapsed: 55394.0 seconds\n",
      "Minibatch loss: 2.023084\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 508000 ---\n",
      "Time elapsed: 55503.2 seconds\n",
      "Minibatch loss: 1.779093\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 509000 ---\n",
      "Time elapsed: 55611.9 seconds\n",
      "Minibatch loss: 1.641946\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 510000 ---\n",
      "Time elapsed: 55720.5 seconds\n",
      "Minibatch loss: 1.724651\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.5%\n",
      "\n",
      "--- Step 511000 ---\n",
      "Time elapsed: 55829.2 seconds\n",
      "Minibatch loss: 2.007806\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 512000 ---\n",
      "Time elapsed: 55937.9 seconds\n",
      "Minibatch loss: 1.967609\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 513000 ---\n",
      "Time elapsed: 56046.9 seconds\n",
      "Minibatch loss: 1.913166\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 514000 ---\n",
      "Time elapsed: 56155.7 seconds\n",
      "Minibatch loss: 2.042332\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 515000 ---\n",
      "Time elapsed: 56264.3 seconds\n",
      "Minibatch loss: 1.688587\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 516000 ---\n",
      "Time elapsed: 56373.0 seconds\n",
      "Minibatch loss: 1.775947\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 517000 ---\n",
      "Time elapsed: 56481.7 seconds\n",
      "Minibatch loss: 2.132708\n",
      "Minibatch accuracy: 84.7%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 518000 ---\n",
      "Time elapsed: 56592.1 seconds\n",
      "Minibatch loss: 1.818083\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 519000 ---\n",
      "Time elapsed: 56700.8 seconds\n",
      "Minibatch loss: 1.800141\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 520000 ---\n",
      "Time elapsed: 56809.7 seconds\n",
      "Minibatch loss: 1.618570\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 521000 ---\n",
      "Time elapsed: 56918.3 seconds\n",
      "Minibatch loss: 1.870446\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 522000 ---\n",
      "Time elapsed: 57026.8 seconds\n",
      "Minibatch loss: 1.767161\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 523000 ---\n",
      "Time elapsed: 57136.9 seconds\n",
      "Minibatch loss: 1.994192\n",
      "Minibatch accuracy: 84.1%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 524000 ---\n",
      "Time elapsed: 57245.9 seconds\n",
      "Minibatch loss: 2.062053\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 525000 ---\n",
      "Time elapsed: 57354.7 seconds\n",
      "Minibatch loss: 2.033961\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 526000 ---\n",
      "Time elapsed: 57463.4 seconds\n",
      "Minibatch loss: 1.805474\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 527000 ---\n",
      "Time elapsed: 57571.8 seconds\n",
      "Minibatch loss: 1.974597\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 528000 ---\n",
      "Time elapsed: 57680.3 seconds\n",
      "Minibatch loss: 1.697837\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 529000 ---\n",
      "Time elapsed: 57789.0 seconds\n",
      "Minibatch loss: 1.581946\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 530000 ---\n",
      "Time elapsed: 57898.2 seconds\n",
      "Minibatch loss: 1.764993\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 531000 ---\n",
      "Time elapsed: 58007.4 seconds\n",
      "Minibatch loss: 1.578577\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 532000 ---\n",
      "Time elapsed: 58116.1 seconds\n",
      "Minibatch loss: 1.801457\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 533000 ---\n",
      "Time elapsed: 58225.0 seconds\n",
      "Minibatch loss: 1.577923\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 534000 ---\n",
      "Time elapsed: 58333.7 seconds\n",
      "Minibatch loss: 1.690741\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 535000 ---\n",
      "Time elapsed: 58442.9 seconds\n",
      "Minibatch loss: 1.848143\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 536000 ---\n",
      "Time elapsed: 58551.8 seconds\n",
      "Minibatch loss: 1.709363\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 537000 ---\n",
      "Time elapsed: 58660.5 seconds\n",
      "Minibatch loss: 2.067491\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 538000 ---\n",
      "Time elapsed: 58771.2 seconds\n",
      "Minibatch loss: 1.413640\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 539000 ---\n",
      "Time elapsed: 58879.9 seconds\n",
      "Minibatch loss: 1.757279\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 540000 ---\n",
      "Time elapsed: 58988.8 seconds\n",
      "Minibatch loss: 1.651843\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 541000 ---\n",
      "Time elapsed: 59097.8 seconds\n",
      "Minibatch loss: 1.889385\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 542000 ---\n",
      "Time elapsed: 59206.7 seconds\n",
      "Minibatch loss: 1.800435\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 543000 ---\n",
      "Time elapsed: 59315.6 seconds\n",
      "Minibatch loss: 2.064845\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 544000 ---\n",
      "Time elapsed: 59424.6 seconds\n",
      "Minibatch loss: 1.585368\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 545000 ---\n",
      "Time elapsed: 59533.3 seconds\n",
      "Minibatch loss: 1.958787\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 546000 ---\n",
      "Time elapsed: 59642.2 seconds\n",
      "Minibatch loss: 1.699501\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 547000 ---\n",
      "Time elapsed: 59751.0 seconds\n",
      "Minibatch loss: 1.507216\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 548000 ---\n",
      "Time elapsed: 59860.4 seconds\n",
      "Minibatch loss: 1.664244\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 549000 ---\n",
      "Time elapsed: 59969.3 seconds\n",
      "Minibatch loss: 1.837114\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 550000 ---\n",
      "Time elapsed: 60078.4 seconds\n",
      "Minibatch loss: 1.946975\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 92.6%\n",
      "\n",
      "--- Step 551000 ---\n",
      "Time elapsed: 60187.2 seconds\n",
      "Minibatch loss: 1.789687\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 552000 ---\n",
      "Time elapsed: 60296.1 seconds\n",
      "Minibatch loss: 1.733389\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 553000 ---\n",
      "Time elapsed: 60404.7 seconds\n",
      "Minibatch loss: 1.405224\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 554000 ---\n",
      "Time elapsed: 60513.4 seconds\n",
      "Minibatch loss: 1.789792\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 555000 ---\n",
      "Time elapsed: 60622.5 seconds\n",
      "Minibatch loss: 1.945378\n",
      "Minibatch accuracy: 85.6%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 556000 ---\n",
      "Time elapsed: 60731.1 seconds\n",
      "Minibatch loss: 1.819918\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 557000 ---\n",
      "Time elapsed: 60840.1 seconds\n",
      "Minibatch loss: 1.637764\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 558000 ---\n",
      "Time elapsed: 60949.0 seconds\n",
      "Minibatch loss: 1.756136\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 559000 ---\n",
      "Time elapsed: 61057.7 seconds\n",
      "Minibatch loss: 1.471204\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 560000 ---\n",
      "Time elapsed: 61166.4 seconds\n",
      "Minibatch loss: 1.559337\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 561000 ---\n",
      "Time elapsed: 61275.0 seconds\n",
      "Minibatch loss: 1.853058\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 562000 ---\n",
      "Time elapsed: 61383.5 seconds\n",
      "Minibatch loss: 1.392567\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 563000 ---\n",
      "Time elapsed: 61492.4 seconds\n",
      "Minibatch loss: 1.636782\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 564000 ---\n",
      "Time elapsed: 61601.6 seconds\n",
      "Minibatch loss: 1.788322\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 565000 ---\n",
      "Time elapsed: 61710.9 seconds\n",
      "Minibatch loss: 1.657114\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 566000 ---\n",
      "Time elapsed: 61819.5 seconds\n",
      "Minibatch loss: 1.761537\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 567000 ---\n",
      "Time elapsed: 61928.7 seconds\n",
      "Minibatch loss: 1.422294\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 568000 ---\n",
      "Time elapsed: 62037.5 seconds\n",
      "Minibatch loss: 2.086820\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 569000 ---\n",
      "Time elapsed: 62146.3 seconds\n",
      "Minibatch loss: 1.715486\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 570000 ---\n",
      "Time elapsed: 62255.1 seconds\n",
      "Minibatch loss: 1.910166\n",
      "Minibatch accuracy: 85.6%\n",
      "Validation accuracy: 92.7%\n",
      "\n",
      "--- Step 571000 ---\n",
      "Time elapsed: 62363.8 seconds\n",
      "Minibatch loss: 1.872828\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 572000 ---\n",
      "Time elapsed: 62472.6 seconds\n",
      "Minibatch loss: 1.458971\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 573000 ---\n",
      "Time elapsed: 62581.2 seconds\n",
      "Minibatch loss: 1.915635\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 574000 ---\n",
      "Time elapsed: 62692.7 seconds\n",
      "Minibatch loss: 1.931231\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 575000 ---\n",
      "Time elapsed: 62801.3 seconds\n",
      "Minibatch loss: 1.832261\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 576000 ---\n",
      "Time elapsed: 62909.7 seconds\n",
      "Minibatch loss: 1.416318\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 577000 ---\n",
      "Time elapsed: 63018.2 seconds\n",
      "Minibatch loss: 1.939504\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 578000 ---\n",
      "Time elapsed: 63126.6 seconds\n",
      "Minibatch loss: 1.560881\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 579000 ---\n",
      "Time elapsed: 63235.2 seconds\n",
      "Minibatch loss: 1.817347\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 580000 ---\n",
      "Time elapsed: 63344.6 seconds\n",
      "Minibatch loss: 2.007879\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 581000 ---\n",
      "Time elapsed: 63453.9 seconds\n",
      "Minibatch loss: 1.892565\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 582000 ---\n",
      "Time elapsed: 63563.0 seconds\n",
      "Minibatch loss: 1.863985\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 583000 ---\n",
      "Time elapsed: 63671.7 seconds\n",
      "Minibatch loss: 1.816332\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 584000 ---\n",
      "Time elapsed: 63780.6 seconds\n",
      "Minibatch loss: 1.584522\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 585000 ---\n",
      "Time elapsed: 63889.2 seconds\n",
      "Minibatch loss: 1.775151\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 586000 ---\n",
      "Time elapsed: 63997.7 seconds\n",
      "Minibatch loss: 2.038095\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 587000 ---\n",
      "Time elapsed: 64106.2 seconds\n",
      "Minibatch loss: 2.026527\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 588000 ---\n",
      "Time elapsed: 64214.9 seconds\n",
      "Minibatch loss: 1.692728\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 589000 ---\n",
      "Time elapsed: 64323.6 seconds\n",
      "Minibatch loss: 1.593702\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 590000 ---\n",
      "Time elapsed: 64432.8 seconds\n",
      "Minibatch loss: 1.747848\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 591000 ---\n",
      "Time elapsed: 64541.6 seconds\n",
      "Minibatch loss: 1.659927\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 592000 ---\n",
      "Time elapsed: 64650.1 seconds\n",
      "Minibatch loss: 1.722993\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 593000 ---\n",
      "Time elapsed: 64758.9 seconds\n",
      "Minibatch loss: 1.776986\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 594000 ---\n",
      "Time elapsed: 64867.8 seconds\n",
      "Minibatch loss: 1.948665\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 595000 ---\n",
      "Time elapsed: 64976.2 seconds\n",
      "Minibatch loss: 1.229578\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 596000 ---\n",
      "Time elapsed: 65084.6 seconds\n",
      "Minibatch loss: 1.616890\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 597000 ---\n",
      "Time elapsed: 65193.2 seconds\n",
      "Minibatch loss: 1.756978\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 598000 ---\n",
      "Time elapsed: 65301.7 seconds\n",
      "Minibatch loss: 1.780928\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 599000 ---\n",
      "Time elapsed: 65410.2 seconds\n",
      "Minibatch loss: 1.932148\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 600000 ---\n",
      "Time elapsed: 65519.0 seconds\n",
      "Minibatch loss: 1.678859\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 601000 ---\n",
      "Time elapsed: 65627.8 seconds\n",
      "Minibatch loss: 1.339856\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 602000 ---\n",
      "Time elapsed: 65736.5 seconds\n",
      "Minibatch loss: 1.313542\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 603000 ---\n",
      "Time elapsed: 65845.0 seconds\n",
      "Minibatch loss: 2.045903\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 604000 ---\n",
      "Time elapsed: 65953.6 seconds\n",
      "Minibatch loss: 1.789810\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 605000 ---\n",
      "Time elapsed: 66062.8 seconds\n",
      "Minibatch loss: 1.580407\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 606000 ---\n",
      "Time elapsed: 66171.5 seconds\n",
      "Minibatch loss: 1.948364\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 607000 ---\n",
      "Time elapsed: 66280.1 seconds\n",
      "Minibatch loss: 1.989128\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 608000 ---\n",
      "Time elapsed: 66389.3 seconds\n",
      "Minibatch loss: 1.919317\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 609000 ---\n",
      "Time elapsed: 66498.0 seconds\n",
      "Minibatch loss: 1.584837\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 610000 ---\n",
      "Time elapsed: 66606.2 seconds\n",
      "Minibatch loss: 1.623669\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 611000 ---\n",
      "Time elapsed: 66714.5 seconds\n",
      "Minibatch loss: 1.678290\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 612000 ---\n",
      "Time elapsed: 66822.7 seconds\n",
      "Minibatch loss: 1.289185\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 613000 ---\n",
      "Time elapsed: 66931.7 seconds\n",
      "Minibatch loss: 1.251317\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 614000 ---\n",
      "Time elapsed: 67040.7 seconds\n",
      "Minibatch loss: 1.738791\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 615000 ---\n",
      "Time elapsed: 67149.0 seconds\n",
      "Minibatch loss: 1.664588\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 616000 ---\n",
      "Time elapsed: 67257.6 seconds\n",
      "Minibatch loss: 1.679586\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 617000 ---\n",
      "Time elapsed: 67366.4 seconds\n",
      "Minibatch loss: 1.554134\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 618000 ---\n",
      "Time elapsed: 67475.4 seconds\n",
      "Minibatch loss: 1.479628\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 619000 ---\n",
      "Time elapsed: 67583.8 seconds\n",
      "Minibatch loss: 1.532563\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 620000 ---\n",
      "Time elapsed: 67692.5 seconds\n",
      "Minibatch loss: 1.500757\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 621000 ---\n",
      "Time elapsed: 67801.1 seconds\n",
      "Minibatch loss: 1.247409\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 622000 ---\n",
      "Time elapsed: 67909.8 seconds\n",
      "Minibatch loss: 1.598583\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 623000 ---\n",
      "Time elapsed: 68018.3 seconds\n",
      "Minibatch loss: 1.664642\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 624000 ---\n",
      "Time elapsed: 68126.5 seconds\n",
      "Minibatch loss: 1.386199\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 625000 ---\n",
      "Time elapsed: 68235.1 seconds\n",
      "Minibatch loss: 1.187168\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 626000 ---\n",
      "Time elapsed: 68343.7 seconds\n",
      "Minibatch loss: 1.680483\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 627000 ---\n",
      "Time elapsed: 68452.5 seconds\n",
      "Minibatch loss: 1.291550\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 628000 ---\n",
      "Time elapsed: 68561.2 seconds\n",
      "Minibatch loss: 1.392564\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 629000 ---\n",
      "Time elapsed: 68670.0 seconds\n",
      "Minibatch loss: 1.333227\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 630000 ---\n",
      "Time elapsed: 68779.2 seconds\n",
      "Minibatch loss: 1.937860\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 631000 ---\n",
      "Time elapsed: 68887.9 seconds\n",
      "Minibatch loss: 1.687973\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 632000 ---\n",
      "Time elapsed: 68996.7 seconds\n",
      "Minibatch loss: 1.804656\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 633000 ---\n",
      "Time elapsed: 69105.2 seconds\n",
      "Minibatch loss: 1.743043\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 634000 ---\n",
      "Time elapsed: 69214.3 seconds\n",
      "Minibatch loss: 1.744090\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 635000 ---\n",
      "Time elapsed: 69323.2 seconds\n",
      "Minibatch loss: 1.349622\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 636000 ---\n",
      "Time elapsed: 69431.6 seconds\n",
      "Minibatch loss: 1.583936\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 637000 ---\n",
      "Time elapsed: 69540.4 seconds\n",
      "Minibatch loss: 1.582322\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 638000 ---\n",
      "Time elapsed: 69649.0 seconds\n",
      "Minibatch loss: 1.632829\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 639000 ---\n",
      "Time elapsed: 69758.1 seconds\n",
      "Minibatch loss: 1.453542\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 640000 ---\n",
      "Time elapsed: 69866.9 seconds\n",
      "Minibatch loss: 1.531833\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 641000 ---\n",
      "Time elapsed: 69975.8 seconds\n",
      "Minibatch loss: 1.480842\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 642000 ---\n",
      "Time elapsed: 70084.7 seconds\n",
      "Minibatch loss: 2.052739\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 643000 ---\n",
      "Time elapsed: 70193.4 seconds\n",
      "Minibatch loss: 1.783227\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 644000 ---\n",
      "Time elapsed: 70302.6 seconds\n",
      "Minibatch loss: 1.509474\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 645000 ---\n",
      "Time elapsed: 70411.3 seconds\n",
      "Minibatch loss: 2.451068\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 646000 ---\n",
      "Time elapsed: 70520.8 seconds\n",
      "Minibatch loss: 1.379512\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 647000 ---\n",
      "Time elapsed: 70630.0 seconds\n",
      "Minibatch loss: 1.809217\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 648000 ---\n",
      "Time elapsed: 70738.9 seconds\n",
      "Minibatch loss: 1.402869\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 649000 ---\n",
      "Time elapsed: 70847.7 seconds\n",
      "Minibatch loss: 1.858849\n",
      "Minibatch accuracy: 85.6%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 650000 ---\n",
      "Time elapsed: 70956.6 seconds\n",
      "Minibatch loss: 1.509378\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 651000 ---\n",
      "Time elapsed: 71065.7 seconds\n",
      "Minibatch loss: 1.561795\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 652000 ---\n",
      "Time elapsed: 71174.7 seconds\n",
      "Minibatch loss: 1.492528\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 653000 ---\n",
      "Time elapsed: 71283.6 seconds\n",
      "Minibatch loss: 1.550096\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 654000 ---\n",
      "Time elapsed: 71391.9 seconds\n",
      "Minibatch loss: 1.519514\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 655000 ---\n",
      "Time elapsed: 71500.6 seconds\n",
      "Minibatch loss: 1.933378\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 656000 ---\n",
      "Time elapsed: 71609.6 seconds\n",
      "Minibatch loss: 1.771696\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 657000 ---\n",
      "Time elapsed: 71718.8 seconds\n",
      "Minibatch loss: 1.683597\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 658000 ---\n",
      "Time elapsed: 71829.3 seconds\n",
      "Minibatch loss: 1.504936\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 659000 ---\n",
      "Time elapsed: 71938.3 seconds\n",
      "Minibatch loss: 1.684678\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 660000 ---\n",
      "Time elapsed: 72047.3 seconds\n",
      "Minibatch loss: 1.451093\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 92.8%\n",
      "\n",
      "--- Step 661000 ---\n",
      "Time elapsed: 72156.6 seconds\n",
      "Minibatch loss: 1.753099\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 662000 ---\n",
      "Time elapsed: 72265.6 seconds\n",
      "Minibatch loss: 2.059139\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 663000 ---\n",
      "Time elapsed: 72374.6 seconds\n",
      "Minibatch loss: 1.801739\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 664000 ---\n",
      "Time elapsed: 72483.5 seconds\n",
      "Minibatch loss: 1.567712\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 665000 ---\n",
      "Time elapsed: 72592.2 seconds\n",
      "Minibatch loss: 2.008239\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 666000 ---\n",
      "Time elapsed: 72700.7 seconds\n",
      "Minibatch loss: 1.702247\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 667000 ---\n",
      "Time elapsed: 72809.6 seconds\n",
      "Minibatch loss: 1.793525\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 668000 ---\n",
      "Time elapsed: 72918.7 seconds\n",
      "Minibatch loss: 1.881442\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 669000 ---\n",
      "Time elapsed: 73027.6 seconds\n",
      "Minibatch loss: 2.054007\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 670000 ---\n",
      "Time elapsed: 73136.0 seconds\n",
      "Minibatch loss: 1.703333\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 671000 ---\n",
      "Time elapsed: 73245.0 seconds\n",
      "Minibatch loss: 1.664772\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 672000 ---\n",
      "Time elapsed: 73354.0 seconds\n",
      "Minibatch loss: 1.494362\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 673000 ---\n",
      "Time elapsed: 73463.1 seconds\n",
      "Minibatch loss: 1.731621\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 674000 ---\n",
      "Time elapsed: 73572.3 seconds\n",
      "Minibatch loss: 1.990284\n",
      "Minibatch accuracy: 84.7%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 675000 ---\n",
      "Time elapsed: 73681.4 seconds\n",
      "Minibatch loss: 1.652898\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 676000 ---\n",
      "Time elapsed: 73790.4 seconds\n",
      "Minibatch loss: 1.571566\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 677000 ---\n",
      "Time elapsed: 73899.0 seconds\n",
      "Minibatch loss: 1.325448\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 678000 ---\n",
      "Time elapsed: 74008.3 seconds\n",
      "Minibatch loss: 1.575237\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 679000 ---\n",
      "Time elapsed: 74117.6 seconds\n",
      "Minibatch loss: 1.959607\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 680000 ---\n",
      "Time elapsed: 74228.4 seconds\n",
      "Minibatch loss: 1.799111\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 681000 ---\n",
      "Time elapsed: 74338.8 seconds\n",
      "Minibatch loss: 1.713812\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 682000 ---\n",
      "Time elapsed: 74448.4 seconds\n",
      "Minibatch loss: 1.795389\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 683000 ---\n",
      "Time elapsed: 74557.7 seconds\n",
      "Minibatch loss: 1.416959\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 684000 ---\n",
      "Time elapsed: 74666.8 seconds\n",
      "Minibatch loss: 1.331409\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 685000 ---\n",
      "Time elapsed: 74775.8 seconds\n",
      "Minibatch loss: 1.602987\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 686000 ---\n",
      "Time elapsed: 74884.7 seconds\n",
      "Minibatch loss: 1.697961\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 687000 ---\n",
      "Time elapsed: 74993.8 seconds\n",
      "Minibatch loss: 1.509770\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 688000 ---\n",
      "Time elapsed: 75102.9 seconds\n",
      "Minibatch loss: 1.414228\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 689000 ---\n",
      "Time elapsed: 75212.2 seconds\n",
      "Minibatch loss: 1.399895\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 690000 ---\n",
      "Time elapsed: 75320.6 seconds\n",
      "Minibatch loss: 1.352312\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 691000 ---\n",
      "Time elapsed: 75429.3 seconds\n",
      "Minibatch loss: 2.082854\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 692000 ---\n",
      "Time elapsed: 75538.1 seconds\n",
      "Minibatch loss: 1.671731\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 693000 ---\n",
      "Time elapsed: 75647.1 seconds\n",
      "Minibatch loss: 1.679074\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 694000 ---\n",
      "Time elapsed: 75755.7 seconds\n",
      "Minibatch loss: 1.304217\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 695000 ---\n",
      "Time elapsed: 75864.2 seconds\n",
      "Minibatch loss: 1.460968\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 696000 ---\n",
      "Time elapsed: 75973.1 seconds\n",
      "Minibatch loss: 1.767962\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 697000 ---\n",
      "Time elapsed: 76081.9 seconds\n",
      "Minibatch loss: 1.622250\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 698000 ---\n",
      "Time elapsed: 76191.1 seconds\n",
      "Minibatch loss: 1.890612\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 699000 ---\n",
      "Time elapsed: 76304.0 seconds\n",
      "Minibatch loss: 1.687995\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 700000 ---\n",
      "Time elapsed: 76413.5 seconds\n",
      "Minibatch loss: 1.388556\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 701000 ---\n",
      "Time elapsed: 76522.4 seconds\n",
      "Minibatch loss: 1.809093\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 702000 ---\n",
      "Time elapsed: 76631.3 seconds\n",
      "Minibatch loss: 1.458055\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 703000 ---\n",
      "Time elapsed: 76740.1 seconds\n",
      "Minibatch loss: 1.734363\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 704000 ---\n",
      "Time elapsed: 76848.8 seconds\n",
      "Minibatch loss: 1.655300\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 705000 ---\n",
      "Time elapsed: 76957.9 seconds\n",
      "Minibatch loss: 1.637172\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 706000 ---\n",
      "Time elapsed: 77066.7 seconds\n",
      "Minibatch loss: 1.516799\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 707000 ---\n",
      "Time elapsed: 77175.7 seconds\n",
      "Minibatch loss: 1.471198\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 708000 ---\n",
      "Time elapsed: 77284.5 seconds\n",
      "Minibatch loss: 1.640050\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 709000 ---\n",
      "Time elapsed: 77393.6 seconds\n",
      "Minibatch loss: 1.708202\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 710000 ---\n",
      "Time elapsed: 77502.4 seconds\n",
      "Minibatch loss: 1.596920\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 711000 ---\n",
      "Time elapsed: 77611.1 seconds\n",
      "Minibatch loss: 1.819937\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 712000 ---\n",
      "Time elapsed: 77719.9 seconds\n",
      "Minibatch loss: 1.668591\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 713000 ---\n",
      "Time elapsed: 77828.5 seconds\n",
      "Minibatch loss: 1.576275\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 714000 ---\n",
      "Time elapsed: 77937.1 seconds\n",
      "Minibatch loss: 1.831838\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 715000 ---\n",
      "Time elapsed: 78045.8 seconds\n",
      "Minibatch loss: 2.075339\n",
      "Minibatch accuracy: 84.5%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 716000 ---\n",
      "Time elapsed: 78154.4 seconds\n",
      "Minibatch loss: 2.049837\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 717000 ---\n",
      "Time elapsed: 78263.3 seconds\n",
      "Minibatch loss: 1.803921\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 718000 ---\n",
      "Time elapsed: 78372.3 seconds\n",
      "Minibatch loss: 1.449360\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 719000 ---\n",
      "Time elapsed: 78481.7 seconds\n",
      "Minibatch loss: 1.290351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 720000 ---\n",
      "Time elapsed: 78590.5 seconds\n",
      "Minibatch loss: 1.637488\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 721000 ---\n",
      "Time elapsed: 78699.9 seconds\n",
      "Minibatch loss: 1.543455\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 722000 ---\n",
      "Time elapsed: 78808.9 seconds\n",
      "Minibatch loss: 1.592603\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 723000 ---\n",
      "Time elapsed: 78917.8 seconds\n",
      "Minibatch loss: 1.322864\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 724000 ---\n",
      "Time elapsed: 79026.3 seconds\n",
      "Minibatch loss: 2.007314\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 725000 ---\n",
      "Time elapsed: 79135.1 seconds\n",
      "Minibatch loss: 1.698527\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 726000 ---\n",
      "Time elapsed: 79244.2 seconds\n",
      "Minibatch loss: 2.040637\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 727000 ---\n",
      "Time elapsed: 79353.3 seconds\n",
      "Minibatch loss: 1.680737\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 728000 ---\n",
      "Time elapsed: 79462.1 seconds\n",
      "Minibatch loss: 1.636099\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 729000 ---\n",
      "Time elapsed: 79571.3 seconds\n",
      "Minibatch loss: 1.742570\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 730000 ---\n",
      "Time elapsed: 79679.9 seconds\n",
      "Minibatch loss: 1.663057\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 731000 ---\n",
      "Time elapsed: 79789.0 seconds\n",
      "Minibatch loss: 1.638707\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.9%\n",
      "\n",
      "--- Step 732000 ---\n",
      "Time elapsed: 79897.9 seconds\n",
      "Minibatch loss: 1.724830\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 733000 ---\n",
      "Time elapsed: 80006.4 seconds\n",
      "Minibatch loss: 1.323554\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 734000 ---\n",
      "Time elapsed: 80115.4 seconds\n",
      "Minibatch loss: 1.705446\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 735000 ---\n",
      "Time elapsed: 80224.6 seconds\n",
      "Minibatch loss: 1.721627\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 736000 ---\n",
      "Time elapsed: 80333.2 seconds\n",
      "Minibatch loss: 1.515850\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 737000 ---\n",
      "Time elapsed: 80441.9 seconds\n",
      "Minibatch loss: 1.611080\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 738000 ---\n",
      "Time elapsed: 80551.0 seconds\n",
      "Minibatch loss: 1.833907\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 739000 ---\n",
      "Time elapsed: 80660.0 seconds\n",
      "Minibatch loss: 1.613061\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.0%\n",
      "\n",
      "--- Step 740000 ---\n",
      "Time elapsed: 80769.0 seconds\n",
      "Minibatch loss: 1.850191\n",
      "Minibatch accuracy: 86.4%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 741000 ---\n",
      "Time elapsed: 80877.7 seconds\n",
      "Minibatch loss: 1.322205\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 742000 ---\n",
      "Time elapsed: 80986.2 seconds\n",
      "Minibatch loss: 1.247606\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 743000 ---\n",
      "Time elapsed: 81094.9 seconds\n",
      "Minibatch loss: 1.642745\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 744000 ---\n",
      "Time elapsed: 81203.6 seconds\n",
      "Minibatch loss: 1.582880\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 745000 ---\n",
      "Time elapsed: 81312.2 seconds\n",
      "Minibatch loss: 1.368074\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 746000 ---\n",
      "Time elapsed: 81421.0 seconds\n",
      "Minibatch loss: 1.667368\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 747000 ---\n",
      "Time elapsed: 81530.2 seconds\n",
      "Minibatch loss: 1.353286\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 748000 ---\n",
      "Time elapsed: 81638.9 seconds\n",
      "Minibatch loss: 1.146953\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 749000 ---\n",
      "Time elapsed: 81747.8 seconds\n",
      "Minibatch loss: 1.679225\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 750000 ---\n",
      "Time elapsed: 81856.3 seconds\n",
      "Minibatch loss: 1.534591\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 751000 ---\n",
      "Time elapsed: 81965.1 seconds\n",
      "Minibatch loss: 1.540575\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 752000 ---\n",
      "Time elapsed: 82073.7 seconds\n",
      "Minibatch loss: 1.659710\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 753000 ---\n",
      "Time elapsed: 82182.6 seconds\n",
      "Minibatch loss: 2.083958\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 754000 ---\n",
      "Time elapsed: 82291.2 seconds\n",
      "Minibatch loss: 1.830345\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 755000 ---\n",
      "Time elapsed: 82400.1 seconds\n",
      "Minibatch loss: 1.994161\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 756000 ---\n",
      "Time elapsed: 82508.6 seconds\n",
      "Minibatch loss: 1.736685\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 757000 ---\n",
      "Time elapsed: 82617.4 seconds\n",
      "Minibatch loss: 1.510556\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 758000 ---\n",
      "Time elapsed: 82726.5 seconds\n",
      "Minibatch loss: 1.516162\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 759000 ---\n",
      "Time elapsed: 82835.2 seconds\n",
      "Minibatch loss: 1.410137\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 760000 ---\n",
      "Time elapsed: 82943.9 seconds\n",
      "Minibatch loss: 1.709666\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 761000 ---\n",
      "Time elapsed: 83052.8 seconds\n",
      "Minibatch loss: 1.632162\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 762000 ---\n",
      "Time elapsed: 83161.5 seconds\n",
      "Minibatch loss: 1.550644\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 763000 ---\n",
      "Time elapsed: 83269.8 seconds\n",
      "Minibatch loss: 1.588136\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 764000 ---\n",
      "Time elapsed: 83378.3 seconds\n",
      "Minibatch loss: 1.477741\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 765000 ---\n",
      "Time elapsed: 83487.4 seconds\n",
      "Minibatch loss: 1.550383\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 766000 ---\n",
      "Time elapsed: 83595.9 seconds\n",
      "Minibatch loss: 1.453405\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 767000 ---\n",
      "Time elapsed: 83704.3 seconds\n",
      "Minibatch loss: 1.387617\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 768000 ---\n",
      "Time elapsed: 83812.9 seconds\n",
      "Minibatch loss: 1.274733\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 769000 ---\n",
      "Time elapsed: 83921.4 seconds\n",
      "Minibatch loss: 1.697763\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 770000 ---\n",
      "Time elapsed: 84029.9 seconds\n",
      "Minibatch loss: 1.585870\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 771000 ---\n",
      "Time elapsed: 84138.9 seconds\n",
      "Minibatch loss: 1.321107\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 772000 ---\n",
      "Time elapsed: 84247.6 seconds\n",
      "Minibatch loss: 1.431271\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 773000 ---\n",
      "Time elapsed: 84356.6 seconds\n",
      "Minibatch loss: 1.617155\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 774000 ---\n",
      "Time elapsed: 84465.8 seconds\n",
      "Minibatch loss: 1.900350\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 775000 ---\n",
      "Time elapsed: 84574.7 seconds\n",
      "Minibatch loss: 1.414093\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 776000 ---\n",
      "Time elapsed: 84683.7 seconds\n",
      "Minibatch loss: 1.539834\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 777000 ---\n",
      "Time elapsed: 84792.4 seconds\n",
      "Minibatch loss: 1.258227\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 778000 ---\n",
      "Time elapsed: 84901.1 seconds\n",
      "Minibatch loss: 1.599437\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 779000 ---\n",
      "Time elapsed: 85009.9 seconds\n",
      "Minibatch loss: 1.452501\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 780000 ---\n",
      "Time elapsed: 85118.7 seconds\n",
      "Minibatch loss: 1.647787\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 781000 ---\n",
      "Time elapsed: 85227.3 seconds\n",
      "Minibatch loss: 1.489614\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 782000 ---\n",
      "Time elapsed: 85335.9 seconds\n",
      "Minibatch loss: 1.264141\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 783000 ---\n",
      "Time elapsed: 85444.5 seconds\n",
      "Minibatch loss: 1.545181\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 784000 ---\n",
      "Time elapsed: 85553.1 seconds\n",
      "Minibatch loss: 1.372292\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 785000 ---\n",
      "Time elapsed: 85661.7 seconds\n",
      "Minibatch loss: 1.291312\n",
      "Minibatch accuracy: 90.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 786000 ---\n",
      "Time elapsed: 85771.0 seconds\n",
      "Minibatch loss: 1.687385\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 787000 ---\n",
      "Time elapsed: 85880.2 seconds\n",
      "Minibatch loss: 2.145051\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 788000 ---\n",
      "Time elapsed: 85989.2 seconds\n",
      "Minibatch loss: 1.121526\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 789000 ---\n",
      "Time elapsed: 86097.8 seconds\n",
      "Minibatch loss: 1.112568\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 790000 ---\n",
      "Time elapsed: 86206.5 seconds\n",
      "Minibatch loss: 1.513374\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 791000 ---\n",
      "Time elapsed: 86315.4 seconds\n",
      "Minibatch loss: 1.600813\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 792000 ---\n",
      "Time elapsed: 86424.0 seconds\n",
      "Minibatch loss: 1.788515\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 793000 ---\n",
      "Time elapsed: 86532.4 seconds\n",
      "Minibatch loss: 1.727682\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 794000 ---\n",
      "Time elapsed: 86641.5 seconds\n",
      "Minibatch loss: 1.785226\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 795000 ---\n",
      "Time elapsed: 86750.6 seconds\n",
      "Minibatch loss: 1.262574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 796000 ---\n",
      "Time elapsed: 86859.2 seconds\n",
      "Minibatch loss: 1.747908\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 797000 ---\n",
      "Time elapsed: 86968.0 seconds\n",
      "Minibatch loss: 1.605513\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 798000 ---\n",
      "Time elapsed: 87077.0 seconds\n",
      "Minibatch loss: 1.457853\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 799000 ---\n",
      "Time elapsed: 87185.8 seconds\n",
      "Minibatch loss: 1.290009\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 800000 ---\n",
      "Time elapsed: 87294.3 seconds\n",
      "Minibatch loss: 1.271068\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 801000 ---\n",
      "Time elapsed: 87403.5 seconds\n",
      "Minibatch loss: 1.673148\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 802000 ---\n",
      "Time elapsed: 87512.1 seconds\n",
      "Minibatch loss: 1.576689\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 803000 ---\n",
      "Time elapsed: 87621.0 seconds\n",
      "Minibatch loss: 1.899814\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 804000 ---\n",
      "Time elapsed: 87729.8 seconds\n",
      "Minibatch loss: 1.770994\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 805000 ---\n",
      "Time elapsed: 87838.7 seconds\n",
      "Minibatch loss: 2.016664\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 806000 ---\n",
      "Time elapsed: 87947.9 seconds\n",
      "Minibatch loss: 1.597477\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.1%\n",
      "\n",
      "--- Step 807000 ---\n",
      "Time elapsed: 88056.4 seconds\n",
      "Minibatch loss: 1.411464\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 808000 ---\n",
      "Time elapsed: 88165.0 seconds\n",
      "Minibatch loss: 1.410960\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 809000 ---\n",
      "Time elapsed: 88273.7 seconds\n",
      "Minibatch loss: 2.272092\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 810000 ---\n",
      "Time elapsed: 88382.5 seconds\n",
      "Minibatch loss: 1.891783\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 811000 ---\n",
      "Time elapsed: 88491.3 seconds\n",
      "Minibatch loss: 1.312511\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 812000 ---\n",
      "Time elapsed: 88600.2 seconds\n",
      "Minibatch loss: 2.093209\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 813000 ---\n",
      "Time elapsed: 88710.0 seconds\n",
      "Minibatch loss: 1.890197\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 814000 ---\n",
      "Time elapsed: 88819.4 seconds\n",
      "Minibatch loss: 1.606215\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 815000 ---\n",
      "Time elapsed: 88928.3 seconds\n",
      "Minibatch loss: 1.480918\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 816000 ---\n",
      "Time elapsed: 89037.0 seconds\n",
      "Minibatch loss: 1.386606\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 817000 ---\n",
      "Time elapsed: 89146.1 seconds\n",
      "Minibatch loss: 1.485438\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 818000 ---\n",
      "Time elapsed: 89255.4 seconds\n",
      "Minibatch loss: 1.449233\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 819000 ---\n",
      "Time elapsed: 89364.4 seconds\n",
      "Minibatch loss: 1.604908\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 820000 ---\n",
      "Time elapsed: 89473.0 seconds\n",
      "Minibatch loss: 1.527847\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 821000 ---\n",
      "Time elapsed: 89581.8 seconds\n",
      "Minibatch loss: 1.628922\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 822000 ---\n",
      "Time elapsed: 89690.8 seconds\n",
      "Minibatch loss: 1.414756\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 823000 ---\n",
      "Time elapsed: 89800.1 seconds\n",
      "Minibatch loss: 1.913507\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 824000 ---\n",
      "Time elapsed: 89908.6 seconds\n",
      "Minibatch loss: 1.582924\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 825000 ---\n",
      "Time elapsed: 90017.3 seconds\n",
      "Minibatch loss: 1.441486\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 826000 ---\n",
      "Time elapsed: 90125.8 seconds\n",
      "Minibatch loss: 1.469882\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 827000 ---\n",
      "Time elapsed: 90234.5 seconds\n",
      "Minibatch loss: 1.591740\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 828000 ---\n",
      "Time elapsed: 90343.2 seconds\n",
      "Minibatch loss: 1.689315\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 829000 ---\n",
      "Time elapsed: 90452.0 seconds\n",
      "Minibatch loss: 1.649899\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 830000 ---\n",
      "Time elapsed: 90561.1 seconds\n",
      "Minibatch loss: 1.180982\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 831000 ---\n",
      "Time elapsed: 90669.7 seconds\n",
      "Minibatch loss: 1.692978\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 832000 ---\n",
      "Time elapsed: 90778.5 seconds\n",
      "Minibatch loss: 1.161273\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 833000 ---\n",
      "Time elapsed: 90887.2 seconds\n",
      "Minibatch loss: 1.274554\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 834000 ---\n",
      "Time elapsed: 90996.0 seconds\n",
      "Minibatch loss: 1.792206\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 835000 ---\n",
      "Time elapsed: 91104.9 seconds\n",
      "Minibatch loss: 1.417036\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 836000 ---\n",
      "Time elapsed: 91214.1 seconds\n",
      "Minibatch loss: 1.322970\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 837000 ---\n",
      "Time elapsed: 91322.9 seconds\n",
      "Minibatch loss: 1.614248\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 838000 ---\n",
      "Time elapsed: 91431.6 seconds\n",
      "Minibatch loss: 1.325053\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 839000 ---\n",
      "Time elapsed: 91540.6 seconds\n",
      "Minibatch loss: 1.818296\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 840000 ---\n",
      "Time elapsed: 91650.2 seconds\n",
      "Minibatch loss: 1.740522\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 841000 ---\n",
      "Time elapsed: 91759.0 seconds\n",
      "Minibatch loss: 1.511955\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 842000 ---\n",
      "Time elapsed: 91867.7 seconds\n",
      "Minibatch loss: 1.397458\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 843000 ---\n",
      "Time elapsed: 91977.0 seconds\n",
      "Minibatch loss: 1.520728\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 844000 ---\n",
      "Time elapsed: 92085.6 seconds\n",
      "Minibatch loss: 1.467383\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 845000 ---\n",
      "Time elapsed: 92194.4 seconds\n",
      "Minibatch loss: 1.401921\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 846000 ---\n",
      "Time elapsed: 92303.1 seconds\n",
      "Minibatch loss: 1.374520\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 847000 ---\n",
      "Time elapsed: 92412.0 seconds\n",
      "Minibatch loss: 1.816839\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 848000 ---\n",
      "Time elapsed: 92521.0 seconds\n",
      "Minibatch loss: 1.382064\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 849000 ---\n",
      "Time elapsed: 92629.6 seconds\n",
      "Minibatch loss: 1.698671\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 850000 ---\n",
      "Time elapsed: 92738.5 seconds\n",
      "Minibatch loss: 1.469718\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 851000 ---\n",
      "Time elapsed: 92847.3 seconds\n",
      "Minibatch loss: 1.349162\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 852000 ---\n",
      "Time elapsed: 92956.1 seconds\n",
      "Minibatch loss: 1.523275\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 853000 ---\n",
      "Time elapsed: 93064.7 seconds\n",
      "Minibatch loss: 1.338080\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 854000 ---\n",
      "Time elapsed: 93173.4 seconds\n",
      "Minibatch loss: 1.251883\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 855000 ---\n",
      "Time elapsed: 93281.9 seconds\n",
      "Minibatch loss: 1.578807\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 856000 ---\n",
      "Time elapsed: 93390.6 seconds\n",
      "Minibatch loss: 1.342266\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 857000 ---\n",
      "Time elapsed: 93499.3 seconds\n",
      "Minibatch loss: 1.501850\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 858000 ---\n",
      "Time elapsed: 93607.9 seconds\n",
      "Minibatch loss: 1.553117\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 859000 ---\n",
      "Time elapsed: 93716.7 seconds\n",
      "Minibatch loss: 1.345837\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 860000 ---\n",
      "Time elapsed: 93826.0 seconds\n",
      "Minibatch loss: 1.283730\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 861000 ---\n",
      "Time elapsed: 93935.0 seconds\n",
      "Minibatch loss: 1.409569\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 862000 ---\n",
      "Time elapsed: 94043.8 seconds\n",
      "Minibatch loss: 1.708718\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 863000 ---\n",
      "Time elapsed: 94152.7 seconds\n",
      "Minibatch loss: 1.416198\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 864000 ---\n",
      "Time elapsed: 94261.5 seconds\n",
      "Minibatch loss: 1.884933\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 865000 ---\n",
      "Time elapsed: 94370.3 seconds\n",
      "Minibatch loss: 1.421696\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 866000 ---\n",
      "Time elapsed: 94479.3 seconds\n",
      "Minibatch loss: 1.529028\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 867000 ---\n",
      "Time elapsed: 94588.2 seconds\n",
      "Minibatch loss: 1.667251\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 868000 ---\n",
      "Time elapsed: 94697.2 seconds\n",
      "Minibatch loss: 1.598548\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 869000 ---\n",
      "Time elapsed: 94806.1 seconds\n",
      "Minibatch loss: 1.567859\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 870000 ---\n",
      "Time elapsed: 94914.9 seconds\n",
      "Minibatch loss: 1.194146\n",
      "Minibatch accuracy: 92.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 871000 ---\n",
      "Time elapsed: 95023.5 seconds\n",
      "Minibatch loss: 1.300061\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 872000 ---\n",
      "Time elapsed: 95132.4 seconds\n",
      "Minibatch loss: 1.232325\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 873000 ---\n",
      "Time elapsed: 95241.3 seconds\n",
      "Minibatch loss: 1.719874\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 874000 ---\n",
      "Time elapsed: 95350.1 seconds\n",
      "Minibatch loss: 1.646691\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 875000 ---\n",
      "Time elapsed: 95458.7 seconds\n",
      "Minibatch loss: 1.702082\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 876000 ---\n",
      "Time elapsed: 95567.7 seconds\n",
      "Minibatch loss: 1.252255\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 877000 ---\n",
      "Time elapsed: 95676.4 seconds\n",
      "Minibatch loss: 1.548376\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 878000 ---\n",
      "Time elapsed: 95785.0 seconds\n",
      "Minibatch loss: 1.395202\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 879000 ---\n",
      "Time elapsed: 95893.8 seconds\n",
      "Minibatch loss: 1.317301\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 880000 ---\n",
      "Time elapsed: 96003.1 seconds\n",
      "Minibatch loss: 1.611150\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 881000 ---\n",
      "Time elapsed: 96112.3 seconds\n",
      "Minibatch loss: 1.429502\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 882000 ---\n",
      "Time elapsed: 96221.1 seconds\n",
      "Minibatch loss: 1.376244\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 883000 ---\n",
      "Time elapsed: 96329.8 seconds\n",
      "Minibatch loss: 1.577971\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 884000 ---\n",
      "Time elapsed: 96438.6 seconds\n",
      "Minibatch loss: 1.350988\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 885000 ---\n",
      "Time elapsed: 96547.4 seconds\n",
      "Minibatch loss: 1.693471\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 886000 ---\n",
      "Time elapsed: 96656.1 seconds\n",
      "Minibatch loss: 1.429579\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 93.2%\n",
      "\n",
      "--- Step 887000 ---\n",
      "Time elapsed: 96764.6 seconds\n",
      "Minibatch loss: 1.477031\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 888000 ---\n",
      "Time elapsed: 96873.4 seconds\n",
      "Minibatch loss: 1.667111\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 889000 ---\n",
      "Time elapsed: 96982.3 seconds\n",
      "Minibatch loss: 1.651692\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 890000 ---\n",
      "Time elapsed: 97091.2 seconds\n",
      "Minibatch loss: 1.315558\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 891000 ---\n",
      "Time elapsed: 97199.9 seconds\n",
      "Minibatch loss: 1.622965\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 892000 ---\n",
      "Time elapsed: 97308.9 seconds\n",
      "Minibatch loss: 1.145776\n",
      "Minibatch accuracy: 91.9%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 893000 ---\n",
      "Time elapsed: 97417.5 seconds\n",
      "Minibatch loss: 1.526733\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 894000 ---\n",
      "Time elapsed: 97526.2 seconds\n",
      "Minibatch loss: 1.545451\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 895000 ---\n",
      "Time elapsed: 97635.2 seconds\n",
      "Minibatch loss: 1.355612\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 896000 ---\n",
      "Time elapsed: 97743.8 seconds\n",
      "Minibatch loss: 1.875067\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 897000 ---\n",
      "Time elapsed: 97852.4 seconds\n",
      "Minibatch loss: 1.435753\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 898000 ---\n",
      "Time elapsed: 97961.2 seconds\n",
      "Minibatch loss: 1.141098\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 899000 ---\n",
      "Time elapsed: 98070.6 seconds\n",
      "Minibatch loss: 1.442300\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 900000 ---\n",
      "Time elapsed: 98180.7 seconds\n",
      "Minibatch loss: 1.532166\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 901000 ---\n",
      "Time elapsed: 98289.8 seconds\n",
      "Minibatch loss: 1.759976\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 902000 ---\n",
      "Time elapsed: 98398.4 seconds\n",
      "Minibatch loss: 1.204534\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 903000 ---\n",
      "Time elapsed: 98507.2 seconds\n",
      "Minibatch loss: 1.342270\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 904000 ---\n",
      "Time elapsed: 98617.9 seconds\n",
      "Minibatch loss: 1.674169\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 905000 ---\n",
      "Time elapsed: 98726.9 seconds\n",
      "Minibatch loss: 1.517002\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 906000 ---\n",
      "Time elapsed: 98836.1 seconds\n",
      "Minibatch loss: 1.720813\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 907000 ---\n",
      "Time elapsed: 98944.7 seconds\n",
      "Minibatch loss: 1.397839\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 908000 ---\n",
      "Time elapsed: 99053.8 seconds\n",
      "Minibatch loss: 1.662468\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 909000 ---\n",
      "Time elapsed: 99163.0 seconds\n",
      "Minibatch loss: 1.828183\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 910000 ---\n",
      "Time elapsed: 99271.8 seconds\n",
      "Minibatch loss: 1.438007\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 911000 ---\n",
      "Time elapsed: 99380.9 seconds\n",
      "Minibatch loss: 1.442958\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 912000 ---\n",
      "Time elapsed: 99489.5 seconds\n",
      "Minibatch loss: 1.423424\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 913000 ---\n",
      "Time elapsed: 99598.3 seconds\n",
      "Minibatch loss: 1.811266\n",
      "Minibatch accuracy: 85.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 914000 ---\n",
      "Time elapsed: 99707.2 seconds\n",
      "Minibatch loss: 1.521544\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 915000 ---\n",
      "Time elapsed: 99816.1 seconds\n",
      "Minibatch loss: 1.237163\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 916000 ---\n",
      "Time elapsed: 99924.9 seconds\n",
      "Minibatch loss: 1.428461\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 917000 ---\n",
      "Time elapsed: 100034.2 seconds\n",
      "Minibatch loss: 1.465653\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 918000 ---\n",
      "Time elapsed: 100143.4 seconds\n",
      "Minibatch loss: 1.521884\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 919000 ---\n",
      "Time elapsed: 100251.9 seconds\n",
      "Minibatch loss: 1.195611\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 920000 ---\n",
      "Time elapsed: 100360.4 seconds\n",
      "Minibatch loss: 1.840229\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 921000 ---\n",
      "Time elapsed: 100468.9 seconds\n",
      "Minibatch loss: 1.670740\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 922000 ---\n",
      "Time elapsed: 100577.7 seconds\n",
      "Minibatch loss: 1.758822\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 923000 ---\n",
      "Time elapsed: 100686.8 seconds\n",
      "Minibatch loss: 1.592520\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 924000 ---\n",
      "Time elapsed: 100795.7 seconds\n",
      "Minibatch loss: 1.312596\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 925000 ---\n",
      "Time elapsed: 100904.4 seconds\n",
      "Minibatch loss: 1.405095\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.5%\n",
      "\n",
      "--- Step 926000 ---\n",
      "Time elapsed: 101013.0 seconds\n",
      "Minibatch loss: 1.572018\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 927000 ---\n",
      "Time elapsed: 101123.1 seconds\n",
      "Minibatch loss: 1.798959\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 928000 ---\n",
      "Time elapsed: 101233.4 seconds\n",
      "Minibatch loss: 1.689834\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 929000 ---\n",
      "Time elapsed: 101342.0 seconds\n",
      "Minibatch loss: 1.262333\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 930000 ---\n",
      "Time elapsed: 101450.8 seconds\n",
      "Minibatch loss: 1.537485\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 931000 ---\n",
      "Time elapsed: 101559.9 seconds\n",
      "Minibatch loss: 1.514640\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 932000 ---\n",
      "Time elapsed: 101668.7 seconds\n",
      "Minibatch loss: 1.304846\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 933000 ---\n",
      "Time elapsed: 101777.6 seconds\n",
      "Minibatch loss: 1.447085\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 934000 ---\n",
      "Time elapsed: 101886.3 seconds\n",
      "Minibatch loss: 1.708317\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 935000 ---\n",
      "Time elapsed: 101995.2 seconds\n",
      "Minibatch loss: 1.574519\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 936000 ---\n",
      "Time elapsed: 102104.2 seconds\n",
      "Minibatch loss: 1.325875\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 937000 ---\n",
      "Time elapsed: 102213.2 seconds\n",
      "Minibatch loss: 1.391859\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 938000 ---\n",
      "Time elapsed: 102322.1 seconds\n",
      "Minibatch loss: 1.331539\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 939000 ---\n",
      "Time elapsed: 102431.3 seconds\n",
      "Minibatch loss: 1.894993\n",
      "Minibatch accuracy: 85.6%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 940000 ---\n",
      "Time elapsed: 102539.9 seconds\n",
      "Minibatch loss: 1.572451\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 941000 ---\n",
      "Time elapsed: 102648.7 seconds\n",
      "Minibatch loss: 1.734989\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 942000 ---\n",
      "Time elapsed: 102757.7 seconds\n",
      "Minibatch loss: 1.618320\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 943000 ---\n",
      "Time elapsed: 102866.8 seconds\n",
      "Minibatch loss: 1.284804\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 944000 ---\n",
      "Time elapsed: 102975.3 seconds\n",
      "Minibatch loss: 1.265291\n",
      "Minibatch accuracy: 90.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 945000 ---\n",
      "Time elapsed: 103083.9 seconds\n",
      "Minibatch loss: 1.846689\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 946000 ---\n",
      "Time elapsed: 103192.5 seconds\n",
      "Minibatch loss: 1.154011\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 947000 ---\n",
      "Time elapsed: 103301.2 seconds\n",
      "Minibatch loss: 1.290392\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 948000 ---\n",
      "Time elapsed: 103409.8 seconds\n",
      "Minibatch loss: 1.455027\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 949000 ---\n",
      "Time elapsed: 103518.5 seconds\n",
      "Minibatch loss: 1.428282\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 950000 ---\n",
      "Time elapsed: 103627.0 seconds\n",
      "Minibatch loss: 1.572760\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 951000 ---\n",
      "Time elapsed: 103736.1 seconds\n",
      "Minibatch loss: 1.526704\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 952000 ---\n",
      "Time elapsed: 103845.0 seconds\n",
      "Minibatch loss: 1.765359\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 953000 ---\n",
      "Time elapsed: 103953.5 seconds\n",
      "Minibatch loss: 1.374203\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 954000 ---\n",
      "Time elapsed: 104062.4 seconds\n",
      "Minibatch loss: 1.507601\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 955000 ---\n",
      "Time elapsed: 104171.2 seconds\n",
      "Minibatch loss: 1.634340\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 956000 ---\n",
      "Time elapsed: 104280.2 seconds\n",
      "Minibatch loss: 1.564260\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 957000 ---\n",
      "Time elapsed: 104389.0 seconds\n",
      "Minibatch loss: 1.686983\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 958000 ---\n",
      "Time elapsed: 104497.7 seconds\n",
      "Minibatch loss: 1.446595\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 959000 ---\n",
      "Time elapsed: 104606.6 seconds\n",
      "Minibatch loss: 1.617459\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 960000 ---\n",
      "Time elapsed: 104715.6 seconds\n",
      "Minibatch loss: 1.358286\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 961000 ---\n",
      "Time elapsed: 104824.0 seconds\n",
      "Minibatch loss: 1.376582\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 962000 ---\n",
      "Time elapsed: 104932.8 seconds\n",
      "Minibatch loss: 1.607396\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 963000 ---\n",
      "Time elapsed: 105041.5 seconds\n",
      "Minibatch loss: 1.469906\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 964000 ---\n",
      "Time elapsed: 105150.2 seconds\n",
      "Minibatch loss: 1.430102\n",
      "Minibatch accuracy: 88.4%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 965000 ---\n",
      "Time elapsed: 105259.1 seconds\n",
      "Minibatch loss: 1.496987\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 966000 ---\n",
      "Time elapsed: 105368.1 seconds\n",
      "Minibatch loss: 1.420666\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 967000 ---\n",
      "Time elapsed: 105476.9 seconds\n",
      "Minibatch loss: 1.390135\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 968000 ---\n",
      "Time elapsed: 105585.4 seconds\n",
      "Minibatch loss: 1.373849\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 969000 ---\n",
      "Time elapsed: 105694.1 seconds\n",
      "Minibatch loss: 1.509534\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 970000 ---\n",
      "Time elapsed: 105802.7 seconds\n",
      "Minibatch loss: 1.552147\n",
      "Minibatch accuracy: 87.3%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 971000 ---\n",
      "Time elapsed: 105911.3 seconds\n",
      "Minibatch loss: 1.884792\n",
      "Minibatch accuracy: 86.6%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 972000 ---\n",
      "Time elapsed: 106020.0 seconds\n",
      "Minibatch loss: 1.622044\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 973000 ---\n",
      "Time elapsed: 106128.8 seconds\n",
      "Minibatch loss: 1.297323\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 974000 ---\n",
      "Time elapsed: 106237.5 seconds\n",
      "Minibatch loss: 1.797010\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 975000 ---\n",
      "Time elapsed: 106346.4 seconds\n",
      "Minibatch loss: 1.577373\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 976000 ---\n",
      "Time elapsed: 106455.3 seconds\n",
      "Minibatch loss: 1.266783\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 977000 ---\n",
      "Time elapsed: 106564.8 seconds\n",
      "Minibatch loss: 1.343937\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 978000 ---\n",
      "Time elapsed: 106673.5 seconds\n",
      "Minibatch loss: 1.436933\n",
      "Minibatch accuracy: 87.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 979000 ---\n",
      "Time elapsed: 106782.8 seconds\n",
      "Minibatch loss: 1.444096\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 980000 ---\n",
      "Time elapsed: 106891.8 seconds\n",
      "Minibatch loss: 1.184764\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 981000 ---\n",
      "Time elapsed: 107000.6 seconds\n",
      "Minibatch loss: 1.690049\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 982000 ---\n",
      "Time elapsed: 107109.7 seconds\n",
      "Minibatch loss: 1.662674\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 983000 ---\n",
      "Time elapsed: 107219.1 seconds\n",
      "Minibatch loss: 1.315067\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 984000 ---\n",
      "Time elapsed: 107328.1 seconds\n",
      "Minibatch loss: 1.466298\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 985000 ---\n",
      "Time elapsed: 107437.2 seconds\n",
      "Minibatch loss: 1.550018\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 93.5%\n",
      "\n",
      "--- Step 986000 ---\n",
      "Time elapsed: 107546.0 seconds\n",
      "Minibatch loss: 1.650924\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 987000 ---\n",
      "Time elapsed: 107654.7 seconds\n",
      "Minibatch loss: 1.727720\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.5%\n",
      "\n",
      "--- Step 988000 ---\n",
      "Time elapsed: 107763.6 seconds\n",
      "Minibatch loss: 1.487329\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 989000 ---\n",
      "Time elapsed: 107873.0 seconds\n",
      "Minibatch loss: 1.219977\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 990000 ---\n",
      "Time elapsed: 107981.9 seconds\n",
      "Minibatch loss: 1.496002\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 991000 ---\n",
      "Time elapsed: 108090.8 seconds\n",
      "Minibatch loss: 1.622674\n",
      "Minibatch accuracy: 88.0%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 992000 ---\n",
      "Time elapsed: 108199.8 seconds\n",
      "Minibatch loss: 1.714671\n",
      "Minibatch accuracy: 87.0%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Step 993000 ---\n",
      "Time elapsed: 108308.4 seconds\n",
      "Minibatch loss: 1.423385\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 93.5%\n",
      "\n",
      "--- Step 994000 ---\n",
      "Time elapsed: 108417.4 seconds\n",
      "Minibatch loss: 1.515155\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 995000 ---\n",
      "Time elapsed: 108526.4 seconds\n",
      "Minibatch loss: 1.534970\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 996000 ---\n",
      "Time elapsed: 108635.0 seconds\n",
      "Minibatch loss: 1.302827\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 93.5%\n",
      "\n",
      "--- Step 997000 ---\n",
      "Time elapsed: 108744.0 seconds\n",
      "Minibatch loss: 1.785767\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 998000 ---\n",
      "Time elapsed: 108852.8 seconds\n",
      "Minibatch loss: 1.706149\n",
      "Minibatch accuracy: 87.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 999000 ---\n",
      "Time elapsed: 108961.5 seconds\n",
      "Minibatch loss: 1.512334\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 93.4%\n",
      "\n",
      "--- Step 1000000 ---\n",
      "Time elapsed: 109070.0 seconds\n",
      "Minibatch loss: 1.147319\n",
      "Minibatch accuracy: 92.3%\n",
      "Validation accuracy: 93.3%\n",
      "\n",
      "--- Test ---\n",
      "Test accuracy: 93.5%\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Run the graph \"\"\"\n",
    "num_steps = 1000001\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Intialized\\n\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset  = (step * batch_size) % (train_l.shape[0] - batch_size)\n",
    "        batch   = train[offset:(offset + batch_size), :, :, :]\n",
    "        batch_l = train_l[offset:(offset + batch_size), :]\n",
    "        feed_dict = {\n",
    "            tf_train   : batch,\n",
    "            tf_train_l : batch_l\n",
    "        }\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_pred], feed_dict = feed_dict\n",
    "        )\n",
    "        \n",
    "        if (step % 1000 == 0):\n",
    "            print('--- Step %d ---' % (step))\n",
    "            print('Time elapsed: %.1f seconds' % (time.time() - start_time))\n",
    "            print('Minibatch loss: %f' % (l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_l))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_pred.eval(), valid_l))\n",
    "            print()\n",
    "    \n",
    "    print('--- Test ---')\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_pred.eval(), test_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "_What approach did you take in coming up with a solution to this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "I knew I wanted to use a convolution neural net, because convolutions are well suited to detecting objects in images. This will become especially important in the subsequent portions of this project, where the numbers can appear in any part of the image, at any angle/rotation, etc.\n",
    "\n",
    "My final convolution architecture (described in following section) was based on previously successful designs. To quickly summarise, the convolution layers increase the input depth, enabling detection of objects across the image, and then fully connected layers handle the learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "My final architecture involves convoluations and fully connected layers. It can be summarised as:\n",
    "\n",
    "- 3 x convolution layers with max pooling and relu activation; depth increases with each additional layer.\n",
    "- 2 x fully connected layers with relu activation; relu numbers decrease with each additional layer.\n",
    "- Drop out is applied.\n",
    "- 5 x ordered logistic classifiers trained on five ordered input labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "_How did you train your model? How did you generate your synthetic dataset?_ Include examples of images from the synthetic data you constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Training was achieved using stochastic gradient descent: training on small, random samples of the data. Loss was summed across the five logistic classifiers.\n",
    "\n",
    "The samples used for training came from a synthetic dataset of images including one to five numbers, that was constructed using the MNIST dataset. Images in the synthetic datatset were constructed as follows:\n",
    "\n",
    "- Randomly sample five images from MNIST (five being the maximum allowable sequence length).\n",
    "- Randomly sample a sequence length of 1 to 5.\n",
    "- For sequence lengths `n` that were less than 5, replace the last `5 - n` images with blank regions and label them `-1`.\n",
    "\n",
    "This process was done to create 100000 sequences for training data, and 30000 for validation and 30000 for test data.\n",
    "\n",
    "Many examples are shown above. Here is another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image below should show [ 0  0  7  9 -1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAACECAYAAAB1VJkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEUUIUvIiKSAv8PKTY1m8SC++0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bdfe190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"The image below should show\", train_labels[train_seq[100]])\n",
    "show(np.hstack(train_images[train_seq[100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 2: Train a Model on a Realistic Dataset\n",
    "Once you have settled on a good architecture, you can train your model on real data. In particular, the [Street View House Numbers (SVHN)](http://ufldl.stanford.edu/housenumbers/) dataset is a good large-scale dataset collected from house numbers in Google Street View. Training on this more challenging dataset, where the digits are not neatly lined-up and have various skews, fonts and colors, likely means you have to do some hyperparameter exploration to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Street View House Numbers (SVNH) datasets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found train.tar.gz but no size to verify against\n",
      "Found test.tar.gz but no size to verify against\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Import the SVNH Data \"\"\"\n",
    "url = 'http://ufldl.stanford.edu/housenumbers/'\n",
    "last_percent_reported = None\n",
    "\n",
    "train_svnh_filename = maybe_download('train.tar.gz', None)\n",
    "test_svnh_filename  = maybe_download('test.tar.gz', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train already present - Skipping extraction of train.tar.gz.\n",
      "test already present - Skipping extraction of test.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "def maybe_extract_svnh(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    # extract data\n",
    "    if os.path.isdir(root) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "    return root\n",
    "\n",
    "train_svnh_folder = maybe_extract_svnh(train_svnh_filename)\n",
    "test_svnh_folder = maybe_extract_svnh(test_svnh_filename)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded datasets are stored in two folders. Each folder contains the relevant .png files and a digitStruct.mat file containing labels and bounding box information. We'll convert these to useable numpy arrays..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "SUPPORT FUNCTIONS\n",
    "Ref: https://discussions.udacity.com/t/how-to-deal-with-mat-files/160657\n",
    "'''\n",
    "\n",
    "# The DigitStructFile is just a wrapper around the h5py data.  It basically references \n",
    "#    inf:              The input h5 matlab file\n",
    "#    digitStructName   The h5 ref to all the file names\n",
    "#    digitStructBbox   The h5 ref to all struc data\n",
    "class DigitStructFile:\n",
    "    def __init__(self, inf):\n",
    "        self.inf = h5py.File(inf, 'r')\n",
    "        self.digitStructName = self.inf['digitStruct']['name']\n",
    "        self.digitStructBbox = self.inf['digitStruct']['bbox']\n",
    "\n",
    "# getName returns the 'name' string for for the n(th) digitStruct. \n",
    "    def getName(self,n):\n",
    "        return ''.join([chr(c[0]) for c in self.inf[self.digitStructName[n][0]].value])\n",
    "\n",
    "# bboxHelper handles the coding difference when there is exactly one bbox or an array of bbox. \n",
    "    def bboxHelper(self,attr):\n",
    "        if (len(attr) > 1):\n",
    "            attr = [self.inf[attr.value[j].item()].value[0][0] for j in range(len(attr))]\n",
    "        else:\n",
    "            attr = [attr.value[0][0]]\n",
    "        return attr\n",
    "\n",
    "# getBbox returns a dict of data for the n(th) bbox. \n",
    "    def getBbox(self,n):\n",
    "        bbox = {}\n",
    "        bb = self.digitStructBbox[n].item()\n",
    "        bbox['height'] = self.bboxHelper(self.inf[bb][\"height\"])\n",
    "        bbox['label'] = self.bboxHelper(self.inf[bb][\"label\"])\n",
    "        bbox['left'] = self.bboxHelper(self.inf[bb][\"left\"])\n",
    "        bbox['top'] = self.bboxHelper(self.inf[bb][\"top\"])\n",
    "        bbox['width'] = self.bboxHelper(self.inf[bb][\"width\"])\n",
    "        return bbox\n",
    "\n",
    "    def getDigitStructure(self,n):\n",
    "        s = self.getBbox(n)\n",
    "        s['name']=self.getName(n)\n",
    "        return s\n",
    "# getAllDigitStructure returns all the digitStruct from the input file.     \n",
    "    def getAllDigitStructure(self):\n",
    "        return [self.getDigitStructure(i) for i in range(len(self.digitStructName))]\n",
    "\n",
    "# Return a restructured version of the dataset (one structure by boxed digit).\n",
    "#\n",
    "#   Return a list of such dicts :\n",
    "#      'filename' : filename of the samples\n",
    "#      'boxes' : list of such dicts (one by digit) :\n",
    "#          'label' : 1 to 9 corresponding digits. 10 for digit '0' in image.\n",
    "#          'left', 'top' : position of bounding box\n",
    "#          'width', 'height' : dimension of bounding box\n",
    "#\n",
    "# Note: We may turn this to a generator, if memory issues arise.\n",
    "    def getAllDigitStructure_ByDigit(self):\n",
    "        pictDat = self.getAllDigitStructure()\n",
    "        result = []\n",
    "        structCnt = 1\n",
    "        for i in range(len(pictDat)):\n",
    "            item = { 'filename' : pictDat[i][\"name\"] }\n",
    "            figures = []\n",
    "            for j in range(len(pictDat[i]['height'])):\n",
    "               figure = {}\n",
    "               figure['height'] = pictDat[i]['height'][j]\n",
    "               figure['label']  = pictDat[i]['label'][j]\n",
    "               figure['left']   = pictDat[i]['left'][j]\n",
    "               figure['top']    = pictDat[i]['top'][j]\n",
    "               figure['width']  = pictDat[i]['width'][j]\n",
    "               figures.append(figure)\n",
    "            structCnt = structCnt + 1\n",
    "            item['boxes'] = figures\n",
    "            result.append(item)\n",
    "        return result\n",
    "\n",
    "def digitStructMatAsList(directory):\n",
    "    fin = os.path.join(directory, 'digitStruct.mat')\n",
    "    dsf = DigitStructFile(fin)\n",
    "    return dsf.getAllDigitStructure_ByDigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all .mat file information into lists\n",
    "# - Takes some time\n",
    "train_svhn_data = digitStructMatAsList(train_svnh_folder)\n",
    "test_svhn_data  = digitStructMatAsList(test_svnh_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example filename\n",
      "\t 1.png\n",
      "Example box information with 2 digits\n",
      "\t [{'width': 81.0, 'top': 77.0, 'label': 1.0, 'left': 246.0, 'height': 219.0}, {'width': 96.0, 'top': 81.0, 'label': 9.0, 'left': 323.0, 'height': 219.0}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Data check \"\"\"\n",
    "print('Example filename\\n\\t', train_svhn_data[0]['filename'])\n",
    "eg_box = train_svhn_data[0]['boxes']\n",
    "print('Example box information with', len(eg_box), 'digits\\n\\t', eg_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create greyscale `.thumbnail` versions of all images.\n",
    "This means all images will have same dimensions and a depth of 1\n",
    "\n",
    "References:\n",
    "    http://pillow.readthedocs.io/en/3.1.x/reference/Image.html\n",
    "    http://stackoverflow.com/questions/12201577/how-can-i-convert-an-rgb-image-into-grayscale-in-python\n",
    "\"\"\"\n",
    "\n",
    "size = 64, 64\n",
    "\n",
    "# Create `*.thumbnail` versions of each training image\n",
    "for infile in glob.glob(train_svnh_folder + \"/*.png\"):\n",
    "    file, ext = os.path.splitext(infile)\n",
    "    im = Image.open(infile).convert('L')\n",
    "    im.thumbnail(size)\n",
    "    # Paste onto white background to get size uniform for all images\n",
    "    background = Image.new('L', size, color = 0)  # (255, 255, 255, 0)\n",
    "    background.paste(\n",
    "        im, (int((size[0] - im.size[0]) / 2), int((size[1] - im.size[1]) / 2))\n",
    "    )\n",
    "    background.save(file + \".thumbnail\", \"PNG\")\n",
    "\n",
    "# Create `*.thumbnail` versions of each test image\n",
    "for infile in glob.glob(test_svnh_folder + \"/*.png\"):\n",
    "    file, ext = os.path.splitext(infile)\n",
    "    im = Image.open(infile).convert('L')\n",
    "    im.thumbnail(size)\n",
    "    # Paste onto black background to get size uniform for all images\n",
    "    background = Image.new('L', size, color = 0)  # (255, 255, 255, 0)\n",
    "    background.paste(\n",
    "        im, (int((size[0] - im.size[0]) / 2), int((size[1] - im.size[1]) / 2))\n",
    "    )\n",
    "    background.save(file + \".thumbnail\", \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert lists to numpy arrays.\n",
    "Use the `.thumbnail` images\n",
    "\"\"\"\n",
    "\n",
    "svhn_max_digits = 6\n",
    "\n",
    "def list_2_nparrays(directory, svhn_data_list):\n",
    "    labels = []\n",
    "    png = []\n",
    "    for img in svhn_data_list:\n",
    "        #png.append(misc.imread(os.path.join(directory, img['filename'])))  # This uses original images\n",
    "        png.append(misc.imread(os.path.join(directory, img['filename'][0] + '.thumbnail')))\n",
    "        labs = []\n",
    "        for b in img['boxes']:\n",
    "            labs.append(b['label'])\n",
    "        # Pad to full sequence length if necessary\n",
    "        if(len(labs) < svhn_max_digits):\n",
    "            labs = np.pad(labs, (0, svhn_max_digits - len(labs)), 'constant', constant_values=(-1))\n",
    "        labels.append(labs)\n",
    "    \n",
    "    return (np.asarray(png), np.asarray(labels))\n",
    "\n",
    "train_svhn_images, train_svhn_labels = list_2_nparrays(train_svnh_folder, train_svhn_data)\n",
    "test_svhn_images,  test_svhn_labels  = list_2_nparrays(test_svnh_folder,  test_svhn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images and labels:\t (33402, 64, 64) (33402, 6)\n",
      "Shape of test images and labels:\t (13068, 64, 64) (13068, 6)\n",
      "\n",
      "Should see [ 1.  9. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFdCAYAAADSR9wBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127879e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should see [ 5. -1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFdCAYAAADSR9wBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126a19450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should see [ 2.  3. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFdCAYAAADSR9wBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nasgiLEkN/T/oTENlnQEJAAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127bb6590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should see [  2.   1.  10.  -1.  -1.  -1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFdCAYAAADSR9wBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAHMlJREFUeJzt3X2wnFWd4PHvj/ASAuZGGUxWRSeaGSfCiHvjgIjAaiwY\nrJLBrSnWOzqsM+VS6FLFZqdq0Foss6TKncIaL+uMTGFtbY3IuFP8M4LsrAFhXtBAKBIEWWIkQHiL\niRLHBBPeOftH92X7Pn1y7+mX5PS9+X6qbhV9+vTTv9PP0788nF8/54mUEpKkOo6oHYAkHc5MwpJU\nkUlYkioyCUtSRSZhSarIJCxJFZmEJakik7AkVWQSlqSKTMKSVNHIJOGI+I8R8VhEPBcRd0fE79SO\n6UAi4qyIuDkino6IVyPigkyfqyJiR0Tsj4jbImJFjVhzIuLzEXFPROyNiF0R8XcR8ZuZfiM5hoi4\nNCLuj4g97b8NEfG7jT4jGXtORHyufRx9pdE+smOIiC+2Y+78e6jRZ2TjB4iIN0XENyPimXaM90fE\neKPPQR/DSCThiPh3wJ8DXwT+NXA/sD4ifq1qYAd2HPBD4LNA1+IbEXEFcBlwCXAasI/WeI4+lEHO\n4CzgL4DTgQ8DRwG3RsSxUx1GfAxPAlcA48Aq4A7gpohYCSMf+zTtk41LaB3zne1zYQwPAkuBZe2/\nD0w9MerxR8QS4AfAC8B5wErgT4B/6ehzaMaQUqr+B9wN/PeOxwE8Bfxp7dgKYn8VuKDRtgNY0/F4\nMfAccFHteA8whl9rj+MDc3gMu4E/mkuxA8cDW4EPAf8AfGWufP60Tpg2z/D8qMf/Z8A/zdLnkIyh\n+plwRBxF62zm9qm21Brx94AzasXVr4hYTuusoHM8e4GNjO54ltA6o/8FzK0xRMQREfFxYBGwYS7F\nDnwN+E5K6Y7Oxjk0ht9oT8k9EhE3RMRJMGfi/yhwb0Tc2J6S2xwRn5568lCOoXoSpnUWtgDY1Wjf\nRetDmGuW0Upoc2I8ERHANcD3U0pTc3ojP4aIOCUinqX1v5PXAh9LKW1lDsQO0P6H4z3A5zNPz4Ux\n3A18itb/yl8KLAf+OSKOY27E/3bgM7T+T+Rc4K+Ar0bEH7afP2RjOHKYG9OcdC3wLuDM2oH06MfA\nqcAY8PvA9RFxdt2QykTEW2j9w/fhlNJLtePpR0ppfcfDByPiHuBx4CJa+2bUHQHck1L6Qvvx/RFx\nCq1/UL55qAOp7RngFVoT/J2WAjsPfTgD20lrTnvkxxMRfwl8BPg3KaWfdjw18mNIKb2cUno0pXRf\nSum/0CpsXc4ciJ3W9NuJwOaIeCkiXgLOAS6PiBdpnW2N+himSSntAX4CrGBu7IOfAlsabVuAt7b/\n+5CNoXoSbp8JbAJWT7W1/xd5NbChVlz9Sik9RmsndY5nMa1fIozMeNoJ+PeAD6aUnuh8bq6MoeEI\n4Jg5Evv3gN+mNR1xavvvXuAG4NSU0qOM/himiYjjaSXgHXNkH/wAeGej7Z20zuYP7XegdpWyXXW8\nCNgPXAz8FnAdrWr3ibVjO0C8x9H64ryH1q8K/lP78Unt5/+0Hf9HaX3Zvg08DBxdO/Z2fNfS+inO\nWbT+ZZ/6W9jRZ2THAHypHfvbgFOA/wa8DHxo1GOfYUzNX0eM9BiALwNnt/fB+4HbaJ3BnzBH4n8v\nrXrC54F3AH8APAt8/FDvg+ofRseAPwtsp/UTkLuA99aOaYZYz2kn31caf/+zo89aWj9x2Q+sB1bU\njrsjtlzsrwAXN/qN5BiA/wE82j5WdgK3TiXgUY99hjHd0ZmER30MwP+i9TPS54AngG8By+dK/O34\nPgI80I7v/wJ/nOlz0McQ7TeSJFVQfU5Ykg5nJmFJqsgkLEkVmYQlqaKDloTn0tKUklTLQUnCc3Bp\nSkmq4qD8RC0i7gY2ppQubz8OWmvAfjWldHWj7wm0FgHZDjw/9GAk6dBbCPw6sD6ltHumjkNfwKdj\nacovTbWllFJEHGhpyvOAvxl2HJI0Aj5B60KWAzoYq6jNtDRl81ptaJ0Bc8MNN7By5UrWrFnD5OTk\nQQjr0Jjr8cPcH4Px1zfXxzBo/Fu2bOGTn/wktPPbTEZhKcvnAb7+9a8zNjbG1q1bWbt2LQATExNM\nTEzUjK1nY2NjjI+Pz95xhM31MRh/fXN9DEOMf9Yp1oORhPtamnJycpLx8XEuuOACbr755oMQliSN\nnqH/OiLNs6UpJelgOljTEV8B/joiNgH3AGto3QPsrw/S+0nSnHRQknBK6cb2b4KvojUN8UPgvJTS\nz2d77VybA26a6/HD3B+D8dc318dwKOOvvpRlRIwDmzZt2jSnJ/IlacrmzZtZtWoVwKqU0uaZ+rp2\nhCRVZBKWpIpMwpJUkUlYkioyCUtSRSZhSarIJCxJFZmEJakik7AkVWQSlqSKTMKSVJFJWJIqMglL\nUkUmYUmqyCQsSRWZhCWpIpOwJFVkEpakikzCklSRSViSKjIJS1JFJmFJqsgkLEkVmYQlqSKTsCRV\nZBKWpIpMwpJUkUlYkioyCUtSRSZhSarIJCxJFZmEJakik7AkVWQSlqSKTMKSVJFJWJIqMglLUkU9\nJ+GIOCsibo6IpyPi1Yi4INPnqojYERH7I+K2iFgxnHAlaX7p50z4OOCHwGeB1HwyIq4ALgMuAU4D\n9gHrI+LoAeKUpHnpyF5fkFL6LvBdgIiITJfLgXUppVvafS4GdgEXAjf2H6okzT9DnROOiOXAMuD2\nqbaU0l5gI3DGMN9LkuaDYRfmltGaotjVaN/Vfk6S1MFfR0hSRT3PCc9iJxDAUqafDS8F7pvphWvW\nrGFsbGxa28TEBBMTE0MOUZJGx1CTcErpsYjYCawGHgCIiMXA6cDXZnrt5OQk4+PjwwxHkkZez0k4\nIo4DVtA64wV4e0ScCvwipfQkcA1wZURsA7YD64CngJuGErEkzSP9nAm/F/gHWgW4BPx5u/0bwB+n\nlK6OiEXAdcAS4E7g/JTSi0OIV5LmlX5+J/xPzFLQSymtBdb2F5IkHT78dYQkVWQSlqSKTMKSVJFJ\nWJIqMglLUkUmYUmqyCQsSRWZhCWpIpOwJFVkEpakikzCklSRSViSKjIJS1JFJmFJqsgkLEkVmYQl\nqSKTsCRVZBKWpIpMwpJUkUlYkioyCUtSRSZhSarIJCxJFZmEJakik7AkVWQSlqSKTMKSVJFJWJIq\nMglLUkUmYUmqyCQsSRWZhCWpIpOwJFVkEpakikzCklSRSViSKjIJS1JFJmFJqqinJBwRn4+IeyJi\nb0Tsioi/i4jfzPS7KiJ2RMT+iLgtIlYML2RJmj96PRM+C/gL4HTgw8BRwK0RcexUh4i4ArgMuAQ4\nDdgHrI+Io4cSsSTNI0f20jml9JHOxxHxKeBnwCrg++3my4F1KaVb2n0uBnYBFwI3DhivJM0rg84J\nLwES8AuAiFgOLANun+qQUtoLbATOGPC9JGne6TsJR0QA1wDfTyk91G5eRisp72p039V+TpLUoafp\niIZrgXcBZw4jkDVr1jA2NjatbWJigomJiWFsXpJGUl9JOCL+EvgIcFZK6acdT+0EAljK9LPhpcB9\nM21zcnKS8fHxfsKRpDmr5+mIdgL+PeCDKaUnOp9LKT1GKxGv7ui/mNavKTYMFqokzT89nQlHxLXA\nBHABsC8ilraf2pNSer7939cAV0bENmA7sA54CrhpKBFL0jzS63TEpbQKb//YaP8j4HqAlNLVEbEI\nuI7WryfuBM5PKb04WKiSNP/0+jvhoumLlNJaYG0f8UjSYcW1IySpIpOwJFVkEpakikzCklSRSViS\nKjIJS1JFJmFJqsgkLEkVmYQlqSKTsCRVZBKWpIoGWdR9qG699VYefvjh1x4vWLCg6HUvv/zytMdH\nHtk9pGafA9m7d29X27XXXtvV9sQTT3S1ve9975v2+Mwzu9e6P+KIsn/zcmPIteWklKY9Puqoo4ri\nePXVV7vaXnnllaL3zO2r1o1XZn7PXFvuPZvbgvy4cm3NcZUeC7kx5bY/TLl98NJLL3W1NfdxL9vL\nae6HQd4z1y+3/3L9cu/b77Zycvs+9xk1P49cn9x7dn5Hn3rqqaKYwDNhSarKJCxJFZmEJamikZkT\nXrBgwazzniVzP7l5n9zrcvORJXObB7J169Zpj3PzSLk5xVxsxxxzTNF79htv7nX9zh9C+Xx1ybZy\nbbkx5dpKXlsaa65faRyl22sq3S+lx0ep5udWOlefi3eQukdpHagptw9KY8vFUTL3P9uxcOyxx866\njdfiKu4pSRo6k7AkVWQSlqSKTMKSVNGcKszlJtaPPvroaY9LC3PN1wHs2bOnqy1XkFi4cGFX269+\n9atpj7dt29bVJyc3ptICXmlRqKm0CJdTWpxqFkZKL/zIyRVZctvL7dNmW+4YK/nB/oFem4vjhRde\n6GrLOf7442ftkyvCvfhi943LSwtnJYXg0s8o93mXFtdKC6slBrm4KRdHyWtnOxaeeeaZovcHz4Ql\nqSqTsCRVZBKWpIpMwpJU0cgU5vbs2cPu3btfe5ybpM8VKZoFg9Ki0/79+7vaHn/88a62ffv2dbXl\nJvOb2ystHJUqXSmqGVvp6wZRUgwtLdiUXjGXU3KVW+lVaaXHUa64Nj4+3tWWK/pu2bJl1u2fdNJJ\nXW251cY6vztTct+XN77xjV1tO3bsmPY4993IKT2e+y0gl8rt00GuKmyOKzfO2YrnuRUZD8QzYUmq\nyCQsSRWZhCWpIpOwJFU0MoW5sbExTjjhhBn75IpdzQJK6RVuuQJQrsiSm5RvXh0H3cWp0gJQv1cJ\nHUi/hcrS4km/hb7S2woN+/Mo2X5u7KVX+J1//vldbW94wxu62k4++eSutkcffXTa4+eee66rz1ln\nndXV9u1vf7urbcmSJV1t5557bldb7ni+6667pj3euHFjV59c0TNXwCu9qrDfZVhLb29UupRlyZK2\nuSJcrvDXGUdpcRM8E5akqkzCklSRSViSKjIJS1JFI1OYe/HFF3n++edfe9zvxH1ukr706rXcZHtp\noaj5HrmrmnJFskHua1dSJMvFkXOwC2KlBomj5LW5z6zknmKQLzrlroRrFtwAVq1a1dXWLAR3Hv9T\nclde5a7iPOecc4pi+8lPftLV9o53vGPa4w0bNnT1yRVWS+/tVroMa8n+Ky2ulRaQS7aXWzp0tu9V\nbl8eMIbinpKkoespCUfEpRFxf0Tsaf9tiIjfbfS5KiJ2RMT+iLgtIlYMN2RJmj96PRN+ErgCGAdW\nAXcAN0XESoCIuAK4DLgEOA3YB6yPiP5XrpGkeaynJJxS+t8ppe+mlB5JKW1LKV0J/Ap4X7vL5cC6\nlNItKaUHgYuBNwEXDjVqSZon+i7MRcQRwEXAImBDRCwHlgG3T/VJKe2NiI3AGcCNvWy/9Iql5gR8\nbkI+N7Gek7tiKTcBn4ut5IqwXAFo2FeqlfQ72MUvKLtSb5A4cp9byZKUuffM7ePc9nP9br/99q62\n1atXd7Vt3769q61ZJHv961/f1ae0qJXrV3qfw+Zrc59jjeO09GrP3Hcv910rve9cUy7W3LHQeWz1\nck/FnqOKiFOAu4CFwLPAx1JKWyPiDCABuxov2UUrOUuSGvr5p+HHwKnAGPD7wPURcfZQo5Kkw0TP\nSTil9DIw9UPI+yLiNFpzwVcDASxl+tnwUuC+2bZ7/fXXs2jRomlt73//+znzzDN7DVGS5oxhXKxx\nBHBMSumxiNgJrAYeAIiIxcDpwNdm28jFF1/M8uXLhxCOJM0dPSXhiPgS8H+AJ4DXAZ8AzgGm1sy7\nBrgyIrYB24F1wFPATcMItt97gZUWgHIFvNJ7ozULF6WxDlLwKBl7rhhRWvAYRElBLBdHaUEj97nl\nxtrcf7MVVGaKLeeDH/xgV9upp57a1fbII490tZ144okzPob8VZy5QlRuedV3vetdXW2ve93rutqa\nS1e+8MILXX1Kr4QrLT6XtOW2n9tXuWMm19bvd600B3QeM70s+drrmfAbgW8A/wrYQ+uM99yU0h3t\nN746IhYB1wFLgDuB81NKZT9PkKTDTE9JOKX06YI+a4G1fcYjSYcV146QpIpMwpJU0cgsZZlS6vv+\nZZ1K7yFWqnSCv9/3yG2/dLm+kuLRIMWv0uJUyVVdpe9Zupxo6ZVkzUJRbgnTpUuXdrXl7hOXazv7\n7O6fyOf65YqGJbHlPo/zzjuvqy3nbW97W1db7t5nzf387ne/u6tPbknNnTt3drXliob9XqmWU1o0\nzCk9npvb66dY3Esu80xYkioyCUtSRSZhSapoZOaEjzzyyGlzZCWrkk29blhyq6jl5pFKLpwoXeVr\nkPibt8eB7s8t9zkee+yxXW25H9kvXLiwq+24444ramvO4+a2tXjx4q623MUETz/9dFfbj370o662\n3Hxhc5/mxt68vQ/Am9/85q623H5/+OGHu9pKb23TvDgod3zkxp773HLH2wMPPNDVlpt3Xrly5bTH\nuTnWxx9/vKstd/uk3NxxTu54O/nkk6c9Xrase92v3Hc0d6FKrl/u+Mh9P5r7OTePntP5Xe7lgijP\nhCWpIpOwJFVkEpakikzCklTRyBTmjjnmmGlFk9IfSDcLW7mJ9lyhIbf90tsglRTrSi/eKC3W5VbE\nahZUcq8tXUGu39tJHeg9mv1y+yXXlrs4ISdXZMmtkFbS58EHH+xqe+ihh7rafvnLX3a1lX6+JeMv\nXSkv9565QleuLVeYaxZN3/rWtxbFkVN63OfG0IzjhBNO6OqTi790+6UXdTTlCq2zXYzxs5/9LHtL\nqxzPhCWpIpOwJFVkEpakikzCklTRyBTm9u3bN+1qm9JVspqFqFyBKVfoyrWVXqU3TLmCRy6OZ599\ntqtt9+7dXW379u2b9jh3BVNu+7miZOmVSLntlRRockW43JVqpVcyluz73Otyn2Pu+MhdOVVa0Mxp\nFopKj9PcGHKx5QpzuSJT7vNtyl3JmBt76ephuWNm69atMz7uJbbcsVVavGxeiZrbVu4K0M4bFecK\nxwfimbAkVWQSlqSKTMKSVJFJWJIqGpnC3IIFC2Zd1rHk6qTSK5hyV96MjY0VvbbkqrFcYar0Kp7c\nVV2PPPJIV9uTTz45a2y5oksutkFuLVXy2tx75pZkLC3C5dpKxjVI8TV3zOQKmqVF5ebxUHproNy2\ncrGVXr3WjDdXcMsdk6XLvOY+o1xszffNvWfuM8otqVn6/Su5HVrJralg+pKuJcXO12Io7ilJGjqT\nsCRVZBKWpIpMwpJU0cgU5ppyE+slSxXm5IoFuYJY6VKWw5QrgpQWj0qKMf0u35fbFuQLGaUFmhKl\nBaBhysU6SKGytCDWVFpULlU6rubxVloIzSkt+ubaSr7fuSJZSXHtQEquwi39DnV+l3u5ktIzYUmq\nyCQsSRWZhCWpIpOwJFU0soW50mJayQT4bFfiTSkt/A1StGkqLSqUfh4l2xqkANRvES4XR+l95wZZ\nMrFE6ZiGWYA80PaaBtlXpWMoKWLl9kFp8anf+y2Wjn2Qzyj3HWrGm4s/97rOz7uX48IzYUmqyCQs\nSRWZhCWpIpOwJFU0MoW5V155ZSj3eMtNopcsmwf5olBuybpcv+ZEfG7ifpBCQ+ln02+RpXRbpcWe\nptyYSosXuX65MZSMvWQZ0kOl5JipoXTJ1VK5wvgg34+mQYqjJcdR6fY7X9fLVZOeCUtSRQMl4Yj4\nXES8GhFfabRfFRE7ImJ/RNwWESsGC1OS5qe+k3BE/A5wCXB/o/0K4LL2c6cB+4D1EdG92owkHeai\nn/mUiDge2AR8BvgCcF9K6T+3n9sBfDmlNNl+vBjYBfz7lNKNmW2NA5s2bdrE+Ph43wORpFGxefNm\nVq1aBbAqpbR5pr79ngl/DfhOSumOzsaIWA4sA26faksp7QU2Amf0+V6SNG/1/OuIiPg48B7gvZmn\nlwGJ1plvp13t5yRJHXpKwhHxFuAa4MMppf5WWD+ANWvWdN3teGJigomJiWG+jSSNlF7PhFcBJwKb\n4///EG4BcHZEXAb8FhDAUqafDS8F7ptpw5OTk84JSzrs9Don/D3gt2lNR5za/rsXuAE4NaX0KLAT\nWD31gnZh7nRgwzAClqT5pKcz4ZTSPuChzraI2AfsTiltaTddA1wZEduA7cA64CngpoGjlaR5ZhiX\nLU/7jVtK6eqIWARcBywB7gTOTykd+rtoStKIGzgJp5Q+lGlbC6wddNuSNN+5doQkVWQSlqSKTMKS\nVJFJWJIqMglLUkUmYUmqyCQsSRWZhCWpIpOwJFVkEpakikzCklSRSViSKjIJS1JFJmFJqsgkLEkV\nmYQlqSKTsCRVZBKWpIpMwpJUkUlYkioyCUtSRSZhSarIJCxJFZmEJakik7AkVWQSlqSKTMKSVJFJ\nWJIqMglLUkUmYUmqyCQsSRWZhCWpIpOwJFVkEpakikzCklSRSViSKjIJS1JFPSXhiPhiRLza+Huo\n0eeqiNgREfsj4raIWDHckCVp/ujnTPhBYCmwrP33gaknIuIK4DLgEuA0YB+wPiKOHjxUSZp/juzj\nNS+nlH5+gOcuB9allG4BiIiLgV3AhcCN/YUoSfNXP2fCvxERT0fEIxFxQ0ScBBARy2mdGd8+1TGl\ntBfYCJwxlGglaZ7pNQnfDXwKOA+4FFgO/HNEHEcrASdaZ76ddrWfkyQ19DQdkVJa3/HwwYi4B3gc\nuAj48TADk6TDQT9zwq9JKe2JiJ8AK4B/BIJW0a7zbHgpcN9s21qzZg1jY2PT2iYmJpiYmBgkREka\naQMl4Yg4nlYC/kZK6bGI2AmsBh5oP78YOB342mzbmpycZHx8fJBwJGnO6SkJR8SXge/QmoJ4M/Bf\ngZeAv213uQa4MiK2AduBdcBTwE1DileS5pVez4TfAnwLOAH4OfB94H0ppd0AKaWrI2IRcB2wBLgT\nOD+l9OLwQpak+aPXwtysE7QppbXA2j7jkaTDimtHSFJFJmFJqsgkLEkVmYQlqSKTsCRVZBKWpIpM\nwpJUkUlYkioyCUtSRSZhSarIJCxJFZmEJakik7AkVWQSlqSKTMKSVJFJWJIqMglLUkUmYUmqyCQs\nSRWZhCWpIpOwJFVkEpakikzCklSRSViSKjIJS1JFJmFJqsgkLEkVmYQlqSKTsCRVZBKWpIpMwpJU\nkUlYkioyCUtSRSZhSarIJCxJFZmEJakik7AkVWQSlqSKek7CEfGmiPhmRDwTEfsj4v6IGG/0uSoi\ndrSfvy0iVgwvZEmaP3pKwhGxBPgB8AJwHrAS+BPgXzr6XAFcBlwCnAbsA9ZHxNFDilmS5o0je+z/\nOeCJlNKnO9oeb/S5HFiXUroFICIuBnYBFwI39huoJM1HvU5HfBS4NyJujIhdEbE5Il5LyBGxHFgG\n3D7VllLaC2wEzhhGwJI0n/SahN8OfAbYCpwL/BXw1Yj4w/bzy4BE68y30672c5KkDr1ORxwB3JNS\n+kL78f0RcQpwKfDNQQJZs2YNY2Nj09omJiaYmJgYZLOSNNJ6TcI/BbY02rYA/7b93zuBAJYy/Wx4\nKXDfTBuenJxkfHx8pi6SNO/0Oh3xA+CdjbZ30i7OpZQeo5WIV089GRGLgdOBDf2HKUnzU69nwpPA\nDyLi87R+6XA68GngP3T0uQa4MiK2AduBdcBTwE0DRytJ80xPSTildG9EfAz4M+ALwGPA5Smlv+3o\nc3VELAKuA5YAdwLnp5ReHF7YkjQ/9HomTErp74G/n6XPWmBtfyFJ0uHDtSMkqSKTsCRVZBKWpIpM\nwpJUkUlYkioyCUtSRSZhSarIJCxJFZmEJakik7AkVdTzZcsHwUKALVuaK2RK0tzUkc8WztY3UkoH\nN5rZAoj4A+BvqgYhSQfHJ1JK35qpwygk4RNo3bl5O/B81WAkaTgWAr8OrE8p7Z6pY/UkLEmHMwtz\nklSRSViSKjIJS1JFJmFJqsgkLEkVmYQlqSKTsCRV9P8ALEUInnEdf2QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1281b3510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Data check '''\n",
    "# Examine shapes and images\n",
    "print('Shape of training images and labels:\\t',\n",
    "      train_svhn_images.shape, train_svhn_labels.shape)\n",
    "print('Shape of test images and labels:\\t',\n",
    "      test_svhn_images.shape, test_svhn_labels.shape)\n",
    "\n",
    "print()\n",
    "for i in xrange(2):\n",
    "    print('Should see', train_svhn_labels[i])\n",
    "    show(train_svhn_images[i])\n",
    "    \n",
    "    print('Should see', test_svhn_labels[i])\n",
    "    show(test_svhn_images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good so far. Now we need to normalize our image data (to 0-1 range), add an extra layer indicating depth, and one-hot encode the labels. We also need to separate out a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original min and max: 0 255.0\n",
      "New min and max: 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalizing data...\n",
    "pixel_max = float(train_svhn_images.max())\n",
    "\n",
    "print(\"Original min and max:\", train_svhn_images.min(), pixel_max)\n",
    "\n",
    "train_svhn_images = train_svhn_images / pixel_max\n",
    "test_svhn_images  = test_svhn_images / pixel_max\n",
    "\n",
    "# Check images are still legit:\n",
    "print(\"New min and max:\", train_svhn_images.min(), train_svhn_images.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (33402, 64, 64, 1) (33402, 6, 11)\n",
      "Validation set (2613, 64, 64, 1) (2613, 6, 11)\n",
      "Test set (10455, 64, 64, 1) (10455, 6, 11)\n"
     ]
    }
   ],
   "source": [
    "num_labels = 11  # -1 to 9\n",
    "num_channels = 1 # grayscale\n",
    "p_valid = 0.2     # Proportion of test set to be used for validation set\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, size[0], size[1], num_channels)).astype(np.float32)\n",
    "    labels = ((np.arange(num_labels) - 1) == labels[:,:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "n_valid = int(test_svhn_labels.shape[0] * p_valid)\n",
    "\n",
    "train_svhn, train_svhn_l = reformat(train_svhn_images, train_svhn_labels)\n",
    "valid_svhn, valid_svhn_l = reformat(test_svhn_images[:n_valid], test_svhn_labels[:n_valid])\n",
    "test_svhn, test_svhn_l   = reformat(test_svhn_images[n_valid:], test_svhn_labels[n_valid:])\n",
    "\n",
    "print('Training set', train_svhn.shape, train_svhn_l.shape)\n",
    "print('Validation set', valid_svhn.shape, valid_svhn_l.shape)\n",
    "print('Test set', test_svhn.shape, test_svhn_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Checking one-hot encoding\n",
    "print(train_svhn_l[0])  # Should correspond to 1,9 shown above (trailed by -1's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to train our DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to determine prediction accuracy\n",
    "def accuracy(predictions, labels):\n",
    "    return(100.0 * np.sum(np.argmax(predictions, 2) == np.argmax(labels, 2))\n",
    "          / predictions.shape[0] / predictions.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"pack:0\", shape=(128, 5, 11), dtype=float32)\n",
      "Tensor(\"pack_1:0\", shape=(2613, 5, 11), dtype=float32)\n",
      "Tensor(\"pack_2:0\", shape=(10455, 5, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "patch_size = 5\n",
    "\n",
    "# Conv depths...\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "\n",
    "# Hidden depths...\n",
    "n_hidden1 = 128\n",
    "n_hidden2 = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data -----------------------\n",
    "    tf_train = tf.placeholder(\n",
    "        tf.float32, shape = (batch_size, size[0], size[1], num_channels)\n",
    "    )\n",
    "    tf_train_l = tf.placeholder(\n",
    "        tf.float32, shape = (batch_size, svhn_max_digits, num_labels)\n",
    "    )\n",
    "    tf_valid = tf.constant(valid_svhn)\n",
    "    tf_test  = tf.constant(test_svhn)\n",
    "    \n",
    "    # Net parameters -----------------------\n",
    "    \n",
    "    # Convolution layers\n",
    "    c1_w = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev = 0.05)\n",
    "    )\n",
    "    c1_b = tf.Variable(\n",
    "        tf.zeros([depth1])\n",
    "    )\n",
    "    c2_w = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, depth1, depth2], stddev = 0.05)\n",
    "    )\n",
    "    c2_b = tf.Variable(\n",
    "        tf.constant(1.0, shape = [depth2])\n",
    "    )\n",
    "    c3_w = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, depth2, depth3], stddev = 0.05)\n",
    "    )\n",
    "    c3_b = tf.Variable(\n",
    "        tf.constant(1.0, shape = [depth3])\n",
    "    )\n",
    "    # Fully connected layers\n",
    "    f1_w = tf.Variable(\n",
    "        tf.truncated_normal([depth3, n_hidden1], stddev = 0.05)\n",
    "    )\n",
    "    f1_b = tf.constant(1.0, shape = [n_hidden1])\n",
    "    f2_w = tf.Variable(\n",
    "        tf.truncated_normal([n_hidden1, n_hidden2], stddev = 0.05)\n",
    "    )\n",
    "    f2_b = tf.constant(1.0, shape = [n_hidden2])\n",
    "    \n",
    "    \n",
    "    # Logistic classifier layers (one for each digit)\n",
    "    s1_w = tf.Variable(tf.truncated_normal([n_hidden2, num_labels], stddev = 0.05))\n",
    "    s1_b = tf.constant(1.0, shape = [num_labels])\n",
    "    s2_w = tf.Variable(tf.truncated_normal([n_hidden2, num_labels], stddev = 0.05))\n",
    "    s2_b = tf.constant(1.0, shape = [num_labels])\n",
    "    s3_w = tf.Variable(tf.truncated_normal([n_hidden2, num_labels], stddev = 0.05))\n",
    "    s3_b = tf.constant(1.0, shape = [num_labels])\n",
    "    s4_w = tf.Variable(tf.truncated_normal([n_hidden2, num_labels], stddev = 0.05))\n",
    "    s4_b = tf.constant(1.0, shape = [num_labels])\n",
    "    s5_w = tf.Variable(tf.truncated_normal([n_hidden2, num_labels], stddev = 0.05))\n",
    "    s5_b = tf.constant(1.0, shape = [num_labels])\n",
    "    \n",
    "    # Model ----------------------\n",
    "    \n",
    "    # Convolution wrapper\n",
    "    def conv2d(X, W, b):\n",
    "        conv = tf.nn.conv2d(X, W, strides = [1, 2, 2, 1], padding = 'SAME') + b\n",
    "        # Order (1) pool then (2) hidden reduces computation requirements and speeds learning\n",
    "        pool = tf.nn.max_pool(conv, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "        hidden = tf.nn.relu(pool)\n",
    "        return hidden\n",
    "    \n",
    "    # Model\n",
    "    def model(data, keep_prob = 1):\n",
    "        # Convolution layers\n",
    "        x = conv2d(data, c1_w, c1_b)\n",
    "        x = conv2d(x,    c2_w, c2_b)\n",
    "        x = conv2d(x,    c3_w, c3_b)\n",
    "        \n",
    "        # Fully connected\n",
    "        shape = x.get_shape().as_list()\n",
    "        reshape = tf.reshape(x, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        x = tf.nn.relu(tf.matmul(reshape, f1_w) + f1_b)\n",
    "        x = tf.nn.relu(tf.matmul(x, f2_w) + f2_b)\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        \n",
    "        # Logistic classifiers for each digit\n",
    "        logit1 = tf.matmul(x, s1_w) + s1_b\n",
    "        logit2 = tf.matmul(x, s2_w) + s2_b\n",
    "        logit3 = tf.matmul(x, s3_w) + s3_b\n",
    "        logit4 = tf.matmul(x, s4_w) + s4_b\n",
    "        logit5 = tf.matmul(x, s5_w) + s5_b\n",
    "        \n",
    "        # Combine predictions to match 2D encoding\n",
    "        # Note. if using sparse_... need pack([]), but not `axis=1`\n",
    "        pred = tf.pack([logit1, logit2, logit3, logit4, logit5], axis=1)\n",
    "        print(pred)\n",
    "        return(pred)\n",
    "\n",
    "    # Run --------------------------\n",
    "    # If model seems to be overfitting, look at:\n",
    "    # l1 and l2 regularisers; batch normalization\n",
    "    \n",
    "    logits = model(tf_train, 0.50)  # Try not to exceed .50\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits[:, 0, :], tf_train_l[:, 0])) +\\\n",
    "        tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits[:, 1, :], tf_train_l[:, 1])) +\\\n",
    "        tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits[:, 2, :], tf_train_l[:, 2])) +\\\n",
    "        tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits[:, 3, :], tf_train_l[:, 3])) +\\\n",
    "        tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits[:, 4, :], tf_train_l[:, 4]))\n",
    "    \n",
    "    # Adam optimizer (has a self adjusting learning rate)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    train_pred = tf.nn.softmax(logits)\n",
    "    valid_pred = tf.nn.softmax(model(tf_valid))\n",
    "    test_pred  = tf.nn.softmax(model(tf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-101-efb7a94dbce4>:6 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Intialized\n",
      "\n",
      "--- Step 0 ---\n",
      "Time elapsed: 2.3 seconds\n",
      "Minibatch loss: 12.467659\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Si/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.0%\n",
      "\n",
      "--- Step 500 ---\n",
      "Time elapsed: 57.8 seconds\n",
      "Minibatch loss: 5.870628\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "\n",
      "--- Step 1000 ---\n",
      "Time elapsed: 118.0 seconds\n",
      "Minibatch loss: 6.072581\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.0%\n",
      "\n",
      "--- Test ---\n",
      "Test accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Run the graph \"\"\"\n",
    "num_steps = 1001 #1000001\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Intialized\\n\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset  = (step * batch_size) % (train_svhn_l.shape[0] - batch_size)\n",
    "        batch   = train_svhn[offset:(offset + batch_size), :, :, :]\n",
    "        batch_l = train_svhn_l[offset:(offset + batch_size), :]\n",
    "        feed_dict = {\n",
    "            tf_train   : batch,\n",
    "            tf_train_l : batch_l\n",
    "        }\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_pred], feed_dict = feed_dict\n",
    "        )\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print('--- Step %d ---' % (step))\n",
    "            print('Time elapsed: %.1f seconds' % (time.time() - start_time))\n",
    "            print('Minibatch loss: %f' % (l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_l))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_pred.eval(), valid_svhn_l))\n",
    "            print()\n",
    "    \n",
    "    print('--- Test ---')\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_pred.eval(), test_svhn_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "_Describe how you set up the training and testing data for your model. How does the model perform on a realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "_What changes did you have to make, if any, to achieve \"good\" results? Were there any options you explored that made the results worse?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "_What were your initial and final results with testing on a realistic dataset? Do you believe your model is doing a good enough job at classifying numbers correctly?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 3: Test a Model on Newly-Captured Images\n",
    "\n",
    "Take several pictures of numbers that you find around you (at least five), and run them through your classifier on your computer to produce example results. Alternatively (optionally), you can try using OpenCV / SimpleCV / Pygame to capture live images from a webcam and run those through your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "_Choose five candidate images of numbers you took from around you and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "_Is your model able to perform equally well on captured pictures or a live camera stream when compared to testing on the realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Leave blank if you did not complete this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Step 4: Explore an Improvement for a Model\n",
    "\n",
    "There are many things you can do once you have the basic classifier in place. One example would be to also localize where the numbers are on the image. The SVHN dataset provides bounding boxes that you can tune to train a localizer. Train a regression loss to the coordinates of the bounding box, and then test it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "_How well does your model localize numbers on the testing set from the realistic dataset? Do your classification results change at all with localization included?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "_Test the localization function on the images you captured in **Step 3**. Does the model accurately calculate a bounding box for the numbers in the images you found? If you did not use a graphical interface, you may need to investigate the bounding boxes by hand._ Provide an example of the localization created on a captured image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Optional Step 5: Build an Application or Program for a Model\n",
    "Take your project one step further. If you're interested, look to build an Android application or even a more robust Python program that can interface with input images and display the classified numbers and even the bounding boxes. You can for example try to build an augmented reality app by overlaying your answer on the image like the [Word Lens](https://en.wikipedia.org/wiki/Word_Lens) app does.\n",
    "\n",
    "Loading a TensorFlow model into a camera app on Android is demonstrated in the [TensorFlow Android demo app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android), which you can simply modify.\n",
    "\n",
    "If you decide to explore this optional route, be sure to document your interface and implementation, along with significant results you find. You can see the additional rubric items that you could be evaluated on by [following this link](https://review.udacity.com/#!/rubrics/413/view)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your optional code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "Provide additional documentation sufficient for detailing the implementation of the Android application or Python program for visualizing the classification of numbers in images. It should be clear how the program or application works. Demonstrations should be provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your documentation here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n",
    "**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
